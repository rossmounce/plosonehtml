</div>

<div id="section2" class="section"><a id="s2" name="s2" toc="s2" title="Materials and Methods"></a><h3>Materials and Methods</h3><a id="article1.body1.sec2.p1" name="article1.body1.sec2.p1"></a><p>Forty-eight objects were constructed, each made from six smooth wooden blocks measuring 1.6 cm high, 3.6 cm long and 2.2 cm wide. The resulting objects were 9.5 cm high, the other dimensions varying according to the arrangement of the component blocks. Constructing the objects from smooth wooden component blocks avoided the textural difference between the top and bottom surfaces of Lego™ bricks used by Newell et al. <a href="#pone.0000890-Newell1">[2]</a>. This was important to obviate undesirable cues to rotation around the x- and y-axes. The objects were painted medium grey to remove visual cues from variations in the natural wood color and grain. Each object had a small (&lt;1 mm) grey pencil dot on one facet that was used to guide presentation of the object by the experimenter to the participant in a particular orientation. Pilot testing showed that participants were never aware of these small dots and debriefing confirmed that this was so in the main experiment also.</p>
<a id="article1.body1.sec2.p2" name="article1.body1.sec2.p2"></a><p>The 48 objects were divided into three sets of sixteen, one for each axis of rotation. Each set was further divided into four subsets of four, with one subset for each modality condition. These subsets were checked to ensure that they contained no ‘mirror-image’ pairs. Difference matrices were calculated for the twelve subsets based on the number of differences in the position (three possibilities: in the middle or at either end of the preceding block along the z-axis) and orientation (two possibilities: either the same as, or orthogonal to, the preceding block along the z-axis) of each component block. These values could range from 0 (identical) to 6 (completely different) and were used to calculate the mean difference between objects. The mean difference between objects within a subset ranged from 5.2 to 5.7; the mean of these subset scores within a set was taken as the score for the set and these ranged from 5.4 to 5.5. Paired t-tests on these scores showed no significant differences between subsets or sets (all p values &gt;.05) and the objects were therefore considered equally discriminable.</p>
<a id="article1.body1.sec2.p3" name="article1.body1.sec2.p3"></a><p>The procedures were approved by the Institutional Review Board of Emory University. Twenty-four undergraduates (12 male and 12 female, mean age 20 years 3 months) participated after giving informed written consent. Participants performed a four-alternative forced-choice object identification task in two within-modal (visual-visual; haptic-haptic) and two cross-modal (visual-haptic; haptic-visual) conditions. Objects were either unrotated between encoding and test presentations, or rotated by 180° about the x-, y-, and z-axes (<a href="#pone-0000890-g001">Figure 1</a>). In each encoding-recognition sequence, participants learned four objects, identified by numbers, either visually or haptically. Each object was presented for 30 seconds haptically or 15 seconds visually; these times were determined by a pilot experiment. The 2:1 haptic:visual ratio of presentation times reflects that used in previous studies <a href="#pone.0000890-Newell1">[2]</a>, <a href="#pone.0000890-Lacey1">[9]</a>, <a href="#pone.0000890-Freides1">[10]</a>. During visual presentation, participants sat at a table on which the objects were placed. The table was 86 cm high so that the initial viewing distance was 30–40 cm and the initial viewing angle as the participants looked down on the objects was approximately 35–45°. As in the earlier study of Newell et al. <a href="#pone.0000890-Newell1">[2]</a>, the seated participants were free to move their head and eyes when looking at the objects but were not allowed to get up and walk around them.</p>
<a id="article1.body1.sec2.p4" name="article1.body1.sec2.p4"></a><p>During haptic presentation, participants felt the objects behind an opaque cloth screen and were free to move their hands around the objects. Unlike the study of Newell et al. <a href="#pone.0000890-Newell1">[2]</a>, the objects were not fixed to a surface but placed in the participants' hands: participants were instructed to keep the objects in exactly the same orientation as presented and not to rotate or otherwise manipulate them. On subsequent recognition trials, the four objects were presented both unrotated and rotated by 180°, about a specific axis from the initial orientation, providing blocks of eight trials. Participants were asked to identify each object by its number. Objects were rotated about each axis in turn, all the modality conditions being completed for a given axis before moving on to the next axis of rotation. The order of the modality conditions, axes of rotation and object sets was fully counterbalanced across subjects.</p>
</div>

<div id="section3" class="section"><a id="s3" name="s3" toc="s3" title="Results"></a><h3>Results</h3><a id="article1.body1.sec3.p1" name="article1.body1.sec3.p1"></a><p><a href="#pone-0000890-g002">Figure 2</a> shows that object rotation substantially degraded recognition accuracy in the within-modal conditions, but only slightly decreased cross-modal recognition accuracy. A two-way (within- vs. cross-modal, unrotated vs. rotated) repeated-measures analysis of variance (RM-ANOVA) showed that object rotation significantly reduced recognition accuracy (F<sub>1,23</sub> = 30.04, p = &lt;.001) and that overall within-modal recognition accuracy was marginally better than overall cross-modal recognition (F<sub>1,23</sub> = 4.23, p = .051). These two factors interacted (F<sub>1,23</sub> = 12.58, p = .002) and post-hoc t-tests showed that this was because within-modal recognition accuracy was highly significantly reduced by rotation (t = 7.25, p &lt;.001) while cross-modal recognition accuracy was not (t = 1.66, p = .11) (<a href="#pone-0000890-g002">Figure 2</a>).</p>
<div class="figure" id="pone-0000890-g002"><div class="img"><a name="pone-0000890-g002" title="Click for larger image " href="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000890.g002&amp;representation=PNG_M" data-doi="info:doi/10.1371/journal.pone.0000890" data-uri="info:doi/10.1371/journal.pone.0000890.g002"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000890.g002&amp;representation=PNG_I" alt="thumbnail" class="thumbnail"></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000890.g002/powerpoint">
                    PPT
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000890.g002/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000890.g002/largerimage">
                    PNG
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000890.g002/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pone.0000890.g002.PNG_L"></span>)
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000890.g002/originalimage">
                    TIFF
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000890.g002/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pone.0000890.g002.TIF"></span>)
