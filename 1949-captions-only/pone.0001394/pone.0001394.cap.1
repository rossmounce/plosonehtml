
<div id="section2" class="section"><a id="s2" name="s2" toc="s2" title="Materials and Methods"></a><h3>Materials and Methods</h3>
<h4>Participants</h4>
<a id="article1.body1.sec2.sec1.p1" name="article1.body1.sec2.sec1.p1"></a><p>Twelve right-handed adults (8 female) from the Carnegie Mellon community participated and gave written informed consent approved by the University of Pittsburgh and Carnegie Mellon Institutional Review Boards. Six additional participants were excluded from the analysis due to head motion greater than 2.5 mm.</p>


<h4>Experimental paradigm</h4>
<a id="article1.body1.sec2.sec2.p1" name="article1.body1.sec2.sec2.p1"></a><p>The stimuli depicted concrete objects from two semantic categories (<em>tools</em> and <em>dwellings</em>), and took the form of white line drawings on a black background. There were five exemplars per category; the objects were <em>drill</em>, <em>hammer</em>, <em>screwdriver</em>, <em>pliers</em>, <em>saw</em>, <em>apartment</em>, <em>castle</em>, <em>house</em>, <em>hut</em>, and <em>igloo</em>. The drawings of the ten objects were presented six times (in six random permutation orders) to each participant. Participants were asked to think of the same object properties each time they saw a given object, to encourage activation of multiple attributes of the depicted object, in addition to those used for visual recognition. The intention was to foster the retrieval and assessment of the most salient properties of an object. To ensure that each participant had a consistent set of properties to think about, he or she was asked to generate a set of properties for each exemplar prior to the scanning session (such as <em>cold</em>, <em>knights</em>, and <em>stone</em> for <em>castle</em>). However, nothing was done to elicit consistency across participants.</p>
<a id="article1.body1.sec2.sec2.p2" name="article1.body1.sec2.sec2.p2"></a><p>Each stimulus was presented for 3s, followed by a 7s rest period, during which the participants were instructed to fixate on an X displayed in the center of the screen. There were six additional presentations of a fixation X, 21s each, distributed across the session to provide a baseline measure of activation. A schematic representation of the presentation timing is shown in <a href="#pone-0001394-g001">Figure 1</a>.</p>


<h4>fMRI procedure</h4>
<a id="article1.body1.sec2.sec3.p1" name="article1.body1.sec2.sec3.p1"></a><p>Functional images were acquired on a Siemens Allegra 3.0T scanner (Siemens, Erlangen, Germany) at the Brain Imaging Research Center of Carnegie Mellon University and the University of Pittsburgh using a gradient echo EPI pulse sequence with TR = 1000 ms, TE = 30 ms, and a 60° flip angle. Seventeen 5-mm thick oblique-axial slices were imaged with a gap of 1 mm between slices. The acquisition matrix was 64×64 with 3.125×3.125×5 mm<sup>3</sup> voxels.</p>


<h4>fMRI data processing and analysis</h4>
<a id="article1.body1.sec2.sec4.p1" name="article1.body1.sec2.sec4.p1"></a><p>Data processing and statistical analysis were performed with Statistical Parametric Mapping software (SPM99, Wellcome Department of Imaging Neuroscience, London, UK). The data were corrected for slice timing, motion, linear trend, and were temporally smoothed with a high-pass filter using a 190 s cutoff. The data were normalized to the Montreal Neurological Institute (MNI) template brain image using a 12-parameter affine transformation. Group contrast maps were constructed using a height threshold of p&lt;0.001 (uncorrected) and an extent threshold of 160 voxels, resulting in the cluster-level threshold of p&lt;0.05, corrected for multiple comparisons.</p>
<a id="article1.body1.sec2.sec4.p2" name="article1.body1.sec2.sec4.p2"></a><p>Analyses of a single brain region at a time used region definitions derived from the Anatomical Automatic Labeling (AAL) system <a href="#pone.0001394-TzourioMazoyer1">[10]</a>. In addition to existing AAL regions, left and right intraparietal sulcus (IPS) regions were defined, and superior, middle, and inferior temporal gyrus regions were separated into anterior, middle, and posterior sections based on planes F and D from the Rademacher scheme <a href="#pone.0001394-Rademacher1">[11]</a>, for a total of 71 regions.</p>
<a id="article1.body1.sec2.sec4.p3" name="article1.body1.sec2.sec4.p3"></a><p>The data were prepared for machine learning methods by spatially normalizing the images into MNI space and resampling to 3×3×6 mm<sup>3</sup> voxels. Voxels outside the brain or absent from at least one participant were excluded from further analysis. The percent signal change (PSC) relative to the fixation condition was computed at each voxel for each object presentation. The mean PSC of the four images acquired within a 4s window, offset 4s from the stimulus onset (to account for the delay in hemodynamic response) provided the main input measure for the machine learning classifiers. The PSC data for each object-presentation were further normalized to have mean zero and variance one to equalize the between-participants variation in exemplars.</p>


<h4>Machine learning methods</h4>
<a id="article1.body1.sec2.sec5.p1" name="article1.body1.sec2.sec5.p1"></a><p>Classifiers were trained to identify cognitive states associated with viewing drawings, using the evoked pattern of functional activity (mean PSC). Classifiers were functions <em>f</em> of the form: <em>f: mean_PSC→Y<sub>j</sub></em>, <em>j</em> = 1, …, <em>m</em>, where <em>Y<sub>j</sub></em> were either categories (<em>tools</em>, <em>dwellings</em>) or ten exemplars (<em>hammer</em>, <em>pliers</em>, …, <em>house</em>), where <em>m</em> was either 2 or 10, accordingly, and where <em>mean_PSC</em> was a vector of mean PSC voxel activations. To evaluate classification performance, trials were divided into disjoint training and test sets. Prior to classification, relevant features (voxels) were extracted (as described below) to reduce the dimensionality of the data, using only the training set for this selection. A classifier was built from the training set, using these selected features. Classification performance was then evaluated on only the left-out test set, to ensure unbiased estimation of the classification error. Our previous exploration indicated that several feature selection methods and classifiers produce comparable results. Here we report results from one feature selection method and one classifier, chosen for simplicity.</p>


<h4>Feature selection</h4>
<a id="article1.body1.sec2.sec6.p1" name="article1.body1.sec2.sec6.p1"></a><p>Feature selection first identified the voxels whose responses were the most stable over six presentations of objects within a participant, and then selected from among the stable voxels those that best discriminated among objects within the training set, using only the data in the training set. The 400 most stable voxels were selected, where voxel stability was computed as the average pairwise correlation between 10-object vectors across six presentations. In the second step, all of the stable voxels were assessed for how discriminating they were, by training a logistic regression classifier to discriminate among object exemplars or categories on various subsets of only the training set. Finally, from among the 400 voxels selected for stability, discriminating subsets of sizes 10, 25, 50, 75, 100, 200, and 400 voxels were selected based on having the highest (absolute valued) regression weights in the logistic regression. Locations of these selected voxels (henceforth, diagnostic voxels) were visualized on a standard brain template using MRIcro <a href="#pone.0001394-Rorden1">[12]</a>.</p>


<h4>Classification</h4>
<a id="article1.body1.sec2.sec7.p1" name="article1.body1.sec2.sec7.p1"></a><p>The Gaussian Naïve Bayes (GNB) pooled variance classifier was used <a href="#pone.0001394-Mitchell1">[13]</a>. It is a generative classifier that models the joint distribution of a class <em>Y</em> and attributes <em>X</em>, and assumes the attributes <em>X<sub>1</sub></em>, …, <em>X<sub>n</sub></em> are conditionally independent given <em>Y</em>. The classification rule is:<a name="" id=""></a><span class="equation"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001394.e001&amp;representation=PNG" class="inline-graphic"></span><br>In this experiment classes were equally frequent. Classification results were evaluated using <em>k</em>-fold cross-validation, where one example per class was left out for each fold. For each participant, a classifier was trained to identify either which of 10 object exemplars or which of two object categories that participant was viewing, based on only 4 s of fMRI data per object presentation. In all analyses, the accuracy of identification was based only on test data that was completely disjoint from the training data. With a two-class classification problem, the chance level is 0.5. With the ten-class classification problem, <em>rank accuracy</em> was used <a href="#pone.0001394-Mitchell1">[13]</a>. The list of potential classes was rank-ordered from most to least likely, and the normalized rank of a correct class in a sorted list was computed. Rank accuracy ranges from 0 to 1, and the chance level is 0.5.</p>
<a id="article1.body1.sec2.sec7.p2" name="article1.body1.sec2.sec7.p2"></a><p>Peak classification accuracy over the previously defined subsets having different numbers of voxels, e.g., 10, 25, …, 400, was reported. To evaluate the statistical significance of this observed classification accuracy, the result was compared to a permutation distribution. For each of the 1,000 non-informative permutations of labels in the training set, permutation classification accuracies for every set of features were computed, and the best permutation accuracy over the subsets with different numbers of voxels was recorded. The observed accuracy was then compared to the distribution of recorded permutation classification accuracies; if the observed accuracy had a p-value of at most 0.001, then the result was considered statistically significant.</p>


<h4>Analyses of a single brain region at a time</h4>
<a id="article1.body1.sec2.sec8.p1" name="article1.body1.sec2.sec8.p1"></a><p>Single anatomical brain regions that consistently identified object exemplars or categories across participants were selected using cross-validation, and the significance of those identifications was tested across participants. Within each participant, a cross-validated accuracy for each region was computed by a logistic regression classifier using all the voxels from that anatomical region. The mean classification accuracy was computed for each anatomical region across participants, and compared to a binomial distribution. The obtained p-values (computed using a normal approximation) were compared to the level of significance α = 0.001, using the Bonferroni correction to account for the multiple comparisons.</p>


<h4>Analysis of the confusion patterns</h4>
<a id="article1.body1.sec2.sec9.p1" name="article1.body1.sec2.sec9.p1"></a><p>Single brain regions were compared in terms of their confusion patterns using a generalization of the principal components analysis method <a href="#pone.0001394-Lavit1">[14]</a>, <a href="#pone.0001394-Abdi1">[15]</a>. Within each participant, for each of the selected regions, a confusion matrix was constructed based on the most likely prediction of the classifier. Next, a regions-by-regions dissimilarity matrix was constructed for each participant, where the dissimilarity between any two anatomical regions was measured as one minus the correlation coefficient of the off-diagonal elements of the corresponding confusion matrices. Each dissimilarity matrix was transformed to a cross-product matrix and normalized by the first eigenvalue.</p>
<a id="article1.body1.sec2.sec9.p2" name="article1.body1.sec2.sec9.p2"></a><p>A compromise matrix, representing the agreement across participants, was constructed as a weighted average of all the participants' regions-by-regions cross-product matrices. Participants' weights were computed from the first principal component of the participants-by-participants similarity matrix (the first principal component is proportional to the mean of the participant matrices). Each entry in the participants-by-participants similarity matrix was computed by the RV-coefficient <a href="#pone.0001394-Robert1">[16]</a>, which is a multivariate extension of the Pearson correlation coefficient, and indicates the overall similarity of the two matrices:<a name="" id=""></a><span class="equation"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001394.e002&amp;representation=PNG" class="inline-graphic"></span><br>The RV-coefficient has been previously used in the fMRI literature <a href="#pone.0001394-Kherif1">[17]</a>, <a href="#pone.0001394-Shinkareva1">[18]</a>. The compromise matrix was further analyzed by principal components analysis.</p>


<h4>Multiple participant analysis</h4>
<a id="article1.body1.sec2.sec10.p1" name="article1.body1.sec2.sec10.p1"></a><p>Data from all but one participant were used to train a classifier to identify the data from the left-out participant. This process was repeated so that it reiteratively left out each of the participants. Feature selection was done by pooling the data of all participants but the one left out. Discriminating voxel subsets of sizes 10, 25, 50, 75, 100, 200, 400, 1000, and 2000 were selected on the basis of logistic regression weights.</p>

</div>

<div id="section3" class="section"><a id="s3" name="s3" toc="s3" title="Results"></a><h3>Results</h3>
<h4>Identifying object exemplars: whole brain</h4>
<a id="article1.body1.sec3.sec1.p1" name="article1.body1.sec3.sec1.p1"></a><p>The highest rank accuracy achieved for any participant while identifying individual object exemplars was 0.94. (The identification process obtained this rank accuracy by correctly identifying the object on its first-ranked guess in 40 out of 60 presentations, on its second-ranked guess in 10 presentations, and on its third- and fourth-ranked guesses in 10 other presentations.) Reliable (p&lt;0.001) classification accuracy for individual object exemplars was reached for eleven out of twelve participants (as shown by the filled bars in <a href="#pone-0001394-g002">Figure 2</a>). The mean classification rank accuracy over all 12 participants was 0.78 (SD = 0.11).</p>
<div class="figure" id="pone-0001394-g002"><div class="img"><a name="pone-0001394-g002" title="Click for larger image " href="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001394.g002&amp;representation=PNG_M" data-doi="info:doi/10.1371/journal.pone.0001394" data-uri="info:doi/10.1371/journal.pone.0001394.g002"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001394.g002&amp;representation=PNG_I" alt="thumbnail" class="thumbnail"></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001394.g002/powerpoint">
                    PPT
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001394.g002/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001394.g002/largerimage">
                    PNG
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001394.g002/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pone.0001394.g002.PNG_L"></span>)
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001394.g002/originalimage">
                    TIFF
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001394.g002/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pone.0001394.g002.TIF"></span>)
