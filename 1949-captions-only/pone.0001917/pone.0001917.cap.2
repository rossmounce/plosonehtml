                </a></li></ul></div><p><strong>Figure 2.  <span>Long-term learning elements of strategy update rules help, while a low level of randomness relatively stabilizes cooperation in Prisoner's Dilemma games played on various networks.</span></strong></p><a id="article1.body1.sec2.sec2.fig1.caption1.p1" name="article1.body1.sec2.sec2.fig1.caption1.p1"></a><p>Small-world (SW, filled, red symbols) networks were built as described in the legend of <a href="#pone-0001917-g001">Figure 1</a>. The Barabasi-Albert-type scale-free networks (SF, open, blue symbols) contained 2,500 nodes, where at each construction step a new node was added with 3 new links attached to the existing nodes. For the description of the canonical repeated Prisoner's Dilemma game, as well as that of the best-takes-over (triangles, all panels), the Q-learning (rectangles, top panel) the best-takes-over long (circles, middle panel), and the best-takes-over long innovative (crosses, P<sub>innovation</sub> = 0.0002, bottom panel) strategy adoption rules, see <a href="#s4">Methods</a> and the ESM1. For each strategy adoption rules and <em>T</em> temptation values 100 random runs of 5,000 time steps were executed. The figure shows that long-term, ‘learning-type’ elements of strategy update rules help cooperation in Prisoner's Dilemma games played on various networks. A low level of randomness (also called as ‘innovation’ in this paper) brings the level of cooperation closer in different network topologies.</p>
