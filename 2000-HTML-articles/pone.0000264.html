

 



<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"
      xmlns:foaf="http://xmlns.com/foaf/0.1/"
      xmlns:dc="http://purl.org/dc/terms/"
      xmlns:doi="http://dx.doi.org/"
      xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
      xmlns:xsd="http://www.w3.org/2001/XMLSchema-datatypes#"
      lang="en" xml:lang="en"
      itemscope itemtype="http://schema.org/Article"
      class="no-js">
<head prefix="og: http://ogp.me/ns#">
  <title>PLOS ONE: Spatio-Temporal Interpolation Is Accomplished by Binocular Form and Motion Mechanisms</title>


<link rel="stylesheet" type="text/css"  href="/css/global-min.css?v=izteQ6tu7kgsJZW_xmrYizvKiHM" />


    <!--[if lte IE 7]>
<link rel="stylesheet" type="text/css"  href="/css/lte_ie7-min.css?v=3bykQUyQmReeuobVyPozcJ9LxRc" />
    <![endif]-->


<link rel="stylesheet" type="text/css"  href="/css/jquery-ui-min.css?v=eXDHTEJM0lIAmDe5k0I0Ad4nxNo" />


<link rel="stylesheet" type="text/css"  href="/css/journal.css?v=T7ZVxJfgk9jNxLAJ2qHz1vZpgYU" />


<link rel="stylesheet" type="text/css" media="print" href="/css/print-min.css?v=T5lb0B3q6EXBsuDluc5V5w+AkRc" />


  <link rel="stylesheet" href="http://f.fontdeck.com/s/css/js/www.plosone.org/24557.css" type="text/css"/>

  <!--chartbeat -->
  <script type="text/javascript">var _sf_startpt = (new Date()).getTime()</script>
  <script>document.documentElement.className += ' js';</script>

  <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <meta http-equiv="X-UA-Compatible" content="IE=EmulateIE7; IE=EmulateIE9"/>
  <meta name="description" content="PLOS ONE: an inclusive, peer-reviewed, open-access resource from the PUBLIC LIBRARY OF SCIENCE. Reports of well-performed scientific studies from all disciplines freely available to the whole world."/>
  <meta name="keywords" content="PLOS, Public Library of Science, Open Access, Open-Access, Science, Medicine, Biology, Research, Peer-review, Inclusive, Interdisciplinary, Ante-disciplinary, Physics, Chemistry, Engineering"/>
  <meta name="almHost" content="http://alm.plos.org/api/v3/articles"/>
  <meta name="searchHost" content="http://api.plos.org/search" />
  <meta name="termsHost" content="http://api.plos.org/terms" />
  <meta name="solrApiKey" content="plos"/>
  <meta name="almAPIKey" content="3pezRBRXdyzYW6ztfwft" />
  <meta name="currentJournal" content="PLoSONE" />
  <meta name="almRequestBatchSize" content="" />

  <meta name="citation_publisher" content="Public Library of Science"/>
  <meta name="citation_doi" content="10.1371/journal.pone.0000264"/>
  <meta name="dc.identifier" content="10.1371/journal.pone.0000264" />

    <meta name="citation_title" content="Spatio-Temporal Interpolation Is Accomplished by Binocular Form and Motion Mechanisms"/>
    <meta itemprop="name" content="Spatio-Temporal Interpolation Is Accomplished by Binocular Form and Motion Mechanisms"/>

      <meta name="citation_author" content="Farid I. Kandil"/>
            <meta name="citation_author_institution" content="Department of General Psychology, Westfälische Wilhelms University of Münster, Münster, Germany"/>
      <meta name="citation_author" content="Markus Lappe"/>
            <meta name="citation_author_institution" content="Department of General Psychology, Westfälische Wilhelms University of Münster, Münster, Germany"/>

    <meta name="citation_date" content="2007/2/28"/>

  <meta name="citation_pdf_url" content="http://dx.plos.org/10.1371/journal.pone.0000264.pdf" />

      <meta name="citation_journal_title" content="PLOS ONE" />
    <meta name="citation_firstpage" content="e264"/>
    <meta name="citation_issue" content="2"/>
    <meta name="citation_volume" content="2"/>
    <meta name="citation_issn" content="1932-6203"/>

    <meta name="citation_journal_abbrev" content="PLoS ONE" />

      <meta name="citation_reference" content="citation_title=Distortions in moving figures viewed through a stationary slit.; citation_author=SM Anstis; citation_author=J Atkinson; citation_journal_title=Am J Psychol; citation_volume=80; citation_number=1; citation_pages=572-585; citation_date=1967; " />
      <meta name="citation_reference" content="citation_title=Acuity for apparent vernier offset.; citation_author=DC Burr; citation_journal_title=Vision Res; citation_volume=19; citation_number=2; citation_pages=835-837; citation_date=1979; " />
      <meta name="citation_reference" content="citation_title=Visual hyperacuity:spatiotemporal interpolation in human vision.; citation_author=M Fahle; citation_author=T Poggio; citation_journal_title=Proc R Soc Lond B Biol Sci; citation_volume=213; citation_number=3; citation_pages=451-477; citation_date=1981; " />
      <meta name="citation_reference" content="citation_title=Post-retinal visual storage.; citation_author=TE Parks; citation_journal_title=Am J Psychol; citation_volume=78; citation_number=4; citation_pages=145-147; citation_date=1965; " />
      <meta name="citation_reference" content="citation_title=Ueber eine neue Art anorthoskopischer Zerrbilder.; citation_author=F Zöllner; citation_journal_title=Annalen der Physik und Chemie; citation_volume=117; citation_number=5; citation_pages=477-484; citation_date=1862; " />
      <meta name="citation_reference" content="citation_title=On the failure of spatiotemporal interpolation: a filtering model.; citation_author=MJ Morgan; citation_author=RJ Watt; citation_journal_title=Vision Res; citation_volume=23; citation_number=6; citation_pages=997-1004; citation_date=1983; " />
      <meta name="citation_reference" content="citation_title=Motion-based analysis of spatial patterns by the human visual system.; citation_author=S Nishida; citation_journal_title=Curr Biol; citation_volume=14; citation_number=7; citation_pages=830-839; citation_date=2004; " />
      <meta name="citation_reference" content="citation_title=Visual processing of motion.; citation_author=DC Burr; citation_author=J Ross; citation_journal_title=Trends Neurosci; citation_volume=9; citation_number=8; citation_pages=304-307; citation_date=1986; " />
      <meta name="citation_reference" content="citation_title=How does binocular delay give information about depth?; citation_author=DC Burr; citation_author=J Ross; citation_journal_title=Vision Res; citation_volume=19; citation_number=9; citation_pages=523-532; citation_date=1979; " />
      <meta name="citation_reference" content="citation_title=Perception of continuity in stroboscopic motion: a temporal frequency analysis.; citation_author=MJ Morgan; citation_journal_title=Vision Res; citation_volume=19; citation_number=10; citation_pages=491-500; citation_date=1979; " />
      <meta name="citation_reference" content="citation_title=Apparent motion and the Pulfrich effect.; citation_author=MJ Morgan; citation_author=P Thompson; citation_journal_title=Perception; citation_volume=4; citation_number=11; citation_pages=3-18; citation_date=1975; " />
      <meta name="citation_reference" content="citation_title=On the failure of spatiotemporal interpolation: a filtering model.; citation_author=MJ Morgan; citation_author=RJ Watt; citation_journal_title=Vision Res; citation_volume=23; citation_number=12; citation_pages=997-1004; citation_date=1983; " />
      <meta name="citation_reference" content="citation_title=Seeing objects in motion.; citation_author=DC Burr; citation_author=J Ross; citation_author=MC Morrone; citation_journal_title=Proc R Soc Lond B Biol Sci; citation_volume=227; citation_number=13; citation_pages=249-265; citation_date=1986; " />
      <meta name="citation_reference" content="citation_title=Spatiotemporal energy models for the perception of motion.; citation_author=EH Adelson; citation_author=JR Bergen; citation_journal_title=J Opt Soc Am A; citation_volume=2; citation_number=14; citation_pages=284-299; citation_date=1985; " />
      <meta name="citation_reference" content="citation_title=Elaborated Reichardt detectors.; citation_author=JP van Santen; citation_author=G Sperling; citation_journal_title=J Opt Soc Am A; citation_volume=2; citation_number=15; citation_pages=300-321; citation_date=1985; " />
      <meta name="citation_reference" content="citation_title=Model of human visual-motion sensing.; citation_author=AB Watson; citation_author=AJJ Ahumada; citation_journal_title=J Opt Soc Am A; citation_volume=2; citation_number=16; citation_pages=322-341; citation_date=1985; " />
      <meta name="citation_reference" content="citation_title=The functional architecture of human visual motion perception.; citation_author=ZL Lu; citation_author=G Sperling; citation_journal_title=Vision Res; citation_volume=35; citation_number=17; citation_pages=2697-2722; citation_date=1995; " />
      <meta name="citation_reference" content="citation_title=Motion contrast: a new metric for direction discrimination.; citation_author=MA Georgeson; citation_author=NE Scott-Samuel; citation_journal_title=Vision Res; citation_volume=39; citation_number=18; citation_pages=4393-4402; citation_date=1999; " />
      <meta name="citation_reference" content="citation_title=Spatio-temporal interpolation in depth.; citation_author=M Fahle; citation_author=E De Luca; citation_journal_title=Vision Res; citation_volume=34; citation_number=19; citation_pages=343-348; citation_date=1994; " />
      <meta name="citation_reference" content="citation_title=Evidence for an early motion system which integrates information from the two eyes.; citation_author=T Carney; citation_journal_title=Vision Res; citation_volume=37; citation_number=20; citation_pages=2361-2368; citation_date=1997; " />
      <meta name="citation_reference" content="citation_title=On the elementary mechanism underlying secondary motion processing.; citation_author=JM Zanker; citation_journal_title=Philos Trans R Soc Lond B Biol Sci; citation_volume=351; citation_number=21; citation_pages=1725-1736; citation_date=1996; " />
      <meta name="citation_reference" content="citation_author=BG Breitmeyer; citation_author=H Ögmen; citation_number=22; citation_pages=viii, 370 pp; citation_date=2006; citation_publisher=Oxford University Press; " />
      <meta name="citation_reference" content="citation_title=Deficits in visual motion processing following ibotenic acid lesions of the middle temporal visual area of the macaque monkey.; citation_author=WT Newsome; citation_author=RH Wurtz; citation_author=MR Dursteler; citation_author=A Mikami; citation_journal_title=J Neurosci; citation_volume=5; citation_number=23; citation_pages=825-840; citation_date=1985; " />
      <meta name="citation_reference" content="citation_title=Center-surround inhibition deepens binocular rivalry suppression.; citation_author=CL Paffen; citation_author=D Alais; citation_author=FA Verstraten; citation_journal_title=Vision Res; citation_volume=45; citation_number=24; citation_pages=2642-2649; citation_date=2005; " />

  <link rel="canonical" href="http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0000264" />

    <meta name="twitter:card" content="summary"/>
    <meta name="twitter:site" content="@plosone"/>
    <meta name="twitter:title" content="Spatio-Temporal Interpolation Is Accomplished by Binocular Form and Motion Mechanisms"/>
    <meta name="twitter:description" content="Spatio-temporal interpolation describes the ability of the visual system to perceive shapes as whole figures (Gestalts), even if they are moving behind narrow apertures, so that only thin slices of them meet the eye at any given point in time. The interpolation process requires registration of the form slices, as well as perception of the shape's global motion, in order to reassemble the slices in the correct order. The commonly proposed mechanism is a spatio-temporal motion detector with a receptive field, for which spatial distance and temporal delays are interchangeable, and which has generally been regarded as monocular. Here we investigate separately the nature of the motion and the form detection involved in spatio-temporal interpolation, using dichoptic masking and interocular presentation tasks. The results clearly demonstrate that the associated mechanisms for both motion and form are binocular rather than monocular. Hence, we question the traditional view according to which spatio-temporal interpolation is achieved by monocular first-order motion-energy detectors in favour of models featuring binocular motion and form detection."/>
      <meta name="twitter:image" content="http://dx.plos.org/10.1371/journal.pone.0000264.g003"/>

  <meta property="og:title" content="Spatio-Temporal Interpolation Is Accomplished by Binocular Form and Motion Mechanisms" />
  <meta property="og:type" content="article" />
  <meta property="og:url" content="http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0000264" />

 <!--end articleInfoX-->

  <link rel="pingback" href="http://www.plosone.org/pingback" />


  <link rel="shortcut icon" href="/images/favicon.ico" type="image/x-icon"/>
  <link rel="home" title="home" href="/"/>
  <link rel="alternate" type="application/rss+xml"
        title="PLOS ONE: New Articles"
        href="http://www.plosone.org/article/feed"/>
</head>
<body>

  <div id="page-wrap">
    <div id="topbanner" class="cf">

<!-- Div for the ad at the top of journal home page-->
<div class="center">
  <div class="title">Advertisement</div>
  <iframe id='a3ac9da4' name='a3ac9da4'
    src='http://ads.plos.org/www/delivery/afr.php?zoneid=345&amp;cb=873'
    frameborder='0' scrolling='no' width='730' height='90'>
    <a href='http://ads.plos.org/www/delivery/ck.php?n=a3ac9da4&amp;cb=4975'
      target='_top'><img src='http://ads.plos.org/www/delivery/avw.php?zoneid=345&amp;cb=5467&amp;n=a3ac9da4'
      border='0' alt=''/>
    </a>
  </iframe>
</div>    </div>

    <div id="pagehdr-wrap">
      <div id="pagehdr">
        <div id="user" class="nav">
          <ul>
            <li><a href="http://www.plos.org">plos.org</a></li>
            <li><a href="https://register.plos.org/ambra-registration/register.action">create account</a></li>
            <li class="btn-style"><a
              href="/user/secure/secureRedirect.action?goTo=%2Farticle%2FfetchArticle.action%3FarticleURI%3Dinfo%253Adoi%252F10.1371%252Fjournal.pone.0000264">sign in</a>
            </li>
          </ul>
        </div>
        <div class="logo">
          <a href="/"><img src="/images/logo.png" alt="PLOS ONE"></a>
        </div>

<div id="nav-main" class="nav">
  <ul>
        <li id="mn-01"><a href="/taxonomy" class="areas-link">Subject Areas</a></li>
    <li id="mn-02"><a href="javascript:void(0);">For Authors</a>
      <div class="submenu" style="width: 540px; margin-left: -300px;">
        <div class="block">
          <div class="submit-script">
            <h3>Submit your Manuscript</h3>
            <ul>
              <li>Fair, rigorous peer review</li>
              <li>Broad scope and wide reach</li>
            </ul>
            <a href="/static/submissionInstructions" class="btn">get started</a>
          </div>
        </div>
        <div class="menu">
          <ul>
            <li><a href="/static/publish">Why Publish with PLOS ONE</a></li>
            <li><a href="/static/publication">Publication Criteria</a></li>
            <li><a href="/static/editorial">Editorial Policies</a></li>
            <li><a href="/static/guidelines">Preparing A Manuscript</a></li>
            <li><a href="/static/figureGuidelines">Figure and Table Guidelines</a></li>
          <li><a href="/static/supportingInformation">Supporting Information Guidelines</a></li>
            <li><a href="/static/submissionInstructions">Submitting a Manuscript</a></li>
          </ul>
        </div>
      </div>
    </li>

    <li id="mn-03"><a href="javascript:void(0);">About Us</a>
      <div class="submenu" style="width:248px; margin-left:-30px;">
        <div class="menu">
          <ul>
            <li><a href="/static/information">Journal Information</a></li>
            <li><a href="/static/edboard">Editorial Board</a></li>
            <li><a href="/static/reviewerGuidelines">Reviewer Guidelines</a></li>
            <li><a href="/static/almInfo">Article-Level Metrics</a></li>
            <li><a href="/static/license">Open-Access License</a></li>
            <li><a href="/static/downloads">Media Downloads</a></li>
            <li><a href="/static/commentGuidelines">Guidelines for Comments</a></li>
            <li><a href="/static/corrections">Corrections</a></li>
            <li><a href="/static/help">Help Using this Site</a></li>
            <li><a href="/static/contact">Contact Us</a></li>
          </ul>
        </div>
      </div>
    </li>
  </ul>
<div id="db">
  <form name="searchForm" action="/search/simple?noSearchFlag=true&amp;query=&amp;articleURI=info%3Adoi%2F10.1371%2Fjournal.pone.0000264" method="get" >
<input type="hidden" name="from" value="globalSimpleSearch" id="from"/><input type="hidden" name="filterJournals" value="PLoSONE" id="filterJournals"/>    <fieldset>
      <legend>Search</legend>
      <label for="search">Search</label>
      <div class="wrap">
        <input id="search" type="text" name="query" placeholder="Search">
        <input type="image" alt="SEARCH" src="/images/icon.search.gif">
      </div>
    </fieldset>
  </form>
    <a id="advSearch" href="/search/advanced?noSearchFlag=true&amp;query=&amp;articleURI=info%3Adoi%2F10.1371%2Fjournal.pone.0000264&filterJournals=PLoSONE">advanced search</a>
</div></div>

      </div>
      <!-- pagehdr-->
    </div>
    <!-- pagehdr-wrap -->

  <!--body and html tags gets closed in global_footer.ftl-->
<script type="text/javascript" src="/javascript/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<div id="pagebdy-wrap">
  <div id="pagebdy">

    <div id="article-block" class="cf">

<div class="article-meta cf">
  <ul id="almSignPost" style="display: none;"></ul>
  <div class="article-type">
    <span class="type oa">Open Access</span>
      <span class="type pr">Peer-Reviewed</span>
  </div>
</div>

<div class="header" id="hdr-article">

<div class="article-kicker">
      <span id="article-type-heading">
        Research Article
      </span>
</div>  <h1 property="dc:title" datatype="" rel="dc:type" href="http://purl.org/dc/dcmitype/Text">
    Spatio-Temporal Interpolation Is Accomplished by Binocular Form and Motion Mechanisms
  </h1>

  <ul class="authors">
      <li>


        <span rel="dc:creator" class="author">
          <span class="person" property="foaf:name" typeof="foaf:Person">
            Farid I. Kandil
              <span class="corresponding">mail</span>, 
          </span>
        </span>

          <div class="author_meta">
            <div class="author_inner">


              
              <p><span class="email">*</span>To whom correspondence should be addressed. E-mail: <a href="mailto:kandil@uni-muenster.de">kandil@uni-muenster.de</a></p>

                <p>Affiliation:
                  Department of General Psychology, Westfälische Wilhelms University of Münster, Münster, Germany
                </p>


              <span class="close">X</span>

            </div>
          </div>
      </li>
      <li>


        <span rel="dc:creator" class="author">
          <span class="person" property="foaf:name" typeof="foaf:Person">
            Markus Lappe
          </span>
        </span>

          <div class="author_meta">
            <div class="author_inner">


              
              

                <p>Affiliation:
                  Department of General Psychology, Westfälische Wilhelms University of Münster, Münster, Germany
                </p>


              <span class="close">X</span>

            </div>
          </div>
      </li>
  </ul>
  <ul class="date-doi-line">
    <li>Published: February 28, 2007</li>
    <li>DOI: 10.1371/journal.pone.0000264</li>
  </ul>


</div><!--end header-->
<div class="main cf" id="pjax-container">
  

<div class="nav items-5" id="nav-article">
  <ul>
  <li>
        <span class="active" name="article">Article</span>
  </li>
  <li>
      <a href="/article/authors/info%3Adoi%2F10.1371%2Fjournal.pone.0000264" name="authors">About the Authors</a>
  </li>
  <li>
      <a href="/article/metrics/info%3Adoi%2F10.1371%2Fjournal.pone.0000264" name="metrics">Metrics</a>
  </li>
  <li>
      <a href="/article/comments/info%3Adoi%2F10.1371%2Fjournal.pone.0000264" name="comments">Comments</a>
  </li>
  <li>
      <a href="/article/related/info%3Adoi%2F10.1371%2Fjournal.pone.0000264" name="related">Related Content</a>
  </li>
  </ul>
</div>

<script type="text/javascript">
  var selected_tab = "article";
</script>
  <div id="figure-thmbs" class="carousel cf">
    <div class="wrapper">
      <div class="slider">
              <div class="item">
                <a href="#pone-0000264-g001" data-doi="info:doi/10.1371/journal.pone.0000264" data-uri="info:doi/10.1371/journal.pone.0000264.g001" title="Figure 1">
                  <span class="thmb-wrap">
                    <img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000264.g001&representation=PNG_I" alt="">
                  </span>
                </a>
              </div>
              <div class="item">
                <a href="#pone-0000264-g002" data-doi="info:doi/10.1371/journal.pone.0000264" data-uri="info:doi/10.1371/journal.pone.0000264.g002" title="Figure 2">
                  <span class="thmb-wrap">
                    <img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000264.g002&representation=PNG_I" alt="">
                  </span>
                </a>
              </div>
              <div class="item">
                <a href="#pone-0000264-g003" data-doi="info:doi/10.1371/journal.pone.0000264" data-uri="info:doi/10.1371/journal.pone.0000264.g003" title="Figure 3">
                  <span class="thmb-wrap">
                    <img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000264.g003&representation=PNG_I" alt="">
                  </span>
                </a>
              </div>
      </div>
    </div>
  </div>

  <div class="nav-col">
    <div class="nav" id="nav-article-page">
      <ul>
        <li class="nav-col-comments"><a href="/article/comments/info%3Adoi%2F10.1371%2Fjournal.pone.0000264">Reader Comments (0)</a></li>
          <li id="nav-figures"><a data-doi="info:doi/10.1371/journal.pone.0000264" >Figures</a></li>
      </ul>
    </div>
  </div>

  <div class="article">







<div class="abstract"><a id="abstract0" name="abstract0" toc="abstract0" title="Abstract"></a><h2>Abstract</h2><a id="article1.front1.article-meta1.abstract1.p1" name="article1.front1.article-meta1.abstract1.p1"></a><p>Spatio-temporal interpolation describes the ability of the visual system to perceive shapes as whole figures (Gestalts), even if they are moving behind narrow apertures, so that only thin slices of them meet the eye at any given point in time. The interpolation process requires registration of the form slices, as well as perception of the shape's global motion, in order to reassemble the slices in the correct order. The commonly proposed mechanism is a spatio-temporal motion detector with a receptive field, for which spatial distance and temporal delays are interchangeable, and which has generally been regarded as monocular. Here we investigate separately the nature of the motion and the form detection involved in spatio-temporal interpolation, using dichoptic masking and interocular presentation tasks. The results clearly demonstrate that the associated mechanisms for both motion and form are binocular rather than monocular. Hence, we question the traditional view according to which spatio-temporal interpolation is achieved by monocular first-order motion-energy detectors in favour of models featuring binocular motion and form detection.</p>
</div>


<div class="articleinfo"><p><strong>Citation: </strong>Kandil FI, Lappe M (2007) Spatio-Temporal Interpolation Is Accomplished by Binocular Form and Motion Mechanisms. PLoS ONE 2(2):
          e264.
            doi:10.1371/journal.pone.0000264</p><p><strong>Academic Editor: </strong>Sheng He, University of Minnesota, United States of America</p><p><strong>Received:</strong> December 22, 2006; <strong>Accepted:</strong> February 9, 2007; <strong>Published:</strong> February 28, 2007</p><p><strong>Copyright:</strong> © 2007 Kandil, Lappe. This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p><p><strong>Funding: </strong>M.L. is supported by the German Science Foundation, the German Federal Ministry of Education and Research, and the EC Project Drivsco. None of these interfered with the design and conduct of the study, the collection, analysis, and interpretation of the data, and the preparation, review, or approval of the manuscript.</p><p><strong>Competing interests:</strong> The authors have declared that no competing interests exist.</p></div>





<div id="section1" class="section"><a id="s1" name="s1" toc="s1" title="Introduction"></a><h3>Introduction</h3><a id="article1.body1.sec1.p1" name="article1.body1.sec1.p1"></a><p>Spatio-temporal interpolation is one of those integrative components underlying our visual experience so perfectly that one would hardly ever think that there is a problem at all. When a car is parked behind a picket fence one can see only a number of narrow stripes of it through the slits. However, when the car starts moving, the percept changes drastically. Rather than seeing a series of successive narrow views of the car, all emerging spatially from the same slits in the fence, what we perceive is a car interpolated in space and time, i.e. a constantly visible unsegregated whole object <a href="#pone.0000264-Anstis1">[e.g. refs. 1]</a>–<a href="#pone.0000264-Zllner1">[5]</a>.</p>
<a id="article1.body1.sec1.p2" name="article1.body1.sec1.p2"></a><p>Two rival mechanisms, ‘retinal painting’ and ‘spatio-temporal receptive fields’, have been proposed to explain how the visual system recombines the incoming slit-views into a complete image again. Retinal painting theory states that the eyes follow the (global) motion of the car thereby placing incoming slit-views of the car next to one another in the original order on the retina. In contrast, spatio-temporal receptive field theory assumes the existence of receptive fields oriented in space-time. For these receptive fields, space and time are to a certain extent interchangeable, allowing objects that appear <em>delayed in time</em> to be considered <em>displaced in space</em>. The receptive fields would then place the incoming slit-views into the correct order by means of internal computations rather than external eye movements. Despite their great differences, both theories rely on the correct detection of both form (the series of views) and motion direction in order to link neighbouring views in the right order. Here we investigate whether the underlying mechanism is monocular or binocular by probing the motion and form information it can use.</p>
<a id="article1.body1.sec1.p3" name="article1.body1.sec1.p3"></a><p>Throughout the history of spatio-temporal interpolation displays, various kinds of stimulus configurations have been used. In the first series of experiments, objects like letters, geometric figures and animals crossed a single but wide slit or ‘aperture’ <a href="#pone.0000264-Anstis1">[e.g. refs. 1]</a>, <a href="#pone.0000264-Parks1">[ 4]</a>–<a href="#pone.0000264-Morgan1">[6]</a>. In these experiments, both motion direction and local form information emerges from within the one single slit. In the second kind of display, large figures are visible through a number of equally spaced narrow slits <a href="#pone.0000264-Nishida1">[7]</a>, <a href="#pone.0000264-Burr2">[8]</a> allowing subjects to perceive the global contour of the form in most single frames of the presentation. Although each slit is too narrow to detect the motion direction <em>within</em> it, subjects readily perceive the motion direction of the global form that they have already detected.</p>
<a id="article1.body1.sec1.p4" name="article1.body1.sec1.p4"></a><p>The third kind of display presents multi-slit views in combination with objects that are smaller than the distance between two slits <a href="#pone.0000264-Burr1">[e.g. refs. 2]</a>, <a href="#pone.0000264-Fahle1">[3]</a>, <a href="#pone.0000264-Burr2">[8]</a>, <a href="#pone.0000264-Burr3">[9]</a>, <a href="#pone.0000264-Morgan2">[see also refs. 10]</a>, <a href="#pone.0000264-Morgan3">[11]</a>. Here, objects are visible through only a single slit at any given point in time, while the global motion direction can be deduced only from the course the objects take between slits. This disentanglement of form and motion cues allowed us to investigate the nature of the associated form and motion detection separately. For each cue, form and motion, we tested whether the interpolation process uses either (i) only monocular, (ii) only binocular or (iii) monocular and binocular information. We used dichoptic masking and interocular completion stimuli to differentiate between these alternatives.</p>
<a id="article1.body1.sec1.p5" name="article1.body1.sec1.p5"></a><p>In the critical conditions of the <em>Dichoptic Masking</em> experiments (1A, 1B &amp; 2A), stimuli are presented dichoptically to the two eyes. Each monocular view for itself contains valid and sufficient information. However, the two monocular views are constructed in such a way that they mask each other when fused into a binocular view and provide thus only ambivalent information to any purely binocular processor.</p>
<a id="article1.body1.sec1.p6" name="article1.body1.sec1.p6"></a><p>In the critical conditions of the <em>Interocular Completion</em> experiments (1C &amp; 2B), stimuli are also presented dichoptically. However, there monocular views by themselves do not carry any valid information, thereby excluding monocular processes from the interpolation. Rather, the valid information can only be obtained by binocular interpolation mechanisms, that is after the monocular views have been summed across eyes into a binocular view.</p>
<a id="article1.body1.sec1.p7" name="article1.body1.sec1.p7"></a><p>The results obtained here clearly show that only binocular form and motion mechanisms are involved, whereas monocular information does not provide any significant contribution as to the interpolation process. This questions the previously mentioned hypothesis according to which interpolation is computed by a unitary monocular spatio-temporal mechanism <a href="#pone.0000264-Fahle1">[3]</a>, <a href="#pone.0000264-Burr2">[8]</a>.</p>
</div>

<div id="section2" class="section"><a id="s2" name="s2" toc="s2" title="Methods"></a><h3>Methods</h3>
<h4>Standard Stimulus</h4>
<a id="article1.body1.sec2.sec1.p1" name="article1.body1.sec2.sec1.p1"></a><p>The layout of the standard stimulus is depicted in <a href="#pone-0000264-g001">Figure 1</a>. The stimulus (<a href="#pone-0000264-g001">Fig. 1A</a>) consists of a number of arrowheads, all equal in size (3 dots wide and 5 high) and all pointing in the same direction (left or right). It is moved dot-by-dot behind a slit mask (<a href="#pone-0000264-g001">Fig. 1B</a>) with slits regularly spread every twelve dots. These slits are only one dot wide, hence allowing only one (vertical) pair of dots of the arrowheads to be seen within one slit at any given point in time. Further, arrowheads are equidistant from one another with the midpoint-to-midpoint distance being integer multiples of the space between two slits, i.e. the ‘inter-slit distance’. Thus, the amount of information does not increase with the number of arrowheads visible at the same time. Presentation of more than one arrowhead was necessary in order to facilitate dichoptic masking and interocular completion paradigms.</p>
<div class="figure" id="pone-0000264-g001"><div class="img"><a name="pone-0000264-g001" title="Click for larger image " href="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000264.g001&amp;representation=PNG_M" data-doi="info:doi/10.1371/journal.pone.0000264" data-uri="info:doi/10.1371/journal.pone.0000264.g001"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000264.g001&amp;representation=PNG_I" alt="thumbnail" class="thumbnail"></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000264.g001/powerpoint">
                    PPT
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000264.g001/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000264.g001/largerimage">
                    PNG
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000264.g001/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pone.0000264.g001.PNG_L"></span>)
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000264.g001/originalimage">
                    TIFF
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000264.g001/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pone.0000264.g001.TIF"></span>)
                </a></li></ul></div><p><strong>Figure 1.  <span>Layout of the standard stimulus.</span></strong></p><a id="article1.body1.sec2.sec1.fig1.caption1.p1" name="article1.body1.sec2.sec1.fig1.caption1.p1"></a><p>The standard stimulus consists of a number of regularly arranged arrowheads (A), all of which pointed either to the left or to the right. This band was shifted dot-wise either to the left or to the right behind a mask (B), which in turn comprised regularly spread slits of 1 dot width. Distances between arrowheads in the stimulus band were integer multiples of the distance between slits; so same information was visible simultaneously within various slits. (C) During the crossing of one slit, only a small fraction of the arrowheads is physically visible at each point in time. Depending on the arrowhead orientation as well as on the motion direction, the two dots visible within the slits either diverged (as shown here) or converged. (D) Across multiple slit crossings the global motion direction emerged. Both, global cues between slits and local cues within slits must be registered in order to obtain the correct response, i.e. the original arrowhead orientation.</p>
<span>doi:10.1371/journal.pone.0000264.g001</span></div><a id="article1.body1.sec2.sec1.p2" name="article1.body1.sec2.sec1.p2"></a><p>The first five frames (frames 00 to 04) of an example sequence are shown in <a href="#pone-0000264-g001">Figure 1C</a>. A stimulus band with arrows pointing to the left is moved to the left behind the slit mask (cf. the example indicated in <a href="#pone-0000264-g001">Fig. 1A</a>). Before the arrows pass their first slit, the screen is completely dark (f 00 in <a href="#pone-0000264-g001">Fig. 1C</a>). In the next frame (f 01), only the first dot of each arrowhead is visible. Then the upper and the lower lines of the arrowheads appear by one dot each, with the dots moving further apart from one another (f 02 and f 03). After passing their first and before entering their second slit, the arrows are completely hidden by the mask and the screen becomes entirely dark again (frames 04 to 06). With subsequent frames, the arrows reach and pass subsequent slits (<a href="#pone-0000264-g001">Fig. 1D</a>).</p>


<h4>Ambiguity</h4>
<a id="article1.body1.sec2.sec2.p1" name="article1.body1.sec2.sec2.p1"></a><p>Each of the four alternatives shown in <a href="#pone-0000264-g001">Figure 1A</a> produces different motion and form cues both inside and between the slits. The global motion direction <em>between the slits</em> solely depends on the motion direction of the arrowheads. In contrast, the local cues, that is, whether the dots diverge or converge <em>within the slits</em> is determined by the combination of the motion direction and orientation of the arrowheads. Hence, in order for any mechanism to inversely obtain the original arrowhead orientation always both, local and global cues must be registered.</p>


<h4>Strategy and subject's task</h4>
<a id="article1.body1.sec2.sec3.p1" name="article1.body1.sec2.sec3.p1"></a><p>Each of the four experiments comprised four conditions, a critical dichoptic condition (‘dich’), a left and a right monocular control (‘monoL’ and ‘monoR’) and a binocular control (‘bino’). According to the requirements of the experiment, either only the monocular or only the binocular views of the dichoptic stimulus were reliable. In the monocular and binocular conditions, monocular or else binocular views of these stimuli were presented as controls.</p>
<a id="article1.body1.sec2.sec3.p2" name="article1.body1.sec2.sec3.p2"></a><p>In each experiment, twenty trials were presented per condition in a randomly interleaved manner. After each trial, subjects had to indicate the pointing direction of the arrowheads in a two-alternative forced-choice (2-AFC) task by pressing the according cursor key on a standard computer keyboard. They were told to guess the correct answer whenever they could not perceive single arrowheads.</p>
<a id="article1.body1.sec2.sec3.p3" name="article1.body1.sec2.sec3.p3"></a><p>A threshold of 75% (midway between perfect and chance level) distinguished between performance on chance and significant level. This is slightly more conservative than the adjusted threshold derived from the binomial distribution (70.9%).</p>
<a id="article1.body1.sec2.sec3.p4" name="article1.body1.sec2.sec3.p4"></a><p>Subjects were free to move their eyes. Eye movements were not recorded.</p>


<h4>Means of presenting monocular and dichoptic stimuli</h4>
<a id="article1.body1.sec2.sec4.p1" name="article1.body1.sec2.sec4.p1"></a><p>For monocular and dichoptic conditions, the visual input to the two eyes had to be separated. Both, red-green anaglyphs as well as LCD shutter goggles allow the presentation of both eyes' views on the same monitor. Red-green anaglyphs were used as a preferred means. As both eyes' views can be drawn on identical monitor frames, they allow a high temporal resolution (166 Hz), which is beneficial for spatio-temporal interpolation. However, in experiments 1A, 1B and 2A, colour differences between the stimuli presented to the two eyes might diminish any effects of interocular completion masking. Thus in these experiments LCD shutter goggles (CrytalEyes 3, StereoGraphics, San Rafael, CA, USA) were used, which allow the stimuli for both eyes to be maximally similar, thus preventing subjects from distinguishing between the two eyes' input simply by colour. A possible drawback would be that the images for the two eyes have to be presented on successive rather than identical frames and that thus the monocular frame rate is reduced by factor two. However, neither the reduced velocity caused by the reduced frame rate nor the resulting monocular flicker of 83 Hz had any negative impact on the subjects' perception of the interpolation stimulus (cf. the nearly perfect results for the monocular conditions in <a href="#pone-0000264-g002">Figs. 2A, B</a> and <a href="#pone-0000264-g003">3A</a>).</p>
<div class="figure" id="pone-0000264-g002"><div class="img"><a name="pone-0000264-g002" title="Click for larger image " href="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000264.g002&amp;representation=PNG_M" data-doi="info:doi/10.1371/journal.pone.0000264" data-uri="info:doi/10.1371/journal.pone.0000264.g002"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000264.g002&amp;representation=PNG_I" alt="thumbnail" class="thumbnail"></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000264.g002/powerpoint">
                    PPT
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000264.g002/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000264.g002/largerimage">
                    PNG
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000264.g002/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pone.0000264.g002.PNG_L"></span>)
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000264.g002/originalimage">
                    TIFF
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000264.g002/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pone.0000264.g002.TIF"></span>)
                </a></li></ul></div><p><strong>Figure 2.  <span>Motion detection.</span></strong></p><a id="article1.body1.sec2.sec4.fig1.caption1.p1" name="article1.body1.sec2.sec4.fig1.caption1.p1"></a><p>(A) In the critical dichoptic condition, two monocular sets of arrowheads with the same arrowhead orientation and motion direction are superimposed with a phase difference of 180 deg. Thus, only monocular but not binocular detectors are presented with unambiguous motion information. Results show clearly that spatiotemporal interpolation mechanisms have no access to monocular motion. Note that in this experiment and in 1B, all stimuli were always shown as red dots on dark ground and that colour coding is introduced here for the sake of clarity: Red, green and yellow mark stimuli presented to the right or left eye or by both eyes, respectively. (B) Two monocular sets of arrowhead stimuli are superimposed in the critical condition of this task. In both, arrowheads point to the same side but move into opposite directions. As a result, only interpolation mechanisms reading out monocular motion information can interpolate the stimuli correctly, while mechanisms relying only on binocular motion detection will perceive ambivalently oriented arrowheads. Stimuli for the four conditions (binocular, monocular left, monocular right and dichoptic) are shown along with the mask behind which they float. (C) Motion direction and hence correct arrowhead orientation can only be derived after the binocular fusion, while monocular motion information is always ambivalent. The stimulus is the same for all four conditions, whereas the mask differs. Here, the mask and the resulting global motion path are shown for the dichoptic condition. In the monocular conditions, exclusively either the red or else the green dots were visible, whereas in the binocular case, all dots were always visible by both eyes. Results for all three experiments are given as means ±1 s.e.m.</p>
<span>doi:10.1371/journal.pone.0000264.g002</span></div><div class="figure" id="pone-0000264-g003"><div class="img"><a name="pone-0000264-g003" title="Click for larger image " href="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000264.g003&amp;representation=PNG_M" data-doi="info:doi/10.1371/journal.pone.0000264" data-uri="info:doi/10.1371/journal.pone.0000264.g003"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000264.g003&amp;representation=PNG_I" alt="thumbnail" class="thumbnail"></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000264.g003/powerpoint">
                    PPT
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000264.g003/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000264.g003/largerimage">
                    PNG
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000264.g003/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pone.0000264.g003.PNG_L"></span>)
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000264.g003/originalimage">
                    TIFF
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000264.g003/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pone.0000264.g003.TIF"></span>)
                </a></li></ul></div><p><strong>Figure 3.  <span>Form detection.</span></strong></p><a id="article1.body1.sec2.sec4.fig2.caption1.p1" name="article1.body1.sec2.sec4.fig2.caption1.p1"></a><p>(A) Due to dichoptic masking, arrowhead orientations may only be detected by monocular form mechanisms, while binocular ones will only perceive ambivalent cues. Stimuli are shown for the four conditions (binocular, monocular left, monocular right and dichoptic). Note that here stimuli were always shown as red dots on dark ground and that colour coding appears here only for the sake of clarity. (B) Since monocular stimuli are incomplete, detection of the arrowheads in this experiment is reserved for binocular interpolation mechanisms which have access to the interocularly combined images. Results for all experiments are shown as means ±1 s.e.m.</p>
<span>doi:10.1371/journal.pone.0000264.g003</span></div>

<h4>Subjects</h4>
<a id="article1.body1.sec2.sec5.p1" name="article1.body1.sec2.sec5.p1"></a><p>Six subjects, between 26 and 33 of age, participated in all experiments. All were naive as to the purpose of the study and all had normal or corrected-to-normal visual acuity and normal stereopsis. Prior to the main experiment, subjects conducted a few practice trials with the standard stimulus depicted in <a href="#pone-0000264-g001">Figure 1</a>.</p>


<h4>Choice of parameters</h4>
<a id="article1.body1.sec2.sec6.p1" name="article1.body1.sec2.sec6.p1"></a><p>Fahle &amp; Poggio <a href="#pone.0000264-Fahle1">[3]</a> and Morgan &amp; Watt <a href="#pone.0000264-Morgan4">[12]</a> found that inter-slit latencies between 20 and 50 ms, inter-slit distances below 20 arcmin and velocities between 1 and 10 deg/sec are optimal for spatio-temporal interpolation, while Nishida <a href="#pone.0000264-Nishida1">[7]</a> used latencies of 80 ms between slits, an inter-slit distance of 32 arcmin and a velocity of 6.7 deg/sec.</p>
<a id="article1.body1.sec2.sec6.p2" name="article1.body1.sec2.sec6.p2"></a><p>In this study, we ran each experiment twice. In the small-scale version, slits were separated by 6 dots, i.e. a distance of 14.4 arcmin and a latency of 36 (red-green anaglyphs) or 72 ms (LCD shutters). In the large-scale version, the space between slits was doubled, resulting in a spatial separation of 12 dots or 28.8 arcmin and delays of 72 and 144 ms for red-green anaglyphs and LCD-shutters, respectively. Size, form and velocity of the arrowheads was kept equal in both versions.</p>


<h4>Presentation</h4>
<a id="article1.body1.sec2.sec7.p1" name="article1.body1.sec2.sec7.p1"></a><p>Stimuli were presented on a 21-inch colour CRT monitor (Iiyama MS102DT) driven with 166 Hz by an Apple Macintosh G4 computer via an ATI Radeon 9000 graphics board. Subjects were seated in a dimly lit room and watched the stimuli from a distance of 114 cm.</p>
<a id="article1.body1.sec2.sec7.p2" name="article1.body1.sec2.sec7.p2"></a><p>Consisting of 22 slits, the stimuli subtended 5.1 and 10.2 deg in width for the small and large-scale version, respectively and 0.8 deg in height. Single dots, as they were visible through the slit masks, measured 2.4 by 2.4 arcmin.</p>
<a id="article1.body1.sec2.sec7.p3" name="article1.body1.sec2.sec7.p3"></a><p>In experiments using red-green anaglyphs, red dots for the right eye and green dots for the left eye were presented. Both dots appeared bright with a luminance of 5.0 cd/m<sup>2</sup> against a dark background (0.05 cd/m<sup>2</sup>), resulting in a high Michelson contrast of 98%. There, stimuli were shown with 166 Hz, thus single trials (with 48 frames) lasted 288 ms and objects had a velocity of 6.6 deg/s. In binocular conditions, red and green dots were superimposed on top of each other, resulting in yellowish dots on the screen.</p>
<a id="article1.body1.sec2.sec7.p4" name="article1.body1.sec2.sec7.p4"></a><p>In experiments 1A, 1B and 2A LCD-shutter goggles were used, which caused the effective monocular refresh rate to be reduced to 83 Hz. As a result, the 48 frames lasted 576 ms and the object velocity was reduced to 3.3 deg/s. In these experiments dots were uniformly red (5.0 cd/m<sup>2</sup>). However, note that for the sake of easy comparability, in the figures stimuli are always depicted as red-green anaglyphs, even if they were actually presented in the experiments using LCD shutter goggles.</p>

</div>

<div id="section3" class="section"><a id="s3" name="s3" toc="s3" title="Results"></a><h3>Results</h3><a id="article1.body1.sec3.p1" name="article1.body1.sec3.p1"></a><p>Two experiments addressed the motion and the form aspect of the spatio-temporal interpolation mechanism individually. In the first experiment, in which motion detection was tested, the form cue was unambiguous, whereas in the second experiment, which tested the form information used, motion direction was unequivocal.</p>
<a id="article1.body1.sec3.p2" name="article1.body1.sec3.p2"></a><p>As mentioned above, all experiments followed the same logic, in that four conditions were tested. The dichoptic condition represents the critical test for the mechanism in question. In experiments 1A, 1B and 2A, stimuli were <em>dichoptically masked</em> and thus presented only monocular detectors with reliable information. In contrast, in experiments 1B and 2B, we presented stimuli that are <em>interocularly completing</em> each other, thereby providing only binocular mechanisms with sufficient cues.</p>
<a id="article1.body1.sec3.p3" name="article1.body1.sec3.p3"></a><p>Next to the critical dichoptic condition, we conducted monocular and binocular controls. In the monocular controls, only the left or right-eye image of the dichoptic stimulus was shown to the left or right eye, respectively; whereas in the binocular control, the full stimulus of the dichoptic condition was presented to both eyes alike.</p>

<h4>Experiment 1A: Dichoptic motion masking I</h4>
<a id="article1.body1.sec3.sec1.p1" name="article1.body1.sec3.sec1.p1"></a><p>Experiments 1A and 1B tested whether monocular detectors are involved in spatio-temporal interpolation by using dichoptically masked motion stimuli. While in experiment 1B monocular motion directions opposed each other and prevented thereby an unambiguous binocular motion percept, experiment 1A used spatial summation in order to mask motion direction.</p>
<a id="article1.body1.sec3.sec1.p2" name="article1.body1.sec3.sec1.p2"></a><p>In the two monocular control conditions of experiment 1A (‘monoR’ and ‘monoL’), the standard stimulus depicted in the <a href="#s2">Methods</a> section was projected either to only the left or the right eye, while the partner eye was presented with a dark screen. Arrowheads were separated by four times the inter-slit distance and the global motion direction hence could be derived unambiguously. In fact, subjects performed nearly perfectly (cf. Results in <a href="#pone-0000264-g002">Fig. 2A</a>). For the critical dichoptic condition, two sets of standard stimuli were presented dichoptically, i.e. one to the left, the other to the right eye. The two sets were presented with an offset of two slits (that is a phase difference of 180 deg) to each other, so that in the combined stimulus arrowheads were separated by two times the inter-slit distance. The rationale is that for putative monocular mechanisms the motion direction is well defined, whereas for binocular mechanisms that collapse monocular views before computing motion direction on basis of these collapsed images, motion direction is ambiguous as arrowheads appear alternately in even and odd-numbered slits and thus merely seem to wobble back and forth. This latter point was controlled in the binocular condition by presenting arrowheads with twice the inter-slit distance. Subjects performed on chance level (cf. Results in <a href="#pone-0000264-g002">Fig. 2A</a>).</p>
<a id="article1.body1.sec3.sec1.p3" name="article1.body1.sec3.sec1.p3"></a><p>In the critical dichoptic condition and with the large-scale version, subjects did not perform better than chance (40.0%±9.21). They reported having had no stable percept of the arrowheads at all indicating that the interpolation process has no access to monocular motion information.</p>
<a id="article1.body1.sec3.sec1.p4" name="article1.body1.sec3.sec1.p4"></a><p>Unfortunately, this experiment could not be conducted in the small-scale version. The distance between the arrowheads in the left and right-eye views was so small that, rather than superimposing the stimuli for the two eyes, subjects fused them into 3D views, thereby annihilating the masking effect. Thus a second experiment had to be conducted in order to test the issue for the small scale.</p>


<h4>Experiment 1B: Dichoptic motion masking II</h4>
<a id="article1.body1.sec3.sec2.p1" name="article1.body1.sec3.sec2.p1"></a><p>In experiment 1B we presented two sets of arrowheads moving in opposite directions. Here, binocular mechanisms were prevented from the interpolation process as they could merely read out ambiguous motion information.</p>
<a id="article1.body1.sec3.sec2.p2" name="article1.body1.sec3.sec2.p2"></a><p>In the monocular control conditions (‘monoL’ and ‘monoR’), subjects saw the standard stimuli that were described in the <a href="#s2">Methods</a> section (cf. <a href="#pone-0000264-g001">Fig. 1</a>) and had to indicate the orientation of the arrowheads as described above. As expected, they performed nearly perfectly (cf. <a href="#pone-0000264-g002">Fig. 2B</a>).</p>
<a id="article1.body1.sec3.sec2.p3" name="article1.body1.sec3.sec2.p3"></a><p>To build up the stimuli for the binocular condition, two sets of arrowheads were used. In each, arrowheads had the same orientation but were moving in opposite directions (one to the left, the other to the right). The two sets were superimposed and presented alike to both eyes. As a result, the stimuli contained no reliable motion information and, accordingly, subjects had no stable percept of the arrowheads and performed at chance level.</p>
<a id="article1.body1.sec3.sec2.p4" name="article1.body1.sec3.sec2.p4"></a><p>In the critical dichoptic condition (see the example shown in <a href="#pone-0000264-g002">Fig. 2B</a>), the two sets of arrowheads were presented dichoptically rather than binocularly, that is each eye was presented with one of the sets. The rational is, that if interpolation were computed by monocular mechanisms, then each of them would detect their motion direction and interpolate their form information into their global form. As a result, subjects would perceive the veridical stimulus, namely two sets of identically oriented arrowheads moving into opposite directions. In contrast, if only binocular motion detection were involved, motion information would be equivocal (much like in the binocular condition) and subjects would either see no interpolated stimuli at all or follow any spuriously dominant motion direction and perceive two arrowheads pointing in opposite directions. Neither case would subserve them with reliable information.</p>
<a id="article1.body1.sec3.sec2.p5" name="article1.body1.sec3.sec2.p5"></a><p>In the actual dichoptic test, subjects did not perform better than chance (60.0%±3.1 and 52.0%±5.5 for the small and large-scale version, respectively). They reported perceiving either arrowheads with different orientations in the same display or no interpolated stimulus at all. The results show that the spatio-temporal interpolation mechanism fails to perceive the stimulus in the dichoptic condition and hence demonstrate that the interpolation mechanism has no access to the reliable monocular information but rather has to rely entirely on binocular motion information.</p>


<h4>Experiment 1C: Interocular completion</h4>
<a id="article1.body1.sec3.sec3.p1" name="article1.body1.sec3.sec3.p1"></a><p>Using interocular motion stimuli, experiment 1C tested the alternative hypothesis, namely that spatio-temporal interpolation can rely on purely binocular motion information.</p>
<a id="article1.body1.sec3.sec3.p2" name="article1.body1.sec3.sec3.p2"></a><p>In the binocular control condition, the standard stimulus was presented to the two eyes. As expected, subjects perceived the arrowheads veridically and performed nearly perfectly (cf. <a href="#pone-0000264-g002">Fig. 2C</a>). For the critical dichoptic condition, alternate slit crossings were directed solely to the left and right eyes of the subjects. Since each successive slit crossing in the standard stimulus had a spatial phase shift of 90 degrees, in the dichoptic condition phase shifts within each eye were separated by 180 degrees, making monocular motion information ambiguous. This latter point was confirmed by the results in the monocular conditions, in which subjects saw either only the slit crossings directed to the left or else right eye, and performed at chance level accordingly.</p>
<a id="article1.body1.sec3.sec3.p3" name="article1.body1.sec3.sec3.p3"></a><p>When subjects were confronted with the dichoptic stimuli they performed nearly perfectly (87.9%±3.9 and 93.0%±3.1 for the small and large-scale version, respectively) and reported a stable percept of the arrowheads.</p>
<a id="article1.body1.sec3.sec3.p4" name="article1.body1.sec3.sec3.p4"></a><p>Taken together, while experiments 1A and 1B show that monocular motion information does not contribute to spatio-temporal interpolation, experiment 1C demonstrates that the interpolation mechanisms exclusively rely on binocular motion signals instead.</p>


<h4>Experiment 2A: Dichoptic form masking</h4>
<a id="article1.body1.sec3.sec4.p1" name="article1.body1.sec3.sec4.p1"></a><p>Using dichoptic form-masking stimuli, experiment 2A investigated whether the spatio-temporal interpolation mechanism can process monocular form information.</p>
<a id="article1.body1.sec3.sec4.p2" name="article1.body1.sec3.sec4.p2"></a><p>Stimuli for the dichoptic condition were derived from the standard stimulus in that each single arrowhead was replaced by a pair of two arrowheads, one for each eye, presented in one of four spatial formations. The left-eye arrowhead was either presented vertically (1) <em>on top of</em> or (2) <em>below</em> the right-eye arrowhead. Alternatively, (3) the left-view arrowhead was presented centrally and the right-view arrowhead was split up into two strokes, one positioned on top of, the other below the left-view arrowhead, (4) or vice-versa (cf. the examples in <a href="#pone-0000264-g003">Fig. 3A</a>). Each of the four formations thus showed a closed-line figure, and these figures could be oriented either way within the same stimulus and per-se gave no clue as to the orientation of the arrowheads (cf. the binocular condition in <a href="#pone-0000264-g003">Fig. 3A</a>). As in the first experiment, subjects had to indicate the orientation of the arrowheads.</p>
<a id="article1.body1.sec3.sec4.p3" name="article1.body1.sec3.sec4.p3"></a><p>As controls, in the binocular condition all arrowheads and strokes were presented binocularly to the two eyes. Subjects could not identify individual arrowheads and performed at chance level. In left and right monocular control conditions, only the left or right-eye view was shown to the left and right eye, respectively, while the partner eye was presented with a blank screen. In both monocular conditions, subjects performed nearly perfectly.</p>
<a id="article1.body1.sec3.sec4.p4" name="article1.body1.sec3.sec4.p4"></a><p>In the critical dichoptic task two reliable monocular views were presented separately to the two eyes which mask each other. If the spatio-temporal interpolation mechanism can read out monocular form information then the result should be two perfectly identified sets of arrowheads. However, since the two monocular views mask each other when merged into one binocular stimulus, binocular interpolation mechanisms cannot succeed here.</p>
<a id="article1.body1.sec3.sec4.p5" name="article1.body1.sec3.sec4.p5"></a><p>Subjects identified arrowhead orientations at chance level (65.7%±5.5 and 59.1%±4,9 for the small and large-scale version, respectively). They reported having had a clear percept of sigma-shaped figures rather than of individual arrowheads in most cases. However, some subjects reported that in some cases the upper and lower arrowhead did not appear fully aligned and could hence be separated. This artefact results from the means of presentation. In order to separate the views for the left and the right views with LCD shutter goggles, monocular views have to be presented on successive frames, that is with a temporal offset of one monitor refresh (6.0 ms). Since the interpolation mechanism transforms temporal delays into spatial offsets, the two arrowheads for the two eyes appear with an offset of half a dot width to each other. A second difficulty occurs when subjects stop focussing their eyes on the same point in depth. When they instead look at points farther or nearer than the stimulus depth layer then interpolated arrowheads might appear with an artificial disparity, i.e. with an artificial separation cue. However, these cases were rather seldom reported but may account for the quite good performance of nearly 66% in the dichoptic condition. In any case, subjects could not use these artefacts consistently, so that their overall performance is well below the threshold of 75%. Hence it can be assumed that the interpolation mechanism has no access to monocular form information.</p>


<h4>Experiment 2B: Interocular form</h4>
<a id="article1.body1.sec3.sec5.p1" name="article1.body1.sec3.sec5.p1"></a><p>In order to examine whether spatio-temporal interpolation instead relies on binocular form information, experiment 2B introduced a dichoptic form stimulus (<a href="#pone-0000264-g003">Fig. 3B</a>). Arrowheads in this task can only be detected by mechanisms with access to binocularly fused images.</p>
<a id="article1.body1.sec3.sec5.p2" name="article1.body1.sec3.sec5.p2"></a><p>Stimuli in this task consisted of three diagonal strokes of five dots length each, arranged vertically on top of each other. The upper and the lower stroke had the same randomized orientation and a vertical distance of seven dots between them. The middle stroke had always the opposite orientation and was aligned with either the upper or the lower one, thereby forming an arrowhead of the desired orientation.</p>
<a id="article1.body1.sec3.sec5.p3" name="article1.body1.sec3.sec5.p3"></a><p>In the binocular controls, all strokes were shown to both eyes, thus subjects here always saw an arrowhead with a flanking stroke either on top or below the former. In the monocular left and right controls the eye in question saw in each target position randomly either the two outer or else the middle stroke, while the partner eye was presented with a blank screen; i.e. in these control tests no full arrowhead was on display. As expected, subjects performed nearly perfect in the binocular and at chance level in the monocular condition.</p>
<a id="article1.body1.sec3.sec5.p4" name="article1.body1.sec3.sec5.p4"></a><p>For the dichoptic test, the two monocular views were superimposed with the result that binocular but not monocular interpolation mechanisms were enabled to perceive all the necessary form information to master the task.</p>
<a id="article1.body1.sec3.sec5.p5" name="article1.body1.sec3.sec5.p5"></a><p>In the critical dichoptic condition subjects identified the targets to high degree (90.0%±1.5 and 90.0±4.18 for the small and the large-scale version, respectively), clearly implying the involvement of binocular form detection in the spatio-temporal interpolation process.</p>

</div>

<div id="section4" class="section"><a id="s4" name="s4" toc="s4" title="Discussion"></a><h3>Discussion</h3><a id="article1.body1.sec4.p1" name="article1.body1.sec4.p1"></a><p>Spatio-temporal interpolation describes the subjective visual illusion that a stimulus is continuously presented in full when in fact it is merely shown moving behind a slit mask so that only slit-wide impressions of it can be seen at any given point in time. For the observer to be able to perceive the whole Gestalt, form and motion detection must act jointly. While the various slit images need to be received and stored, the stimuli's global motion direction determines the correct order in which the slit-views have to be reassembled into a whole form again.</p>

<h4>Binocular motion and form detection subserves spatio-temporal interpolation</h4>
<a id="article1.body1.sec4.sec1.p1" name="article1.body1.sec4.sec1.p1"></a><p>The first experiment showed that only binocular but not monocular motion information can be used in spatio-temporal interpolation. In experiment 1A, dichoptic motion masking prevented binocular motion detectors from identifying the global motion direction and reserved the task for monocular detectors. Since subjects could not identify the resulting target orientation better than chance, we conclude that the spatio-temporal interpolation mechanisms cannot make use of monocular motion information. Experiment 1B tested the same issue with presenting monocularly well-defined but binocularly ambiguous motion information. As in the critical dichoptic condition, subjects performed at chance level, results obtained here confirm the findings in experiment 1A in that monocular motion information does not contribute to spatio-temporal interpolation. Complementarily, experiment 1C cross-checked whether binocular motion information is exploited by using a dichoptic motion signal that presented monocular detectors with ambivalent motion signals, while providing only binocular mechanisms with reliable motion information. Subjects performed with almost perfect accuracy, which indicates clearly that binocular detection is involved.</p>
<a id="article1.body1.sec4.sec1.p2" name="article1.body1.sec4.sec1.p2"></a><p>The second experiment probed the form-detection part in the interpolation process and followed the same experimental rationale. In a dichoptic form-masking task (expt. 2A), monocular form information from both eyes was reliable individually but masked each other completely so that binocular detectors were prevented from identifying the target's orientation. Subjects performed at chance level, a result supporting the notion that monocular form detection is not involved. Experiment 2B then tested whether spatio-temporal interpolation can else rely on binocular form cues. There, a dichoptically presented target element could only be identified by mechanisms with access to the binocularly fused image. Subjects identified the targets perfectly, confirming that the underlying mechanism is binocular.</p>
<a id="article1.body1.sec4.sec1.p3" name="article1.body1.sec4.sec1.p3"></a><p>Taken together we found that only binocular but not monocular motion and form detectors underlie spatio-temporal interpolation.</p>


<h4>Comparison between small and large-scale versions</h4>
<a id="article1.body1.sec4.sec2.p1" name="article1.body1.sec4.sec2.p1"></a><p>Furthermore, the interpolation mechanism can integrate information from small and large-scale stimuli. Subjects reported that the interpolated arrowheads looked stable when the inter-slit latency was 36 and 72 ms. Only with the longest interval of 144 ms, that is when large-scale stimuli had to be observed using LCD shutter goggles, arrowheads appeared slightly deformed at their tails. However, this lesser quality did not influence psychophysical results as subjects identified the orientation of the arrowheads in the binocular in expt. 1B with similar ease as the monocular controls in expt 1C, although stimuli in 1B are presented using LCD shutters whereas those in 1C are displayed as red-green anaglyphs.</p>
<a id="article1.body1.sec4.sec2.p2" name="article1.body1.sec4.sec2.p2"></a><p>These findings relate to the results found by Fahle &amp; Poggio <a href="#pone.0000264-Fahle1">[3]</a> and Morgan &amp; Watt <a href="#pone.0000264-Morgan4">[12]</a> in that the quality of interpolation deteriorates with prolonged temporal separation between slit crossings. The fact that the thresholds obtained here are higher than the thresholds reported by Morgan &amp; Watt may relate to the narrow dot width of the stimuli used there <a href="#pone.0000264-Morgan4">[12]</a>.</p>


<h4>Implications for interpolation models</h4>
<a id="article1.body1.sec4.sec3.p1" name="article1.body1.sec4.sec3.p1"></a><p>In their articles on spatio-temporal interpolation, Burr et al. <a href="#pone.0000264-Burr2">[8]</a>, <a href="#pone.0000264-Burr4">[13]</a> explained their findings with a spatio-temporal filter similar to the spatio-temporal motion-energy models forwarded independently at the same time <a href="#pone.0000264-Fahle1">[3]</a>, <a href="#pone.0000264-Adelson1">[14]</a>, <a href="#pone.0000264-vanSanten1">[15]</a>, <a href="#pone.0000264-Watson1">[16]</a>.</p>
<a id="article1.body1.sec4.sec3.p2" name="article1.body1.sec4.sec3.p2"></a><p>In their use as motion detector models, these filters are ideally activated by a stimulus drifting with the proper velocity in the proper direction. Conversely, if one knows that the detector is activated and furthermore knows the start point and the start time, then one can predict the expected time point for every position and, more importantly here, the stimulus' position for a given time. In that sense, time and place are exchangeable. Burr et al. <a href="#pone.0000264-Burr2">[8]</a>, <a href="#pone.0000264-Burr4">[13]</a> argued that these models, once activated by the global motion velocity of the stimuli, might then also detect the target forms (in their case verniers) in that for them, temporal delays are interchangeable with spatial distances.</p>
<a id="article1.body1.sec4.sec3.p3" name="article1.body1.sec4.sec3.p3"></a><p>However, spatio-temporal energy models have been proven to be computationally equivalent to the monocular elaborated Reichardt detector <a href="#pone.0000264-vanSanten1">[15]</a>, <a href="#pone.0000264-Lu1">[see also 17</a>,<a href="#pone.0000264-Georgeson1">&amp; 18]</a>. Since we show here that both motion and form detection underlying spatio-temporal interpolation are <em>binocular</em>, we propose that the notion of analogy between spatio-temporal interpolation mechanisms and the <em>monocular</em> spatio-temporal motion-energy mechanisms / Reichardt detectors needs to be revised.</p>
<a id="article1.body1.sec4.sec3.p4" name="article1.body1.sec4.sec3.p4"></a><p>Furthermore our results call for a re-interpretation of some previously reported data that was taken as experimental evidence for the monocular nature of the spatio-temporal interpolation detector. Fahle &amp; DeLuca <a href="#pone.0000264-Fahle2">[19]</a> dichoptically presented two verniers of opposite orientation moving towards each other and found that subjects perceived a single vernier oriented in three-dimensional space, moving either away from the subjects or toward them. Fahle and De Luca argued that for the stereo detector to be activated properly, the verniers must have been interpolated already monocularly. However, in their displays the left and right upper stroke of the vernier appeared simultaneously behind the slits and so did the left and right lower strokes. Thus as a new explanation, we propose here that the stereo detector might have simply fused the upper and the lower monocular strokes individually into an upper and a lower three-dimensional stroke; and that then the binocular interpolation mechanism computed the real form of the stereoscopically fused strokes.</p>


<h4>Adjusted models of spatio-temporal interpolation</h4>
<a id="article1.body1.sec4.sec4.p1" name="article1.body1.sec4.sec4.p1"></a><p>In that we question the monocular nature of the spatio-temporal interpolation mechanism, our finding reintroduces the question of what mechanisms may constitute spatio-temporal interpolation. Three candidate models seem possible:</p>


<ol class="order">

<li>A unitary spatio-temporal interpolation detector, which is binocular rather than monocular. Following Burr et al.'s <a href="#pone.0000264-Burr1">[2]</a>, <a href="#pone.0000264-Burr2">[8]</a>, <a href="#pone.0000264-Burr4">[13]</a> proposal that interpolation can be detected by a spatio-temporal energy mechanism, and further following the proposition <a href="#pone.0000264-Lu1">[17]</a>, <a href="#pone.0000264-Carney1">[20]</a> that both, the binocular first-order and the binocular third-order, motion mechanism are also standard motion-energy detectors, one could argue that interpolation might be achieved by any of these binocular motion detectors. Since stimuli could be interpolated even for cycle frequencies of approx. 7 Hz (24 frames of 6 ms each per cycle) the employment of the third-order motion detector with its temporal cut-off frequency of 2–4 Hz, appears less likely <a href="#pone.0000264-Lu1">[17]</a>, <a href="#pone.0000264-Zanker1">[21]</a>. However, Burr's model remains incomplete insofar as it is “lacking a clear statement of how or by what underlying mechanism the unitary gestalt is formed” <a href="#pone.0000264-Breitmeyer1">[22]</a>.</li>

<li>In the concurring retinal-painting model, binocular motion mechanisms would grasp the global motion and induce suitable smooth-pursuit eye movements. Due to the resulting retinal shift, temporally delayed dots and lines would be drawn onto the retina with a spatial displacement. While this model incorporates also a monocular stage, namely the two retinae, the actual forms are only detected on the binocular salience map. This model is subserved by the findings that area MT on the one hand plays an integrative role in initiating and controlling smooth-pursuit eye movements <a href="#pone.0000264-Newsome1">[e.g. 23]</a> and on the other hand computes three-dimensional (i.e. binocular) motion <a href="#pone.0000264-Paffen1">[e.g. 24]</a>. In this model motion and form can be thought of as detected by completely separate units, linked externally by the eye movements.</li>

<li>The third model incorporates two separate mechanisms for form and motion. A binocular first-order motion detector would detect the global motion direction and feed this information forward into a binocular form detector, which in turn shifts incoming slit-views accordingly and integrates across a duration of 50 to 80 ms. It might also be the binocular third-order rather than the binocular first-order motion detector that grasps the global motion and feeds this information then back into the form detector (‘saliency map’). However, as already mentioned above, the cycle frequency of 7 Hz may be too fast for the third-order motion detector <a href="#pone.0000264-Lu1">[17]</a>. Either way, this model would incorporate internal links between motion and form detectors.</li>

</ol>

<h4>Conclusion</h4>
<a id="article1.body1.sec4.sec5.p1" name="article1.body1.sec4.sec5.p1"></a><p>We have demonstrated that the mechanisms underlying spatio-temporal interpolation comprise binocular sub-mechanisms for form and motion detection. This finding contradicts earlier assumptions according to which spatio-temporal interpolation is subserved by monocular detectors, or more specific, by a monocular unitary spatio-temporal energy detector <a href="#pone.0000264-Fahle1">[3]</a>, <a href="#pone.0000264-Burr2">[8]</a>. From this new viewpoint, three modified models seem possible–a binocular unitary spatio-temporal motion detector, binocular motion detectors in area MT (V5) inducing smooth-pursuit eye movements which in succession turn temporal delays into spatial offsets and finally two separate binocular detectors for form and motion that are linked via feed-back loops internally.</p>

</div>





<div><a id="ack" name="ack" toc="ack" title="Acknowledgments"></a><h3>Acknowledgments</h3>
<a id="article1.back1.ack1.p1" name="article1.back1.ack1.p1"></a><p>Animated QuickTime® demonstrations can be obtained from the first author's homepage: <a href="http://wwwpsy.uni-muenster.de/Psychologie.inst2/AELappe/personen/kandil.html">http://wwwpsy.uni-muenster.de/Psychologi​e.inst2/AELappe/personen/kandil.html</a></p>
</div><div class="contributions"><a id="authcontrib" name="authcontrib" toc="authcontrib" title="Author Contributions"></a><h3>Author Contributions</h3><p>Conceived and designed the experiments: FK ML. Performed the experiments: FK. Analyzed the data: FK. Contributed reagents/materials/analysis tools: FK ML. Wrote the paper: FK ML.</p></div><div><a id="references" name="references" toc="references" title="References"></a><h3>References</h3><ol class="references"><li><span class="label">1.
              </span><a name="pone.0000264-Anstis1" id="pone.0000264-Anstis1"></a>Anstis SM, Atkinson J (1967) Distortions in moving figures viewed through a stationary slit. Am J Psychol  80: 572–585.  <ul class="find" data-citedArticleID="934772" data-doi="10.2307/1421189"><li><a href="http://dx.doi.org/10.2307/1421189" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Distortions+in+moving+figures+viewed+through+a+stationary+slit." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Distortions+in+moving+figures+viewed+through+a+stationary+slit.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">2.
              </span><a name="pone.0000264-Burr1" id="pone.0000264-Burr1"></a>Burr DC (1979) Acuity for apparent vernier offset. Vision Res  19: 835–837.  <ul class="find" data-citedArticleID="934776" data-doi="10.1016/0042-6989(79)90162-7"><li><a href="http://dx.doi.org/10.1016/0042-6989(79)90162-7" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Acuity+for+apparent+vernier+offset." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Acuity+for+apparent+vernier+offset.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">3.
              </span><a name="pone.0000264-Fahle1" id="pone.0000264-Fahle1"></a>Fahle M, Poggio T (1981) Visual hyperacuity:spatiotemporal interpolation in human vision. Proc R Soc Lond B Biol Sci  213: 451–477.  <ul class="find" data-citedArticleID="934786" data-doi="10.1098/rspb.1981.0075"><li><a href="http://dx.doi.org/10.1098/rspb.1981.0075" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Visual+hyperacuity%3Aspatiotemporal+interpolation+in+human+vision." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Visual+hyperacuity%3Aspatiotemporal+interpolation+in+human+vision.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">4.
              </span><a name="pone.0000264-Parks1" id="pone.0000264-Parks1"></a>Parks TE (1965) Post-retinal visual storage. Am J Psychol  78: 145–147.  <ul class="find" data-citedArticleID="934808" data-doi="10.2307/1421101"><li><a href="http://dx.doi.org/10.2307/1421101" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Post-retinal+visual+storage." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Post-retinal+visual+storage.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">5.
              </span><a name="pone.0000264-Zllner1" id="pone.0000264-Zllner1"></a>Zöllner F (1862) Ueber eine neue Art anorthoskopischer Zerrbilder. Annalen der Physik und Chemie  117: 477–484.  <ul class="find" data-citedArticleID="934814" data-doi="10.1002/andp.18621931108"><li><a href="http://dx.doi.org/10.1002/andp.18621931108" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Ueber+eine+neue+Art+anorthoskopischer+Zerrbilder." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Ueber+eine+neue+Art+anorthoskopischer+Zerrbilder.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">6.
              </span><a name="pone.0000264-Morgan1" id="pone.0000264-Morgan1"></a>Morgan MJ, Watt RJ (1983) On the failure of spatiotemporal interpolation: a filtering model. Vision Res  23: 997–1004.  <ul class="find" data-citedArticleID="934794" data-doi="10.1016/0042-6989(83)90010-x"><li><a href="http://dx.doi.org/10.1016/0042-6989(83)90010-x" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=On+the+failure+of+spatiotemporal+interpolation%3A+a+filtering+model." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22On+the+failure+of+spatiotemporal+interpolation%3A+a+filtering+model.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">7.
              </span><a name="pone.0000264-Nishida1" id="pone.0000264-Nishida1"></a>Nishida S (2004) Motion-based analysis of spatial patterns by the human visual system. Curr Biol  14: 830–839.  <ul class="find" data-citedArticleID="934804" data-doi="10.1016/j.cub.2004.04.044"><li><a href="http://dx.doi.org/10.1016/j.cub.2004.04.044" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Motion-based+analysis+of+spatial+patterns+by+the+human+visual+system." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Motion-based+analysis+of+spatial+patterns+by+the+human+visual+system.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">8.
              </span><a name="pone.0000264-Burr2" id="pone.0000264-Burr2"></a>Burr DC, Ross J (1986) Visual processing of motion. Trends Neurosci  9: 304–307.  <ul class="find" data-citedArticleID="934778" data-doi="10.1016/0166-2236(86)90088-3"><li><a href="http://dx.doi.org/10.1016/0166-2236(86)90088-3" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Visual+processing+of+motion." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Visual+processing+of+motion.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">9.
              </span><a name="pone.0000264-Burr3" id="pone.0000264-Burr3"></a>Burr DC, Ross J (1979) How does binocular delay give information about depth? Vision Res  19: 523–532.  <ul class="find" data-citedArticleID="934780" data-doi="10.1016/0042-6989(79)90137-8"><li><a href="http://dx.doi.org/10.1016/0042-6989(79)90137-8" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=How+does+binocular+delay+give+information+about+depth%3F" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22How+does+binocular+delay+give+information+about+depth%3F%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">10.
              </span><a name="pone.0000264-Morgan2" id="pone.0000264-Morgan2"></a>Morgan MJ (1979) Perception of continuity in stroboscopic motion: a temporal frequency analysis. Vision Res  19: 491–500.  <ul class="find" data-citedArticleID="934796" data-doi="10.1016/0042-6989(79)90133-0"><li><a href="http://dx.doi.org/10.1016/0042-6989(79)90133-0" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Perception+of+continuity+in+stroboscopic+motion%3A+a+temporal+frequency+analysis." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Perception+of+continuity+in+stroboscopic+motion%3A+a+temporal+frequency+analysis.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">11.
              </span><a name="pone.0000264-Morgan3" id="pone.0000264-Morgan3"></a>Morgan MJ, Thompson P (1975) Apparent motion and the Pulfrich effect. Perception  4: 3–18.  <ul class="find" data-citedArticleID="934798" data-doi="10.1068/p040003"><li><a href="http://dx.doi.org/10.1068/p040003" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Apparent+motion+and+the+Pulfrich+effect." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Apparent+motion+and+the+Pulfrich+effect.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">12.
              </span><a name="pone.0000264-Morgan4" id="pone.0000264-Morgan4"></a>Morgan MJ, Watt RJ (1983) On the failure of spatiotemporal interpolation: a filtering model. Vision Res  23: 997–1004.  <ul class="find" data-citedArticleID="934800" data-doi="10.1016/0042-6989(83)90010-x"><li><a href="http://dx.doi.org/10.1016/0042-6989(83)90010-x" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=On+the+failure+of+spatiotemporal+interpolation%3A+a+filtering+model." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22On+the+failure+of+spatiotemporal+interpolation%3A+a+filtering+model.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">13.
              </span><a name="pone.0000264-Burr4" id="pone.0000264-Burr4"></a>Burr DC, Ross J, Morrone MC (1986) Seeing objects in motion. Proc R Soc Lond B Biol Sci  227: 249–265.  <ul class="find" data-citedArticleID="934782" data-doi="10.1098/rspb.1986.0022"><li><a href="http://dx.doi.org/10.1098/rspb.1986.0022" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Seeing+objects+in+motion." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Seeing+objects+in+motion.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">14.
              </span><a name="pone.0000264-Adelson1" id="pone.0000264-Adelson1"></a>Adelson EH, Bergen JR (1985) Spatiotemporal energy models for the perception of motion. J Opt Soc Am A  2: 284–299.  <ul class="find" data-citedArticleID="934770" data-doi="10.1364/josaa.2.000284"><li><a href="http://dx.doi.org/10.1364/josaa.2.000284" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Spatiotemporal+energy+models+for+the+perception+of+motion." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Spatiotemporal+energy+models+for+the+perception+of+motion.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">15.
              </span><a name="pone.0000264-vanSanten1" id="pone.0000264-vanSanten1"></a>van Santen JP, Sperling G (1985) Elaborated Reichardt detectors. J Opt Soc Am A  2: 300–321.  <ul class="find" data-citedArticleID="934816" data-doi="10.1364/josaa.2.000300"><li><a href="http://dx.doi.org/10.1364/josaa.2.000300" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Elaborated+Reichardt+detectors." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Elaborated+Reichardt+detectors.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">16.
              </span><a name="pone.0000264-Watson1" id="pone.0000264-Watson1"></a>Watson AB, Ahumada AJJ (1985) Model of human visual-motion sensing. J Opt Soc Am A  2: 322–341.  <ul class="find" data-citedArticleID="934810" data-doi="10.1364/josaa.2.000322"><li><a href="http://dx.doi.org/10.1364/josaa.2.000322" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Model+of+human+visual-motion+sensing." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Model+of+human+visual-motion+sensing.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">17.
              </span><a name="pone.0000264-Lu1" id="pone.0000264-Lu1"></a>Lu ZL, Sperling G (1995) The functional architecture of human visual motion perception. Vision Res  35: 2697–2722.  <ul class="find" data-citedArticleID="934792" data-doi="10.1016/0042-6989(95)00025-u"><li><a href="http://dx.doi.org/10.1016/0042-6989(95)00025-u" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=The+functional+architecture+of+human+visual+motion+perception." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22The+functional+architecture+of+human+visual+motion+perception.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">18.
              </span><a name="pone.0000264-Georgeson1" id="pone.0000264-Georgeson1"></a>Georgeson MA, Scott-Samuel NE (1999) Motion contrast: a new metric for direction discrimination. Vision Res  39: 4393–4402.  <ul class="find" data-citedArticleID="934790" data-doi="10.1016/s0042-6989(99)00147-9"><li><a href="http://dx.doi.org/10.1016/s0042-6989(99)00147-9" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Motion+contrast%3A+a+new+metric+for+direction+discrimination." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Motion+contrast%3A+a+new+metric+for+direction+discrimination.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">19.
              </span><a name="pone.0000264-Fahle2" id="pone.0000264-Fahle2"></a>Fahle M, De Luca E (1994) Spatio-temporal interpolation in depth. Vision Res  34: 343–348.  <ul class="find" data-citedArticleID="934788" data-doi="10.1016/0042-6989(94)90092-2"><li><a href="http://dx.doi.org/10.1016/0042-6989(94)90092-2" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Spatio-temporal+interpolation+in+depth." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Spatio-temporal+interpolation+in+depth.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">20.
              </span><a name="pone.0000264-Carney1" id="pone.0000264-Carney1"></a>Carney T (1997) Evidence for an early motion system which integrates information from the two eyes. Vision Res  37: 2361–2368.  <ul class="find" data-citedArticleID="934784" data-doi="10.1016/s0042-6989(97)00053-9"><li><a href="http://dx.doi.org/10.1016/s0042-6989(97)00053-9" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Evidence+for+an+early+motion+system+which+integrates+information+from+the+two+eyes." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Evidence+for+an+early+motion+system+which+integrates+information+from+the+two+eyes.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">21.
              </span><a name="pone.0000264-Zanker1" id="pone.0000264-Zanker1"></a>Zanker JM (1996) On the elementary mechanism underlying secondary motion processing. Philos Trans R Soc Lond B Biol Sci  351: 1725–1736.  <ul class="find" data-citedArticleID="934812" data-doi="10.1098/rstb.1996.0154"><li><a href="http://dx.doi.org/10.1098/rstb.1996.0154" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=On+the+elementary+mechanism+underlying+secondary+motion+processing." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22On+the+elementary+mechanism+underlying+secondary+motion+processing.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">22.
              </span><a name="pone.0000264-Breitmeyer1" id="pone.0000264-Breitmeyer1"></a>Breitmeyer BG, Ögmen H (2006) Visual Masking: Time Slices Through Conscious and Unconscious Vision. Oxford; New York: Oxford University Press.   <ul class="find-nolinks"></ul></li><li><span class="label">23.
              </span><a name="pone.0000264-Newsome1" id="pone.0000264-Newsome1"></a>Newsome WT, Wurtz RH, Dursteler MR, Mikami A (1985) Deficits in visual motion processing following ibotenic acid lesions of the middle temporal visual area of the macaque monkey. J Neurosci  5: 825–840.  <ul class="find" data-citedArticleID="934802"><li><a href="http://www.crossref.org/guestquery/?auth2=&amp;atitle2=Deficits+in+visual+motion+processing+following+ibotenic+acid+lesions+of+the+middle+temporal+visual+area+of+the+macaque+monkey.&amp;auth=&amp;atitle=Deficits+in+visual+motion+processing+following+ibotenic+acid+lesions+of+the+middle+temporal+visual+area+of+the+macaque+monkey." target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Deficits+in+visual+motion+processing+following+ibotenic+acid+lesions+of+the+middle+temporal+visual+area+of+the+macaque+monkey." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Deficits+in+visual+motion+processing+following+ibotenic+acid+lesions+of+the+middle+temporal+visual+area+of+the+macaque+monkey.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">24.
              </span><a name="pone.0000264-Paffen1" id="pone.0000264-Paffen1"></a>Paffen CL, Alais D, Verstraten FA (2005) Center-surround inhibition deepens binocular rivalry suppression. Vision Res  45: 2642–2649.  <ul class="find" data-citedArticleID="934806" data-doi="10.1016/j.visres.2005.04.018"><li><a href="http://dx.doi.org/10.1016/j.visres.2005.04.018" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Center-surround+inhibition+deepens+binocular+rivalry+suppression." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Center-surround+inhibition+deepens+binocular+rivalry+suppression.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li></ol></div>

  </div>

      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000264.XML" value="65753"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000264.PDF" value="241307"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000264.g001.PNG_L" value="24514"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000264.g001.PNG_M" value="17493"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000264.g001.PNG_S" value="3422"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000264.g001.TIF" value="62882"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000264.g001.PNG_I" value="16043"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000264.g002.PNG_L" value="203275"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000264.g002.PNG_M" value="35416"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000264.g002.PNG_S" value="12463"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000264.g002.TIF" value="415766"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000264.g002.PNG_I" value="112672"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000264.g003.PNG_L" value="95391"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000264.g003.PNG_M" value="36610"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000264.g003.PNG_S" value="3905"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000264.g003.TIF" value="304406"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000264.g003.PNG_I" value="73014"/>

</div>
<div class="sidebar">

  <div class="article-actions cf">
      <div class="download">
        <span class="btn"><a href="/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0000264&amp;representation=PDF" title="Download" target="_blank">Download PDF</a></span>
      </div>
      <div class="btn-reveal dropdown">
        <div class="dropdown-icon">
          <span class="btn">&nbsp;</span>
        </div>

        <div class="content">
          <ul class="bullet">
            <li><a href="/article/citationList.action?articleURI=info%3Adoi%2F10.1371%2Fjournal.pone.0000264" title="Download citations">Citation</a></li>
            <li><a href="/article/fetchObjectAttachment.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0000264&amp;representation=XML" title="Download article XML">XML</a></li>
          </ul>
        </div>
      </div> <!-- end btn-reveal dropdown-->


    <div class="btn-reveal flt-l">
        <span class="btn">Print</span>
        <div class="content">
            <ul class="bullet">
                <li id="print-article"><a href="#" onclick="if(typeof(_gaq) != 'undefined'){ _gaq.push(['_trackEvent','Article', 'Print', 'Click']); } window.print(); return false;" title="Print Article">Print article</a></li>
                <li>
                  <a href="https://www.odysseypress.com/onlinehost/reprint_order.php?type=A&page=0&journal=7&doi=10.1371/journal.pone.0000264&volume=&issue=&title=Spatio-Temporal Interpolation Is Accomplished by Binocular Form and Motion Mechanisms&author_name=Farid%20I.%20Kandil%2C%20Markus%20Lappe&start_page=1&end_page=8" title="Odyssey Press">EzReprint</a>
                </li>
            </ul>
        </div>
    </div>

    <div class="btn-reveal flt-r">
        <span class="btn">Share</span>
        <div class="content">
            <ul class="social">
                <li><a href="http://www.reddit.com/submit?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0000264" target="_blank" title="Submit to Reddit"><img src="/images/icon.reddit.16.png" width="16" height="16" alt="Reddit">Reddit</a></li>

                <li><a href="https://plus.google.com/share?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0000264" target="_blank" title="Share on Google+"><img src="/images/icon.gplus.16.png" width="16" height="16" alt="Google+">Google+</a></li>

                <li><a href="http://www.stumbleupon.com/submit?url=http%3A%2F%2Fwww.plosone.org%2Farticle%2Finfo%253Adoi%252F10.1371%252Fjournal.pone.0000264" target="_blank" title="Add to StumbleUpon"><img src="/images/icon.stumble.16.png" width="16" height="16" alt="StumbleUpon">StumbleUpon</a></li>

                <li><a href="http://www.facebook.com/share.php?u=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0000264&amp;t=Spatio-Temporal%20Interpolation%20Is%20Accomplished%20by%20Binocular%20Form%20and%20Motion%20Mechanisms" target="_blank" title="Share on Facebook"><img src="/images/icon.fb.16.png" width="16" height="16" alt="Facebook">Facebook</a></li>

                <li><a href="http://www.linkedin.com/shareArticle?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0000264&title=Spatio-Temporal%20Interpolation%20Is%20Accomplished%20by%20Binocular%20Form%20and%20Motion%20Mechanisms&summary=Checkout%20this%20article%20I%20found%20at%20PLOS" target="_blank" title="Add to LinkedIn"><img src="/images/icon.linkedin.16.png" width="16" height="16" alt="Mendeley">LinkedIn</a></li>

                <li><a href="http://www.citeulike.org/posturl?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0000264&amp;title=Spatio-Temporal%20Interpolation%20Is%20Accomplished%20by%20Binocular%20Form%20and%20Motion%20Mechanisms" target="_blank" title="Add to CiteULike"><img src="/images/icon.cul.16.png" width="16" height="16" alt="CiteULike">CiteULike</a></li>

                <li><a href="http://www.mendeley.com/import/?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0000264" target="_blank" title="Add to Mendeley"><img src="/images/icon.mendeley.16.png" width="16" height="16" alt="Mendeley">Mendeley</a></li>

                <li><a href="https://www.pubchase.com/library?add_aid=10.1371%2Fjournal.pone.0000264&amp;source=plos" target="_blank" title="Add to PubChase"><img src="/images/icon.pc.16.png" width="16" height="16" alt="PubChase">PubChase</a></li>


                <script type="text/javascript">
                    // replace tweet with one that's pre-shortened to 140 chars
                    function truncateTweetText() {
                        var twtTitle = 'Spatio-Temporal Interpolation Is Accomplished by Binocular Form and Motion Mechanisms';
                        var twtUrl = 'http://dx.plos.org/10.1371/journal.pone.0000264';
                        // all URLs posted to twitter get auto-shortened to 20 chars.
                        var maxLength = 140 - (20 + 1);
                        // truncate the title to include space for twtTag and ellipsis (here, 10 = tag length + space + ellipsis)
                        if (twtTitle.length > maxLength) { twtTitle = twtTitle.substr(0, (maxLength - 10)) + '...'; }
                        // set the href to use the shortened tweet
                        $('#twitter-share-link').prop('href', 'http://twitter.com/intent/tweet?text=' + encodeURIComponent('#PLOSONE: ' + twtTitle + ' ' + twtUrl));
                    }
                </script>
                <li><a href="http://twitter.com/intent/tweet?text=#PLOSONE%3A%20Spatio-Temporal%20Interpolation%20Is%20Accomplished%20by%20Binocular%20Form%20and%20Motion%20Mechanisms http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0000264" onclick="truncateTweetText();" target="_blank" title="Share on Twitter" id="twitter-share-link"><img src="/images/icon.twtr.16.png" width="16" height="16" alt="Twitter">Twitter</a></li>

                <li><a href="/article/email/info%3Adoi%2F10.1371%2Fjournal.pone.0000264" title="Email this article"><img src="/images/icon.email.16.png" width="16" height="16" alt="Email">Email</a></li>
            </ul>
        </div>
    </div><!--end btn-reveal flt-r-->
</div><!-- end article-actions-->

<!-- begin Crossmark -->

<a id="open-crossmark" href="#" style="margin-top: -28px; display:block"><img style="border: 0; display: none;
 padding: 10px 0 18px 0;"  id="crossmark-icon" src="/images/logo-crossmark-bw.png" /></a>
<div id="crossmark-dialog" style="display: none;" title="">
    <!-- the external CrossMark data is loaded inside this iframe -->
    <iframe id="crossmark-dialog-frame" frameborder="0"></iframe>
</div>

<!-- end crossmark -->


<div class="block" id="subject-area-sidebar-block">
    <div class="header">
        <h3>Subject Areas</h3><div title="More information" id="subject-area-sidebar-block-help-icon"><img align="right"
                                                                                                           alt="info" src="/images/button_info.png"/><div id="subject-area-sidebar-block-help"><img align="right"
                                                                                                                                                                                                    src="/images/button_info.png"/><p>
        <b>We want your feedback.</b> Do these subject areas make sense for this article? If not, click the flag
        next to the incorrect subject area and we will review it. Thanks for your help!
    </p></div></div>
    </div>


    <ul id="subject-area-sidebar-list">




















          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Eye+movements%22" title="Search for articles in the subject area:'Eye movements'"><div class="flagText">Eye movements</div></a>
              <div data-categoryid="48081" data-articleid="23366"
                   data-categoryname="Eye movements"
                   class="flagImage" title="Flag 'Eye movements' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Eyes%22" title="Search for articles in the subject area:'Eyes'"><div class="flagText">Eyes</div></a>
              <div data-categoryid="47415" data-articleid="23366"
                   data-categoryname="Eyes"
                   class="flagImage" title="Flag 'Eyes' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Interpolation%22" title="Search for articles in the subject area:'Interpolation'"><div class="flagText">Interpolation</div></a>
              <div data-categoryid="36543" data-articleid="23366"
                   data-categoryname="Interpolation"
                   class="flagImage" title="Flag 'Interpolation' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Motion+detectors%22" title="Search for articles in the subject area:'Motion detectors'"><div class="flagText">Motion detectors</div></a>
              <div data-categoryid="24481" data-articleid="23366"
                   data-categoryname="Motion detectors"
                   class="flagImage" title="Flag 'Motion detectors' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Retina%22" title="Search for articles in the subject area:'Retina'"><div class="flagText">Retina</div></a>
              <div data-categoryid="48243" data-articleid="23366"
                   data-categoryname="Retina"
                   class="flagImage" title="Flag 'Retina' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Shutters%22" title="Search for articles in the subject area:'Shutters'"><div class="flagText">Shutters</div></a>
              <div data-categoryid="27265" data-articleid="23366"
                   data-categoryname="Shutters"
                   class="flagImage" title="Flag 'Shutters' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Stroke%22" title="Search for articles in the subject area:'Stroke'"><div class="flagText">Stroke</div></a>
              <div data-categoryid="21665" data-articleid="23366"
                   data-categoryname="Stroke"
                   class="flagImage" title="Flag 'Stroke' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Visual+system%22" title="Search for articles in the subject area:'Visual system'"><div class="flagText">Visual system</div></a>
              <div data-categoryid="34487" data-articleid="23366"
                   data-categoryname="Visual system"
                   class="flagImage" title="Flag 'Visual system' as inappropriate"></div>
          </li>
    </ul>
</div>

<div class="ad">
    <div class="title">Advertisement</div>






  <iframe id='a0852f54' name='a0852f54'
    src='http://ads.plos.org/www/delivery/afr.php?zoneid=381&amp;cb=9634'
    frameborder='0' scrolling='no' width='160' height='600'>
    <a href='http://ads.plos.org/www/delivery/ck.php?n=a0852f54&amp;cb=6113'
      target='_top'><img src='http://ads.plos.org/www/delivery/avw.php?zoneid=381&amp;cb=2071&amp;n=a0852f54'
      border='0' alt=''/>
    </a>
  </iframe>



</div>

<div id="twitter-alm-timeline" class="twitter-alm-timeline"></div>


</div><!-- sidebar -->
    </div>
  </div>
</div>
<script src="http://wl.figshare.com/static/p_widget.js" type="text/javascript"></script><div id="pageftr">
  <div class="ftr-cols cf">
    <div class="col col-1">
      <img src="/images/logo-plos-footer.png" alt="PLOS Logo" class="logo" />
      <p><a href="/static/releaseNotes">Ambra 2.9.16</a> Managed Colocation provided <br />by <a href="http://www.isc.org/">Internet Systems Consortium</a>.<p>
      <div class="nav nav-aux">
        <a href="/static/privacy">Privacy Policy</a> |
        <a href="/static/terms">Terms of Use</a> |
        <a href="http://www.plos.org/advertise/">Advertise</a> |
        <a href="http://www.plos.org/about/media-inquiries/">Media Inquiries</a>
      </div>
    </div>
    <div class="col col-2">
      <p><a href="http://www.plos.org/publications/journals/">Publications</a></p>
      <div class="nav">
        <ul>
          <li><a href="http://www.plosbiology.org">PLOS Biology</a></li>
          <li><a href="http://www.plosmedicine.org">PLOS Medicine</a></li>
          <li><a href="http://www.ploscompbiol.org">PLOS Computational Biology</a></li>
          <li><a href="http://currents.plos.org">PLOS Currents</a></li>
          <li><a href="http://www.plosgenetics.org">PLOS Genetics</a></li>
          <li><a href="http://www.plospathogens.org">PLOS Pathogens</a></li>
          <li><a href="http://www.plosone.org">PLOS ONE</a></li>
          <li><a href="http://www.plosntds.org">PLOS Neglected Tropical Diseases</a></li>
        </ul>
      </div>
    </div>
    <div class="col col-3">
      <div class="nav">
        <p><a href="http://www.plos.org">plos.org</a></p>
        <p><a href="http://blogs.plos.org">Blogs</a></p>
        <p><a href="http://www.ploscollections.org">Collections</a></p>
        <p><a href="/feedback/new">Send us feedback</a></p>

        <p>California (US) corporation #C2354500, based in San Francisco</p>
      </div>
    </div>
  </div>
</div><!-- pageftr -->

</div><!-- end page-wrap, this div is in header.ftl -->
<script type="text/javascript" src="/javascript/jquery-1.8.1-min.js?v=Tm7VCOzZz3lE03ghpkS6SWkHbyI"></script>
<script type="text/javascript" src="/javascript/ga-min.js?v=lNQ4gt8QcPDatjsdOFl_FGpPhLY"></script>
<script type="text/javascript" src="/javascript/jquery.hoverIntent-min.js?v=mRiGNYY9cIXxVb8u0K_MdW7hHnc"></script>
<script type="text/javascript" src="/javascript/jquery.placeholder-min.js?v=21Pn56Ur9h1N4K4VZDa0nqI3Pxo"></script>
<script type="text/javascript" src="/javascript/jquery.jsonp-2.4.0-min.js?v=lqTpzoHfSq3I5Ygo01qq5WankEo"></script>
<script type="text/javascript" src="/javascript/jquery-ui-1.9.2.custom-min.js?v=raSSlfNO0YsV5uUpAKmTB9n5VTc"></script>
<script type="text/javascript" src="/javascript/jquery.tooltip-min.js?v=cw+6Smh+mdryIA25xvqIvHMrnZM"></script>
<script type="text/javascript" src="/javascript/jquery.uniform-min.js?v=kYUAnX6W2W_2fK3RIuQ2m_YFG9U"></script>
<script type="text/javascript" src="/javascript/jquery.pjax-min.js?v=939kLBjL5_YKbx71T1RHjYaD4l8"></script>
<script type="text/javascript" src="/javascript/imagesloaded-min.js?v=XeuAp8Gc3mvQUo+wZCSF8ttPwvw"></script>
<script type="text/javascript" src="/javascript/figviewer-min.js?v=yPUa0sUQ_iHkI+IRv2i9bjyZJFo"></script>
<script type="text/javascript" src="/javascript/global-min.js?v=0Q3PwjeaWtXYDnqIsQvnL_ou0qs"></script>
<script type="text/javascript" src="/javascript/jquery.touchswipe-min.js?v=huaek_e6HqTduvCNAN91dJolTyw"></script>
<script type="text/javascript" src="/javascript/jquery.base64-min.js?v=VwV1zeVqKZj5FCAdlK0q5NRxbBg"></script>
<script type="text/javascript" src="/javascript/alm-min.js?v=Y5gm6B0b4Kx2YHNObNrgEeBgXlY"></script>
<script type="text/javascript" src="/javascript/taxonomy-browser-min.js?v=vBVMuDMYkGJCXIUxLe35GoyiJNw"></script>
<script type="text/javascript" src="/javascript/jquery.filterize-min.js?v=j0ZKVnHyk2nhFy8eIuNJkp7xaM0"></script>
<script type="text/javascript" src="/javascript/plosone-min.js?v=TK4H4arL_XBSwwJq+K1N3kqYfAI"></script>
<script type="text/javascript" src="/javascript/twitter-min.js?v=xKgcxLsQFXy+at1ao1NVke8nFlM"></script>
<script type="text/javascript" src="/javascript/crossmark.1.4-min.js?v=3FO4k0SjwTaGNnKGNSqthar1080"></script>
<script type="text/javascript">
  var _sf_async_config={uid:16579,domain:"plosone.org"};
  (function(){
    function loadChartbeat() {
      window._sf_endpt=(new Date()).getTime();
      var e = document.createElement('script');
      e.setAttribute('language', 'javascript');
      e.setAttribute('type', 'text/javascript');
      e.setAttribute('src',
          (("https:" == document.location.protocol) ? "https://a248.e.akamai.net/chartbeat.download.akamai.com/102508/" : "http://static.chartbeat.com/") +
              "js/chartbeat.js");
      document.body.appendChild(e);
    }
    var oldonload = window.onload;
    window.onload = (typeof window.onload != 'function') ?
        loadChartbeat : function() { oldonload(); loadChartbeat(); };
  })();
</script>
<!-- <script type="application/javascript" src="http://crossmark.crossref.org/javascripts/v1.3/crossmark.min.js"></script> -->

</body>
</html>
