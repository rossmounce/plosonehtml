

 



<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"
      xmlns:foaf="http://xmlns.com/foaf/0.1/"
      xmlns:dc="http://purl.org/dc/terms/"
      xmlns:doi="http://dx.doi.org/"
      xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
      xmlns:xsd="http://www.w3.org/2001/XMLSchema-datatypes#"
      lang="en" xml:lang="en"
      itemscope itemtype="http://schema.org/Article"
      class="no-js">
<head prefix="og: http://ogp.me/ns#">
  <title>PLOS ONE: Parts, Wholes, and Context in Reading: A Triple Dissociation</title>


<link rel="stylesheet" type="text/css"  href="/css/global-min.css?v=izteQ6tu7kgsJZW_xmrYizvKiHM" />


    <!--[if lte IE 7]>
<link rel="stylesheet" type="text/css"  href="/css/lte_ie7-min.css?v=3bykQUyQmReeuobVyPozcJ9LxRc" />
    <![endif]-->


<link rel="stylesheet" type="text/css"  href="/css/jquery-ui-min.css?v=eXDHTEJM0lIAmDe5k0I0Ad4nxNo" />


<link rel="stylesheet" type="text/css"  href="/css/journal.css?v=T7ZVxJfgk9jNxLAJ2qHz1vZpgYU" />


<link rel="stylesheet" type="text/css" media="print" href="/css/print-min.css?v=T5lb0B3q6EXBsuDluc5V5w+AkRc" />


  <link rel="stylesheet" href="http://f.fontdeck.com/s/css/js/www.plosone.org/24557.css" type="text/css"/>

  <!--chartbeat -->
  <script type="text/javascript">var _sf_startpt = (new Date()).getTime()</script>
  <script>document.documentElement.className += ' js';</script>

  <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <meta http-equiv="X-UA-Compatible" content="IE=EmulateIE7; IE=EmulateIE9"/>
  <meta name="description" content="PLOS ONE: an inclusive, peer-reviewed, open-access resource from the PUBLIC LIBRARY OF SCIENCE. Reports of well-performed scientific studies from all disciplines freely available to the whole world."/>
  <meta name="keywords" content="PLOS, Public Library of Science, Open Access, Open-Access, Science, Medicine, Biology, Research, Peer-review, Inclusive, Interdisciplinary, Ante-disciplinary, Physics, Chemistry, Engineering"/>
  <meta name="almHost" content="http://alm.plos.org/api/v3/articles"/>
  <meta name="searchHost" content="http://api.plos.org/search" />
  <meta name="termsHost" content="http://api.plos.org/terms" />
  <meta name="solrApiKey" content="plos"/>
  <meta name="almAPIKey" content="3pezRBRXdyzYW6ztfwft" />
  <meta name="currentJournal" content="PLoSONE" />
  <meta name="almRequestBatchSize" content="" />

  <meta name="citation_publisher" content="Public Library of Science"/>
  <meta name="citation_doi" content="10.1371/journal.pone.0000680"/>
  <meta name="dc.identifier" content="10.1371/journal.pone.0000680" />

    <meta name="citation_title" content="Parts, Wholes, and Context in Reading: A Triple Dissociation"/>
    <meta itemprop="name" content="Parts, Wholes, and Context in Reading: A Triple Dissociation"/>

      <meta name="citation_author" content="Denis G. Pelli"/>
            <meta name="citation_author_institution" content="Psychology and Neural Science, New York University, New York, New York, United States of America"/>
      <meta name="citation_author" content="Katharine A. Tillman"/>
            <meta name="citation_author_institution" content="Psychology and Neural Science, New York University, New York, New York, United States of America"/>

    <meta name="citation_date" content="2007/8/1"/>

  <meta name="citation_pdf_url" content="http://dx.plos.org/10.1371/journal.pone.0000680.pdf" />

      <meta name="citation_journal_title" content="PLOS ONE" />
    <meta name="citation_firstpage" content="e680"/>
    <meta name="citation_issue" content="8"/>
    <meta name="citation_volume" content="2"/>
    <meta name="citation_issn" content="1932-6203"/>

    <meta name="citation_journal_abbrev" content="PLoS ONE" />

      <meta name="citation_reference" content="citation_title=Cognitive psychology.; citation_author=U Neisser; citation_number=1; citation_date=1967; citation_publisher=Appleton-Century-Croft; " />
      <meta name="citation_reference" content="citation_title=Probability summation and regional variation in contrast sensitivity across the visual field.; citation_author=JG Robson; citation_author=N Graham; citation_journal_title=Vision Res; citation_volume=21; citation_number=2; citation_pages=409-418; citation_date=1981; " />
      <meta name="citation_reference" content="citation_title=Feature detection and letter identification.; citation_author=DG Pelli; citation_author=CW Burns; citation_author=B Farell; citation_author=DC Moore-Page; citation_journal_title=Vision Res; citation_volume=46; citation_number=3; citation_pages=4646-4674; citation_date=2006; " />
      <meta name="citation_reference" content="citation_title=A feature-integration theory of attention.; citation_author=AM Treisman; citation_author=G Gelade; citation_journal_title=Cognit Psychol,; citation_volume=12(1); citation_number=4; citation_pages=97-136; citation_date=1980; " />
      <meta name="citation_reference" content="citation_title=Basic objects in natural categories.; citation_author=E Rosch; citation_author=C Mervis; citation_author=W Gray; citation_author=D Johnson; citation_author=P Boyes-Braem; citation_journal_title=Cognitive Psychology; citation_volume=8; citation_number=5; citation_pages=382-439; citation_date=1976; " />
      <meta name="citation_reference" content="citation_title=Visual feature integration in a world of objects.; citation_author=W Prinzmetal; citation_journal_title=Current Directions in Psychological Science; citation_volume=4; citation_number=6; citation_pages=90-94; citation_date=1995; " />
      <meta name="citation_reference" content="citation_title=The remarkable inefficiency of word recognition.; citation_author=DG Pelli; citation_author=B Farell; citation_author=DC Moore; citation_journal_title=Nature; citation_volume=423; citation_number=7; citation_pages=752-756; citation_date=2003; " />
      <meta name="citation_reference" content="citation_title=Are faces processed like words? A diagnostic test for recognition by parts.; citation_author=M Martelli; citation_author=NJ Majaj; citation_author=DG Pelli; citation_journal_title=Journal of Vision; citation_volume=5(1); citation_number=8; citation_pages=6, 58-70; citation_date=2005; " />
      <meta name="citation_reference" content="citation_title=Recognition-by-components: a theory of human image understanding.; citation_author=I Biederman; citation_journal_title=Psychol Rev; citation_volume=94; citation_number=9; citation_pages=115-147; citation_date=1987; " />
      <meta name="citation_reference" content="citation_title=The viewpoint complexity of an object-recognition task.; citation_author=BS Tjan; citation_author=GE Legge; citation_journal_title=Vision Res; citation_volume=38; citation_number=10; citation_pages=2335-2350; citation_date=1998; " />
      <meta name="citation_reference" content="citation_title=Objects, parts, and categories.; citation_author=B Tversky; citation_author=K Hemenway; citation_journal_title=J Exp Psychol Gen; citation_volume=113; citation_number=11; citation_pages=169-197; citation_date=1984; " />
      <meta name="citation_reference" content="citation_title=The role of parts and spatial relations in object identification.; citation_author=CB Cave; citation_author=SM Kosslyn; citation_journal_title=Perception; citation_volume=22; citation_number=12; citation_pages=229-248; citation_date=1993; " />
      <meta name="citation_reference" content="citation_title=Looking at upside-down faces.; citation_author=RK Yin; citation_journal_title=Journal of Experimental Psychology; citation_volume=81(1); citation_number=13; citation_pages=141-145; citation_date=1969; " />
      <meta name="citation_reference" content="citation_title=Parts and wholes in face recognition.; citation_author=JW Tanaka; citation_author=MJ Farah; citation_journal_title=Q J Exp Psychol A; citation_volume=46; citation_number=14; citation_pages=225-245; citation_date=1993; " />
      <meta name="citation_reference" content="citation_title=Configurational information in face perception.; citation_author=AW Young; citation_author=D Hellawell; citation_author=DC Hay; citation_journal_title=Perception; citation_volume=16; citation_number=15; citation_pages=747-759; citation_date=1987; " />
      <meta name="citation_reference" content="citation_title=The many faces of configural processing.; citation_author=D Maurer; citation_author=RL Grand; citation_author=CJ Mondloch; citation_journal_title=Trends Cogn Sci; citation_volume=6; citation_number=16; citation_pages=255-260; citation_date=2002; " />
      <meta name="citation_reference" content="citation_title=Expertise in object and face recognition.; citation_author=J Tanaka; citation_author=I Gauthier; citation_number=17; citation_pages=83-125; citation_date=1997; citation_publisher=Academic Press; " />
      <meta name="citation_reference" content="citation_title=Learning to read: the great debate; an inquiry into the science, art, and ideology of old and new methods of teaching children to read, 1910-1965.; citation_author=JS Chall; citation_number=18; citation_date=1967; citation_publisher=McGraw-Hill; " />
      <meta name="citation_reference" content="citation_title=How research might inform the debate about early reading acquisition.; citation_author=KE Stanovich; citation_author=PJ Stanovich; citation_journal_title=Journal of Research in Reading; citation_volume=18; citation_number=19; citation_pages=87-105; citation_date=1995; " />
      <meta name="citation_reference" content="citation_title=One second of reading.; citation_author=PB Gough; citation_number=20; citation_date=1972; citation_publisher=MIT Press; " />
      <meta name="citation_reference" content="citation_title=How should reading be taught?; citation_author=K Rayner; citation_author=BR Foorman; citation_author=CA Perfetti; citation_author=D Pesetsky; citation_author=MS Seidenberg; citation_journal_title=Sci Am; citation_volume=286; citation_number=21; citation_pages=84-91; citation_date=2002; " />
      <meta name="citation_reference" content="citation_title=Words and context.; citation_author=PB Gough; citation_author=JA Afford; citation_author=PH Wilcox; citation_number=22; citation_date=1981; citation_publisher=Erlbaum; " />
      <meta name="citation_reference" content="citation_title=Visual interference in the parafoveal recognition of initial and final letters of words.; citation_author=H Bouma; citation_journal_title=Vision Res; citation_volume=13; citation_number=23; citation_pages=767-782; citation_date=1973; " />
      <meta name="citation_reference" content="citation_title=Crowding is unlike ordinary masking: Distinguishing feature integration from detection.; citation_author=DG Pelli; citation_author=M Palomares; citation_author=NJ Majaj; citation_journal_title=Journal of Vision; citation_volume=4(12); citation_number=24; citation_pages=12, 1136-1169; citation_date=2004; " />
      <meta name="citation_reference" content="citation_title=Crowding and eccentricity determine reading rate.; citation_author=DG Pelli; citation_author=KA Tillman; citation_author=J Freeman; citation_author=M Su; citation_author=TD Berger; citation_author=NJ Majaj; citation_journal_title=Journal of Vision; citation_volume=7(2); citation_number=25; citation_date=2007; " />
      <meta name="citation_reference" content="citation_title=The effect of type size and case alternation on word identification.; citation_author=F Smith; citation_author=D Lott; citation_author=B Cronnell; citation_journal_title=Am J Psychol; citation_volume=82; citation_number=26; citation_pages=248-253; citation_date=1969; " />
      <meta name="citation_reference" content="citation_title=Psychophysics of reading. XVIII. The effect of print size on reading speed in normal peripheral vision.; citation_author=ST Chung; citation_author=JS Mansfield; citation_author=GE Legge; citation_journal_title=Vision Res; citation_volume=38; citation_number=27; citation_pages=2949-2962; citation_date=1998; " />
      <meta name="citation_reference" content="citation_title=Perceptual span for letter distinctions during reading.; citation_author=NR Underwood; citation_author=GW McConkie; citation_journal_title=Reading Research Quarterly; citation_volume=20; citation_number=28; citation_pages=153-162; citation_date=1985; " />
      <meta name="citation_reference" content="citation_title=Interaction effects in parafoveal letter recognition.; citation_author=H Bouma; citation_journal_title=Nature; citation_volume=226; citation_number=29; citation_pages=177-178; citation_date=1970; " />
      <meta name="citation_reference" content="citation_title=Familiarity of configuration vs discriminability of features in the visual identification of words.; citation_author=F Smith; citation_journal_title=Psychonomic Science,; citation_volume=14; citation_number=30; citation_pages=261-3; citation_date=1969; " />
      <meta name="citation_reference" content="citation_title=Reading: letters, words, and their spatial-frequency content.; citation_author=PJ Beckmann; citation_author=GE Legge; citation_author=A Luebker; citation_journal_title=SID 91 Digest,; citation_number=31; citation_pages=106-108; citation_date=1991; " />
      <meta name="citation_reference" content="citation_title=“Influence of case and block type on visual word recognition,” paper presented at the 35th Annual Meeting of the Psychonomics Society, St. Louis, MO, November 12, 1994; citation_author=PA Allen; citation_author=B Wallace; citation_author=S Sperry; citation_author=H Vires-Collins; citation_number=32; " />
      <meta name="citation_reference" content="citation_title=Case alternation impairs word identification. Bull. Psychon.; citation_author=M Coltheart; citation_author=R Freeman; citation_journal_title=Soc.,; citation_volume=3; citation_number=33; citation_pages=102-104; citation_date=1974; " />
      <meta name="citation_reference" content="citation_title=Preliminary letter identification in the perception of words and nonwords.; citation_author=JL McClelland; citation_journal_title=J Exp Psychol [Hum Percept],; citation_volume=2; citation_number=34; citation_pages=80-91; citation_date=1976; " />
      <meta name="citation_reference" content="citation_title=Loves music, loves to dance.; citation_author=MH Clark; citation_number=35; citation_date=1991; citation_publisher=Simon&Schuster; " />
      <meta name="citation_reference" content="citation_title=Rapid serial visual presentation (RSVP): A method for studying language processing.; citation_author=MC Potter; citation_number=36; citation_pages=91-118; citation_date=1984; citation_publisher=Erlbaum; " />
      <meta name="citation_reference" content="citation_title=Process decomposition from double dissociation of subprocesses.; citation_author=S Sternberg; citation_journal_title=Cortex; citation_volume=39; citation_number=37; citation_pages=180-182; citation_date=2003; " />
      <meta name="citation_reference" content="citation_title=Familiarity of configuration vs discriminability of features in the visual identification of words.; citation_author=F Smith; citation_journal_title=Psychonomic Science,; citation_volume=14; citation_number=38; citation_pages=261-3; citation_date=1969; " />
      <meta name="citation_reference" content="citation_title=Sensitivity to syntax in visual cortex.; citation_author=S Dikker; citation_author=H Rabagliati; citation_author=L Pylkkänen; citation_number=39; citation_date=submitted; " />
      <meta name="citation_reference" content="citation_title=The role of structural prediction in rapid syntactic analysis.; citation_author=E Lau; citation_author=C Stroud; citation_author=S Plesch; citation_author=C Phillips; citation_journal_title=Brain and Language; citation_volume=98; citation_number=40; citation_pages=74-88; citation_date=2006; " />
      <meta name="citation_reference" content="citation_title=A visual M170 effect of morphological complexity.; citation_author=E Zweig; citation_author=L Pylkkänen; citation_number=41; citation_date=submitted; " />
      <meta name="citation_reference" content="citation_title=Shapes as cues to word recognition.; citation_author=P Groff; citation_journal_title=Visible Language; citation_volume=9; citation_number=42; citation_pages=67-71; citation_date=1975; " />
      <meta name="citation_reference" content="citation_title=Word shape's in poor shape for the race to the lexicon.; citation_author=KR Paap; citation_author=SL Newsome; citation_author=RW Noel; citation_journal_title=J Exp Psychol Hum Percept Perform; citation_volume=10; citation_number=43; citation_pages=413-428; citation_date=1984; " />
      <meta name="citation_reference" content="citation_title=Reading mutilated text.; citation_author=K Rayner; citation_author=JS Kaiser; citation_journal_title=Journal of Educational Psychology; citation_volume=67; citation_number=44; citation_pages=301-306; citation_date=1975; " />
      <meta name="citation_reference" content="citation_title=The psychology and pedagogy of reading.; citation_author=EB Huey; citation_number=45; citation_date=1908; citation_publisher=Macmillan; " />
      <meta name="citation_reference" content="citation_title=E-Z Reader: A cognitive-control, serial-attention model of eye-movement behavior during reading.; citation_author=ED Reichle; citation_author=A Pollatsek; citation_author=K Rayner; citation_journal_title=Cognitive Systems Research,; citation_volume=7; citation_number=46; citation_pages=4-22; citation_date=2006; " />
      <meta name="citation_reference" content="citation_title=SWIFT: a dynamical model of saccade generation during reading.; citation_author=R Engbert; citation_author=A Nuthmann; citation_author=EM Richter; citation_author=R Kliegl; citation_journal_title=Psychol Rev; citation_volume=112; citation_number=47; citation_pages=777-813; citation_date=2005; " />
      <meta name="citation_reference" content="citation_title=Mr. Chips 2002: new insights from an ideal-observer model of reading.; citation_author=GE Legge; citation_author=TA Hooven; citation_author=TS Klitz; citation_author=JS Mansfield; citation_author=BS Tjan; citation_journal_title=Vision Res; citation_volume=42; citation_number=48; citation_pages=2219-2234; citation_date=2002; " />
      <meta name="citation_reference" content="citation_title=DRC: a dual route cascaded model of visual word recognition and reading aloud.; citation_author=M Coltheart; citation_author=K Rastle; citation_author=C Perry; citation_author=R Langdon; citation_author=J Ziegler; citation_journal_title=Psychol Rev; citation_volume=108; citation_number=49; citation_pages=204-256; citation_date=2001; " />
      <meta name="citation_reference" content="citation_title=Effect of letter spacing on visual span and reading speed.; citation_author=D Yu; citation_author=S-H Cheung; citation_author=GE Legge; citation_author=STL Chung; citation_journal_title=Journal of Vision; citation_volume=7(2); citation_number=50; citation_pages=2, 1-10; citation_date=2007; " />
      <meta name="citation_reference" content="citation_title=Conceptual processing of text during skimming and rapid sequential reading.; citation_author=ME Masson; citation_journal_title=Memory&Cognition; citation_volume=11; citation_number=51; citation_pages=262-274; citation_date=1983; " />
      <meta name="citation_reference" content="citation_title=The VideoToolbox software for visual psychophysics: transforming numbers into movies.; citation_author=DG Pelli; citation_journal_title=Spat Vis; citation_volume=10; citation_number=52; citation_pages=437-442; citation_date=1997; " />
      <meta name="citation_reference" content="citation_title=The Psychophysics Toolbox.; citation_author=DH Brainard; citation_journal_title=Spat Vis; citation_volume=10; citation_number=53; citation_pages=433-436; citation_date=1997; " />
      <meta name="citation_reference" content="citation_title=Psychophysics of Reading in Normal and Low Vision.; citation_author=GE Legge; citation_number=54; citation_date=2007; citation_publisher=Lawrence Erlbaum Associates; " />
      <meta name="citation_reference" content="citation_title=QUEST: a Bayesian adaptive psychometric method.; citation_author=AB Watson; citation_author=DG Pelli; citation_journal_title=Percept Psychophys; citation_volume=33; citation_number=55; citation_pages=113-120; citation_date=1983; " />
      <meta name="citation_reference" content="citation_title=The “silent substitution” method in visual research.; citation_author=O Estevez; citation_author=H Spekreijse; citation_journal_title=Vision Res; citation_volume=22; citation_number=56; citation_pages=681-691; citation_date=1982; " />
      <meta name="citation_reference" content="citation_title=Amblyopic reading is crowded.; citation_author=DM Levi; citation_author=S Song; citation_author=DG Pelli; citation_journal_title=Journal of Vision; citation_volume=7(2); citation_number=57; citation_date=2007; " />
      <meta name="citation_reference" content="citation_title=An escape from crowding.; citation_author=J Freeman; citation_author=DG Pelli; citation_journal_title=Journal of Vision; citation_volume=7(2); citation_number=58; citation_date=2007; " />

  <link rel="canonical" href="http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0000680" />

    <meta name="twitter:card" content="summary"/>
    <meta name="twitter:site" content="@plosone"/>
    <meta name="twitter:title" content="Parts, Wholes, and Context in Reading: A Triple Dissociation"/>
    <meta name="twitter:description" content="Research in object recognition has tried to distinguish holistic recognition from recognition by parts. One can also guess an object from its context. Words are objects, and how we recognize them is the core question of reading research. Do fast readers rely most on letter-by-letter decoding (i.e., recognition by parts), whole word shape, or sentence context? We manipulated the text to selectively knock out each source of information while sparing the others. Surprisingly, the effects of the knockouts on reading rate reveal a triple dissociation. Each reading process always contributes the same number of words per minute, regardless of whether the other processes are operating."/>
      <meta name="twitter:image" content="http://dx.plos.org/10.1371/journal.pone.0000680.g002"/>

  <meta property="og:title" content="Parts, Wholes, and Context in Reading: A Triple Dissociation" />
  <meta property="og:type" content="article" />
  <meta property="og:url" content="http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0000680" />

 <!--end articleInfoX-->

  <link rel="pingback" href="http://www.plosone.org/pingback" />


  <link rel="shortcut icon" href="/images/favicon.ico" type="image/x-icon"/>
  <link rel="home" title="home" href="/"/>
  <link rel="alternate" type="application/rss+xml"
        title="PLOS ONE: New Articles"
        href="http://www.plosone.org/article/feed"/>
</head>
<body>

  <div id="page-wrap">
    <div id="topbanner" class="cf">

<!-- Div for the ad at the top of journal home page-->
<div class="center">
  <div class="title">Advertisement</div>
  <iframe id='a3ac9da4' name='a3ac9da4'
    src='http://ads.plos.org/www/delivery/afr.php?zoneid=345&amp;cb=5047'
    frameborder='0' scrolling='no' width='730' height='90'>
    <a href='http://ads.plos.org/www/delivery/ck.php?n=a3ac9da4&amp;cb=9733'
      target='_top'><img src='http://ads.plos.org/www/delivery/avw.php?zoneid=345&amp;cb=2630&amp;n=a3ac9da4'
      border='0' alt=''/>
    </a>
  </iframe>
</div>    </div>

    <div id="pagehdr-wrap">
      <div id="pagehdr">
        <div id="user" class="nav">
          <ul>
            <li><a href="http://www.plos.org">plos.org</a></li>
            <li><a href="https://register.plos.org/ambra-registration/register.action">create account</a></li>
            <li class="btn-style"><a
              href="/user/secure/secureRedirect.action?goTo=%2Farticle%2FfetchArticle.action%3FarticleURI%3Dinfo%253Adoi%252F10.1371%252Fjournal.pone.0000680">sign in</a>
            </li>
          </ul>
        </div>
        <div class="logo">
          <a href="/"><img src="/images/logo.png" alt="PLOS ONE"></a>
        </div>

<div id="nav-main" class="nav">
  <ul>
        <li id="mn-01"><a href="/taxonomy" class="areas-link">Subject Areas</a></li>
    <li id="mn-02"><a href="javascript:void(0);">For Authors</a>
      <div class="submenu" style="width: 540px; margin-left: -300px;">
        <div class="block">
          <div class="submit-script">
            <h3>Submit your Manuscript</h3>
            <ul>
              <li>Fair, rigorous peer review</li>
              <li>Broad scope and wide reach</li>
            </ul>
            <a href="/static/submissionInstructions" class="btn">get started</a>
          </div>
        </div>
        <div class="menu">
          <ul>
            <li><a href="/static/publish">Why Publish with PLOS ONE</a></li>
            <li><a href="/static/publication">Publication Criteria</a></li>
            <li><a href="/static/editorial">Editorial Policies</a></li>
            <li><a href="/static/guidelines">Preparing A Manuscript</a></li>
            <li><a href="/static/figureGuidelines">Figure and Table Guidelines</a></li>
          <li><a href="/static/supportingInformation">Supporting Information Guidelines</a></li>
            <li><a href="/static/submissionInstructions">Submitting a Manuscript</a></li>
          </ul>
        </div>
      </div>
    </li>

    <li id="mn-03"><a href="javascript:void(0);">About Us</a>
      <div class="submenu" style="width:248px; margin-left:-30px;">
        <div class="menu">
          <ul>
            <li><a href="/static/information">Journal Information</a></li>
            <li><a href="/static/edboard">Editorial Board</a></li>
            <li><a href="/static/reviewerGuidelines">Reviewer Guidelines</a></li>
            <li><a href="/static/almInfo">Article-Level Metrics</a></li>
            <li><a href="/static/license">Open-Access License</a></li>
            <li><a href="/static/downloads">Media Downloads</a></li>
            <li><a href="/static/commentGuidelines">Guidelines for Comments</a></li>
            <li><a href="/static/corrections">Corrections</a></li>
            <li><a href="/static/help">Help Using this Site</a></li>
            <li><a href="/static/contact">Contact Us</a></li>
          </ul>
        </div>
      </div>
    </li>
  </ul>
<div id="db">
  <form name="searchForm" action="/search/simple?noSearchFlag=true&amp;query=&amp;articleURI=info%3Adoi%2F10.1371%2Fjournal.pone.0000680" method="get" >
<input type="hidden" name="from" value="globalSimpleSearch" id="from"/><input type="hidden" name="filterJournals" value="PLoSONE" id="filterJournals"/>    <fieldset>
      <legend>Search</legend>
      <label for="search">Search</label>
      <div class="wrap">
        <input id="search" type="text" name="query" placeholder="Search">
        <input type="image" alt="SEARCH" src="/images/icon.search.gif">
      </div>
    </fieldset>
  </form>
    <a id="advSearch" href="/search/advanced?noSearchFlag=true&amp;query=&amp;articleURI=info%3Adoi%2F10.1371%2Fjournal.pone.0000680&filterJournals=PLoSONE">advanced search</a>
</div></div>

      </div>
      <!-- pagehdr-->
    </div>
    <!-- pagehdr-wrap -->

  <!--body and html tags gets closed in global_footer.ftl-->
<script type="text/javascript" src="/javascript/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<div id="pagebdy-wrap">
  <div id="pagebdy">

    <div id="article-block" class="cf">

<div class="article-meta cf">
  <ul id="almSignPost" style="display: none;"></ul>
  <div class="article-type">
    <span class="type oa">Open Access</span>
      <span class="type pr">Peer-Reviewed</span>
  </div>
</div>

<div class="header" id="hdr-article">

<div class="article-kicker">
      <span id="article-type-heading">
        Research Article
      </span>
</div>  <h1 property="dc:title" datatype="" rel="dc:type" href="http://purl.org/dc/dcmitype/Text">
    Parts, Wholes, and Context in Reading: A Triple Dissociation
  </h1>

  <ul class="authors">
      <li>


        <span rel="dc:creator" class="author">
          <span class="person" property="foaf:name" typeof="foaf:Person">
            Denis G. Pelli
              <span class="corresponding">mail</span>, 
          </span>
        </span>

          <div class="author_meta">
            <div class="author_inner">


              
              <p><span class="email">*</span>To whom correspondence should be addressed. E-mail: <a href="mailto:denis.pelli@nyu.edu">denis.pelli@nyu.edu</a></p>

                <p>Affiliation:
                  Psychology and Neural Science, New York University, New York, New York, United States of America
                </p>


              <span class="close">X</span>

            </div>
          </div>
      </li>
      <li>


        <span rel="dc:creator" class="author">
          <span class="person" property="foaf:name" typeof="foaf:Person">
            Katharine A. Tillman
          </span>
        </span>

          <div class="author_meta">
            <div class="author_inner">


              
              

                <p>Affiliation:
                  Psychology and Neural Science, New York University, New York, New York, United States of America
                </p>


              <span class="close">X</span>

            </div>
          </div>
      </li>
  </ul>
  <ul class="date-doi-line">
    <li>Published: August 01, 2007</li>
    <li>DOI: 10.1371/journal.pone.0000680</li>
  </ul>


</div><!--end header-->
<div class="main cf" id="pjax-container">
  

<div class="nav items-5" id="nav-article">
  <ul>
  <li>
        <span class="active" name="article">Article</span>
  </li>
  <li>
      <a href="/article/authors/info%3Adoi%2F10.1371%2Fjournal.pone.0000680" name="authors">About the Authors</a>
  </li>
  <li>
      <a href="/article/metrics/info%3Adoi%2F10.1371%2Fjournal.pone.0000680" name="metrics">Metrics</a>
  </li>
  <li>
      <a href="/article/comments/info%3Adoi%2F10.1371%2Fjournal.pone.0000680" name="comments">Comments</a>
  </li>
  <li>
      <a href="/article/related/info%3Adoi%2F10.1371%2Fjournal.pone.0000680" name="related">Related Content</a>
  </li>
  </ul>
</div>

<script type="text/javascript">
  var selected_tab = "article";
</script>
  <div id="figure-thmbs" class="carousel cf">
    <div class="wrapper">
      <div class="slider">
              <div class="item">
                <a href="#pone-0000680-g001" data-doi="info:doi/10.1371/journal.pone.0000680" data-uri="info:doi/10.1371/journal.pone.0000680.g001" title="Figure 1">
                  <span class="thmb-wrap">
                    <img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000680.g001&representation=PNG_I" alt="">
                  </span>
                </a>
              </div>
              <div class="item">
                <a href="#pone-0000680-g002" data-doi="info:doi/10.1371/journal.pone.0000680" data-uri="info:doi/10.1371/journal.pone.0000680.g002" title="Figure 2">
                  <span class="thmb-wrap">
                    <img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000680.g002&representation=PNG_I" alt="">
                  </span>
                </a>
              </div>
              <div class="item">
                <a href="#pone-0000680-t001" data-doi="info:doi/10.1371/journal.pone.0000680" data-uri="info:doi/10.1371/journal.pone.0000680.t001" title="Table 1">
                  <span class="thmb-wrap">
                    <img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000680.t001&representation=PNG_I" alt="">
                  </span>
                </a>
              </div>
              <div class="item">
                <a href="#pone-0000680-t002" data-doi="info:doi/10.1371/journal.pone.0000680" data-uri="info:doi/10.1371/journal.pone.0000680.t002" title="Table 2">
                  <span class="thmb-wrap">
                    <img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000680.t002&representation=PNG_I" alt="">
                  </span>
                </a>
              </div>
              <div class="item">
                <a href="#pone-0000680-t003" data-doi="info:doi/10.1371/journal.pone.0000680" data-uri="info:doi/10.1371/journal.pone.0000680.t003" title="Table 3">
                  <span class="thmb-wrap">
                    <img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000680.t003&representation=PNG_I" alt="">
                  </span>
                </a>
              </div>
              <div class="item">
                <a href="#pone-0000680-t004" data-doi="info:doi/10.1371/journal.pone.0000680" data-uri="info:doi/10.1371/journal.pone.0000680.t004" title="Table 4">
                  <span class="thmb-wrap">
                    <img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000680.t004&representation=PNG_I" alt="">
                  </span>
                </a>
              </div>
      </div>
    </div>
  </div>

  <div class="nav-col">
    <div class="nav" id="nav-article-page">
      <ul>
        <li class="nav-col-comments"><a href="/article/comments/info%3Adoi%2F10.1371%2Fjournal.pone.0000680">Reader Comments (10)</a></li>
          <li id="nav-figures"><a data-doi="info:doi/10.1371/journal.pone.0000680" >Figures</a></li>
      </ul>
    </div>
  </div>

  <div class="article">







<div class="abstract"><a id="abstract0" name="abstract0" toc="abstract0" title="Abstract"></a><h2>Abstract</h2><a id="article1.front1.article-meta1.abstract1.p1" name="article1.front1.article-meta1.abstract1.p1"></a><p>Research in object recognition has tried to distinguish holistic recognition from recognition by parts. One can also guess an object from its context. Words are objects, and how we recognize them is the core question of reading research. Do fast readers rely most on letter-by-letter decoding (i.e., recognition by parts), whole word shape, or sentence context? We manipulated the text to selectively knock out each source of information while sparing the others. Surprisingly, the effects of the knockouts on reading rate reveal a triple dissociation. Each reading process always contributes the same number of words per minute, regardless of whether the other processes are operating.</p>
</div>


<div class="articleinfo"><p><strong>Citation: </strong>Pelli DG, Tillman KA (2007) Parts, Wholes, and Context in Reading: A Triple Dissociation. PLoS ONE 2(8):
          e680.
            doi:10.1371/journal.pone.0000680</p><p><strong>Academic Editor: </strong>Aldo Rustichini, University of Minnesota, United States of America</p><p><strong>Received:</strong> January 22, 2007; <strong>Accepted:</strong> May 19, 2007; <strong>Published:</strong> August 1, 2007</p><p><strong>Copyright:</strong> © 2007 Pelli, Tillman. This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p><p><strong>Funding: </strong>Supported by NIH grant R01-EY004432</p><p><strong>Competing interests:</strong> The authors have declared that no competing interests exist.</p></div>





<div id="section1" class="section"><a id="s1" name="s1" toc="s1" title="Introduction"></a><h3>Introduction</h3><a id="article1.body1.sec1.p1" name="article1.body1.sec1.p1"></a><p>We take reading to be serial object recognition, where each word is an object. What are the roles of parts, wholes, and context in object recognition? After a hundred years of research into how people identify objects—discrete, nameable, visual stimuli—there seems to be a tentative consensus that the first step is independent feature detection and that the last step is categorization <a href="#pone.0000680-Neisser1">[1]</a>–<a href="#pone.0000680-Rosch1">[5]</a>. What happens in between is less clear. In particular, must the detected features be combined into individual “parts” that must in turn be combined before the object is identified, or is the whole object recognized in one fell swoop? <a href="#pone.0000680-Prinzmetal1">[6]</a>.</p>
<a id="article1.body1.sec1.p2" name="article1.body1.sec1.p2"></a><p>Many papers on object recognition appeal to the Gestalt notion of the whole being greater than the sum of its parts, but have had only limited success in finding experimental paradigms that bear on that. Experiments using words and faces have found an advantage for identifying whole objects over isolated parts (letters and facial features), which has been taken as evidence for holistic processing, but the effects are not large enough to rule out a solely parts-based process <a href="#pone.0000680-Pelli2">[7]</a>, <a href="#pone.0000680-Martelli1">[8]</a>. Other attempts to distinguish holistic from by-parts processing have measured effects of occlusion, scrambling, viewpoint, expertise, inversion, mismatched and misaligned composites, and self-crowding <a href="#pone.0000680-Martelli1">[8]</a>–<a href="#pone.0000680-Young1">[15]</a>. Though every study presents data consistent with one process or the other, none of these tests, except scrambling and self-crowding, manages to rule out the alternative <a href="#pone.0000680-Maurer1">[see 16]</a>.</p>
<a id="article1.body1.sec1.p3" name="article1.body1.sec1.p3"></a><p>Past work has used qualitative tests to choose between holistic and by-parts processes. However, “the distinction between [holistic and by-parts] processing may be a continuum rather than a dichotomy” <a href="#pone.0000680-Tanaka2">[17]</a>. Some recognition tasks may benefit from both holistic and by-parts processes. If so, one might ask how much each process contributes. Information from the object's environment and the observer's prior knowledge can be used to recognize objects as well. We lump all task-relevant information other than the object itself into the catch-all “context”. Here we introduce quantitative measures of the contribution of each recognition process: by-parts, holistic, and context.</p>
<a id="article1.body1.sec1.p4" name="article1.body1.sec1.p4"></a><p>The question is: if parts, wholes, and context all play roles in object recognition, do the mental processes associated with them interact? Does impairing one process impair the others as well? Or, alternatively, if we remove one process, will the others continue working, unaffected? To explore this question, we turn to reading.</p>
<a id="article1.body1.sec1.p5" name="article1.body1.sec1.p5"></a><p>We want to know how people quickly and effortlessly recognize an object when there are a vast number of possibilities. Ordinary reading demonstrates this amazing human skill. In studying object recognition, reading is one of the few cases where one knows the composition: letters are parts, words are wholes, and sentences provide context. Using reading, we can attempt to isolate and measure the contributions of parts, wholes, and context to the recognition of words as objects.</p>
<a id="article1.body1.sec1.p6" name="article1.body1.sec1.p6"></a><p>This analysis addresses a central question in reading. What makes fast readers fast, and how should reading be taught to make everyone fast? This question has fuelled a century of reading wars <a href="#pone.0000680-Chall1">[18]</a>, <a href="#pone.0000680-Stanovich1">[19]</a>. Each of three processes, which we will call L, W, and S, has been championed at some time, along with a method of reading instruction tailored to emphasize it over the others. L, W, and S each take a different input, but all three processes emit words. Mechanical letter-by-letter decoding, “L”, was once disparaged as fit only for beginning readers. Today it is accepted as the basis for fast adult reading, and schools now teach it through practice in ‘phonics,’ grapheme-to-phoneme conversion <a href="#pone.0000680-Gough1">[20]</a>, <a href="#pone.0000680-Rayner1">[21]</a>. Now consider “S”. Text is somewhat predictable. Readers can predict the next word in a passage 20 to 35% of the time, depending on their reading experience <a href="#pone.0000680-Stanovich1">[19]</a>, <a href="#pone.0000680-Gough2">[22]</a>. In the whole-language method, children are encouraged to use the story and sentence context (S) to guess the next word. Lastly, holistic recognition of words by their shape, “W”, once seemed a promising visual account for fast recognition of words, supported by evidence from the Word Superiority Effect (but see ref. 7), and motivated the whole-word method, which had children memorize and read the same few “sight words” over and over. Work on ‘crowding’ has shown that words are not usually recognized as wholes, even by adults, but rather that the visual system must isolate and recognize the individual letters to get the word <a href="#pone.0000680-Martelli1">[8]</a>. However, when the word is crowded, it is impossible to isolate the letters. We call what can still be gleaned ‘word shape’.</p>
<a id="article1.body1.sec1.p7" name="article1.body1.sec1.p7"></a><p>Bouma showed that words can only be recognized when the letters are spaced far enough apart <a href="#pone.0000680-Bouma1">[23]</a>. This critical spacing depends on where the word is in the visual field and little else <a href="#pone.0000680-Pelli3">[24]</a>. When the letters are separated by less than the critical spacing, the reader cannot identify them, and the word is illegible. Critical spacing increases in proportion to distance from fixation. For text of any given letter spacing, there is a central field that is uncrowded, and a peripheral field that is crowded <a href="#pone.0000680-Pelli4">[25]</a>.</p>
<div class="figure" id="pone-0000680-g001"><div class="img"><a name="pone-0000680-g001" title="Click for larger image " href="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000680.g001&amp;representation=PNG_M" data-doi="info:doi/10.1371/journal.pone.0000680" data-uri="info:doi/10.1371/journal.pone.0000680.g001"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000680.g001&amp;representation=PNG_I" alt="thumbnail" class="thumbnail"></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000680.g001/powerpoint">
                    PPT
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000680.g001/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000680.g001/largerimage">
                    PNG
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000680.g001/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pone.0000680.g001.PNG_L"></span>)
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000680.g001/originalimage">
                    TIFF
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000680.g001/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pone.0000680.g001.TIF"></span>)
                </a></li></ul></div><p><strong>Figure 1.  <span>The LWS model of reading rate.</span></strong></p><a id="article1.body1.sec1.fig1.caption1.p1" name="article1.body1.sec1.fig1.caption1.p1"></a><p>The model (Eq. 1) has three parameters (<em>L, W, S</em>), the reading rates of the letter-by-letter, whole-word, and sentence-context processes. For each condition, the model predicts a reading rate (word/min) that is the sum of the rates of the spared processes. For each of 11 observers, we measured reading rates for all eight possible combinations of knockouts. The model was fit, separately, to the data for each observer. This table shows the model fit. The observer data are shown in <a href="#pone-0000680-t001">Table 1</a>. The fit's mean±SD, across observers, is shown for each condition. (The SE is about one third the SD.) The model fits every observer well (<a href="#pone-0000680-t002">Table 2</a>). The overall RMS error, across conditions and observers, is 22 word/min. The excellent fit of the additive model (Eq. 1) proves triple dissociation with a combination rule of summation <a href="#pone.0000680-Sternberg1">[37]</a>.</p>
<span>doi:10.1371/journal.pone.0000680.g001</span></div><a id="article1.body1.sec1.p8" name="article1.body1.sec1.p8"></a><p>Here we measure the contributions of the L, W, and S processes to reading rate by manipulating text in ways that selectively knock out each source of information while sparing the others. Scrambling word order knocks out the S information, which the reader uses to guess the word from its context:<a name="" id=""></a><span class="equation"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000680.e001&amp;representation=PNG" class="inline-graphic"></span><br></p>
<a id="article1.body1.sec1.p9" name="article1.body1.sec1.p9"></a><p>Alternating case knocks out the W information, which the reader uses to recognize words by their gross shape: <a name="" id=""></a><span class="equation"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000680.e002&amp;representation=PNG" class="inline-graphic"></span><br></p>
<a id="article1.body1.sec1.p10" name="article1.body1.sec1.p10"></a><p>Substituting similar letters (indistinguishable when viewed peripherally) knocks out the L information, which the reader uses to identify the word by identifying its letters:<a name="" id=""></a><span class="equation"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000680.e003&amp;representation=PNG" class="inline-graphic"></span><br></p>
<a id="article1.body1.sec1.p11" name="article1.body1.sec1.p11"></a><p>The alternating-case and word-shuffling manipulations are borrowed from the reading literature as-is <a href="#pone.0000680-Smith1">[26]</a>, <a href="#pone.0000680-Chung1">[27]</a>, but we used our knowledge of object recognition to refine the substitution paradigm <a href="#pone.0000680-Underwood1">[28]</a>. As you see, the L knockout is devastating, but we know it spares word shape, as defined here, because the substitutions are undetectable when crowded. Recognizing an object by parts (a word by letters) requires isolation of each part from the rest of the object <a href="#pone.0000680-Pelli3">[24]</a>, <a href="#pone.0000680-Bouma2">[29]</a>. When the isolation field is bigger than the word, which happens when the word is far from the center of gaze, the word can only be seen holistically. We use this–what can be seen holistically–as our definition of ‘word shape’. Thus, any two letter strings that are indistinguishable under these conditions have the same word shape.</p>
<a id="article1.body1.sec1.p12" name="article1.body1.sec1.p12"></a><p>Using peripheral viewing, we discovered, by trial and error, which letter substitutions could be made without affecting word shape. In the demonstration below, you can verify that our substitutions preserve the word shape of “Reading”, by fixing your eye on the plus and comparing the two words peripherally.<a name="" id=""></a><span class="equation"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000680.e004&amp;representation=PNG" class="inline-graphic"></span><br></p>
<a id="article1.body1.sec1.p13" name="article1.body1.sec1.p13"></a><p>They are indistinguishable even though only 3 of the 7 letters (d, i, and g) are the same. The letter substitutes that passed our test (indistinguishable when viewed in the periphery with flanker letters on both sides) are listed in <a href="#pone-0000680-g002">Figure 2</a>. This list was used for letter substitution.</p>
<a id="article1.body1.sec1.p14" name="article1.body1.sec1.p14"></a><p>Alternating case knocks out the holistic word process (W), which can identify some highly familiar words even when the letter (L) information is degraded by crowding or letter substitution. <a name="" id=""></a><span class="equation"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000680.e005&amp;representation=PNG" class="inline-graphic"></span><br></p>
<a id="article1.body1.sec1.p15" name="article1.body1.sec1.p15"></a><p>When fixating the plus, the word “and” on the left is obvious, even though the letters are crowded. You are using word shape to read “and”. On the right, you can see that there is a word, and you might even be able to get the exposed letters on the ends <a href="#pone.0000680-Bouma1">[23]</a>, but you can't read the word. Alternating case has destroyed word shape. Few words can be recognized by word shape alone, which is consistent with reports that alternating case has at most a small effect on reading speed and accuracy (26, 30–34).</p>
<a id="article1.body1.sec1.p16" name="article1.body1.sec1.p16"></a><p>We can be confident that each of these three manipulations affects only one of the three sources of word information in the text. But what about the corresponding recognition processes? The three kinds of information are distinct, but the processes may not be. Can we selectively knock out one process, or does impairing one process impair the others as well? By applying these manipulations one at a time, we measure how much each word-recognition process–L, W, and S–contributes to normal reading rate. By applying them in combination, we test the selectivity of the knockouts and discover the degree to which the reading processes depend on each other.</p>
<div class="figure" id="pone-0000680-g002"><div class="img"><a name="pone-0000680-g002" title="Click for larger image " href="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000680.g002&amp;representation=PNG_M" data-doi="info:doi/10.1371/journal.pone.0000680" data-uri="info:doi/10.1371/journal.pone.0000680.g002"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000680.g002&amp;representation=PNG_I" alt="thumbnail" class="thumbnail"></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000680.g002/powerpoint">
                    PPT
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000680.g002/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000680.g002/largerimage">
                    PNG
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000680.g002/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pone.0000680.g002.PNG_L"></span>)
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000680.g002/originalimage">
                    TIFF
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000680.g002/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pone.0000680.g002.TIF"></span>)
                </a></li></ul></div><p><strong>Figure 2.  <span>Letter substitutes.</span></strong></p><a id="article1.body1.sec1.fig2.caption1.p1" name="article1.body1.sec1.fig2.caption1.p1"></a><p>Each letter can be randomly replaced by any of its substitutes, including the letter itself. The font is Helvetica Neue LT 85 Heavy from Linotype.</p>
<span>doi:10.1371/journal.pone.0000680.g002</span></div></div>

<div id="section2" class="section"><a id="s2" name="s2" toc="s2" title="Results"></a><h3>Results</h3><a id="article1.body1.sec2.p1" name="article1.body1.sec2.p1"></a><p>We applied every combination of the three knockouts <span class="inline-formula"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000680.e006&amp;representation=PNG" class="inline-graphic"></span> to text from a bestselling Mary Higgins Clark murder mystery <a href="#pone.0000680-Clark1">[35]</a> and measured reading rate (<a href="#pone-0000680-g001">Figure 1</a>, <a href="#pone-0000680-t001">Tables 1</a>–<a href="#pone-0000680-t002">2</a>). These reading rates were collected using Rapid Serial Visual Presentation (RSVP) <a href="#pone.0000680-Potter1">[36]</a> in conjunction with a staircase procedure to find the threshold word presentation rate yielding 80% correct accuracy (see <a href="#s4">Materials and Methods</a>). We also measured rates of both oral and silent reading of printed pages (<a href="#pone-0000680-t003">Table 3</a>).</p>
<div class="figure" id="pone-0000680-t001"><div class="img"><a name="pone-0000680-t001" title="Click for larger image " href="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000680.t001&amp;representation=PNG_M" data-doi="info:doi/10.1371/journal.pone.0000680" data-uri="info:doi/10.1371/journal.pone.0000680.t001"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000680.t001&amp;representation=PNG_I" alt="thumbnail" class="thumbnail"></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000680.t001/powerpoint">
                    PPT
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000680.t001/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000680.t001/largerimage">
                    PNG
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000680.t001/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pone.0000680.t001.PNG_L"></span>)
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000680.t001/originalimage">
                    TIFF
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000680.t001/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pone.0000680.t001.TIF"></span>)
                </a></li></ul></div><p><strong>Table 1.  <span>Reading rate for each condition for each observer.</span></strong></p><span>doi:10.1371/journal.pone.0000680.t001</span></div><div class="figure" id="pone-0000680-t002"><div class="img"><a name="pone-0000680-t002" title="Click for larger image " href="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000680.t002&amp;representation=PNG_M" data-doi="info:doi/10.1371/journal.pone.0000680" data-uri="info:doi/10.1371/journal.pone.0000680.t002"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000680.t002&amp;representation=PNG_I" alt="thumbnail" class="thumbnail"></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000680.t002/powerpoint">
                    PPT
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000680.t002/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000680.t002/largerimage">
                    PNG
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000680.t002/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pone.0000680.t002.PNG_L"></span>)
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000680.t002/originalimage">
                    TIFF
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000680.t002/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pone.0000680.t002.TIF"></span>)
                </a></li></ul></div><p><strong>Table 2.  <span>Parameters of the model's fit to each observer's reading rates.</span></strong></p><span>doi:10.1371/journal.pone.0000680.t002</span></div><div class="figure" id="pone-0000680-t003"><div class="img"><a name="pone-0000680-t003" title="Click for larger image " href="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000680.t003&amp;representation=PNG_M" data-doi="info:doi/10.1371/journal.pone.0000680" data-uri="info:doi/10.1371/journal.pone.0000680.t003"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000680.t003&amp;representation=PNG_I" alt="thumbnail" class="thumbnail"></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000680.t003/powerpoint">
                    PPT
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000680.t003/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000680.t003/largerimage">
                    PNG
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000680.t003/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pone.0000680.t003.PNG_L"></span>)
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000680.t003/originalimage">
                    TIFF
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000680.t003/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pone.0000680.t003.TIF"></span>)
                </a></li></ul></div><p><strong>Table 3.  <span>Three ways to measure reading rate.</span></strong></p><span>doi:10.1371/journal.pone.0000680.t003</span></div><a id="article1.body1.sec2.p2" name="article1.body1.sec2.p2"></a><p>For every reader tested, for both RSVP and page-reading, a simple additive model,<a name="" id=""></a><span class="equation"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000680.e007&amp;representation=PNG" class="inline-graphic"><span class="note">(1)</span></span><br>provides an excellent fit to the 8 reading rates, where <em>R</em> is reading rate (word/min), <em>L, W</em>, and <em>S</em> are the observer-dependent reading rate contributions of the three sources of information, each assumed to be zero when knocked out, and <em>ε</em> is the error in the fit. Across conditions and readers, the RMS error is a mere 22 word/min (out of a total rate of 396 word/min with no knock-outs). The additive model represents triple dissociation with a combination rule of summation <a href="#pone.0000680-Sternberg1">[37]</a>. Each knockout zeroes one component without affecting the other two. The excellent fit with large effects and negligible error proves the triple dissociation.</p>
<a id="article1.body1.sec2.p3" name="article1.body1.sec2.p3"></a><p>Confirming the psychologists and educators who emphasize phonics, mechanistic letter decoding, L, accounts for the lion's share (62%) of the adult reading rate. This is recognition by parts. Holistic word recognition, W, accounts for only a small fraction (16%) of reading rate. (This is consistent with Smith's report of 21% reduction in reading speed when case is alternated in this way <a href="#pone.0000680-Smith3">[38]</a>.) The contextual sentence process, S, accounts for 22% of reading rate, on average, but is variable across readers (mean±SD = 87±30 word/min), which may reflect individual differences in print exposure (see ref. 19).</p>
<a id="article1.body1.sec2.p4" name="article1.body1.sec2.p4"></a><p>A 3-way analysis of variance of each observer's reading rates for the eight conditions (no repeated measure) shows that the main effect of L is significant (p&lt;0.05) for nearly all the observers (10 of 11), and that the main effects of S and W are significant for nearly half of the observers (5 and 4 of 11). Interactions reached significance in only three cases (L*S for 2 of the 11 observers; L*W for 1 observer). Doing one 3-way analysis of variance of all the data, treating the 11 observers' results as repeated measures, finds highly significant (p&lt;0.001) main effects of L, W, and S, a small interaction of L and S (p&lt;0.01), and no other 2- or 3-way interaction (p&gt;0.5). The main effects account for most of the variance: L 75%, S 7.1%, W 3.3%. The L*S interaction accounts for only 1% of the variance. L and S are slightly super-additive, a synergy. The benefit from L and S together is slightly more than the sum of the benefits from just L or S alone. However, this effect is very small–too weak to have any practical consequence–and is no longer statistically significant when we apply Bonferroni correction for 7 hypotheses. Thus, the ANOVA endorses the additive account.</p>
<a id="article1.body1.sec2.p5" name="article1.body1.sec2.p5"></a><p>Understanding individual differences in reading rate would be invaluable. The breakdown in <a href="#pone-0000680-t002">Table 2</a> compares the contributions of each process across observers. There is surprisingly little difference in the contributions of each of the 3 processes across our group of 11 normal readers. However, note that observers JS and KT, our fastest readers, also have the highest percent contribution of the S (context) process. This supports the idea that the context process reflects differences in print exposure <a href="#pone.0000680-Stanovich1">[19]</a>. Even so, these readers are fast mostly because their L processes are fast.</p>
</div>

<div id="section3" class="section"><a id="s3" name="s3" toc="s3" title="Discussion"></a><h3>Discussion</h3><a id="article1.body1.sec3.p1" name="article1.body1.sec3.p1"></a><p>Our main result is the discovery of a triple dissociation among L, W, and S. A within-task triple dissociation with a composite measure is evidence that “the task is accomplished by a complex process that contains [three] functionally distinct and separately modifiable parts” <a href="#pone.0000680-Sternberg1">[37]</a>.</p>
<a id="article1.body1.sec3.p2" name="article1.body1.sec3.p2"></a><p>That letters, words, and sentences are all involved in reading is nothing new, but finding that their contributions to reading rate are additive is startling. Even so, our results are consistent with the Gough et al. <a href="#pone.0000680-Gough2">[22]</a> study that isolated the contributions of word ‘form’ and sentence ‘context’. They measured the proportion of words correctly named, isolating the contribution of form by measuring the effect of word duration and isolating the effect of context by measuring the effect of the number of preceding words in the sentence. They found that “the probability of word recognition given both form and context conforms very closely to the values one would obtain if the contributions of form and context were independent” <a href="#pone.0000680-Gough2">[22]</a>. Here, we separate ‘form’ into its two components, L and W, and show that the contributions of these processes to reading rate are independent of each other and of the contribution of the sentence context (S).</p>
<a id="article1.body1.sec3.p3" name="article1.body1.sec3.p3"></a><p>What do our results say about the mechanisms underlying reading? One might be tempted to think that the additive, independent contributions to reading rate mean that there are three completely autonomous reading processes. But that doesn't follow. It's not that simple. If the three processes were operating independently, most of the words produced by the two weaker processes (S and W) would be redundant with those produced by the stronger process (L). This would mean that the contributions of S and W would be greater when L is absent than when L is present, which is not what we find.</p>
<a id="article1.body1.sec3.p4" name="article1.body1.sec3.p4"></a><p>Additivity of rates implies exclusivity. The contribution of each process to reading rate is the same whether the other processes are working or not. Thus, the contributions are not redundant. The three processes are not working on the same words. This requires coordination among the processes. For the contributions of S and W to be equally valuable with and without L, L has to skip those words or devote only a small fraction of the time it normally would to those words before moving on to the next one.</p>
<a id="article1.body1.sec3.p5" name="article1.body1.sec3.p5"></a><p>Imagine that L, W, and S are three technicians in a computer store. As customers arrive, the technicians avoid handling the same ones. Instead, L is a generalist, who handles most of the customers, while S and W are specialists, who only handle certain kinds of computer problems. S's and W's total performance cannot match L's, because they only handle certain customers, but, for those customers, they are faster than L would be. When such a customer walks in, S or W immediately lets L know he will handle it, so L can take the next person.</p>
<a id="article1.body1.sec3.p6" name="article1.body1.sec3.p6"></a><p>The customers (words) that S and W can handle are infrequent. L could work on all words, but usually does not need to, because S and W handle some of them. To help, S or W must let L know immediately that he will get this word. In the case of S, supposing early notification is supported by MEG and ERP research showing that predictable words are processed much earlier than unpredictable ones <a href="#pone.0000680-Dikker1">[39]</a>, <a href="#pone.0000680-Lau1">[40]</a>. We suspect that W, too, warns L early. It takes time to assemble the parts of a complex word <a href="#pone.0000680-Zweig1">[41]</a>. From this, we might expect the one-step assembly (features to word) of the holistic (W) process to be faster than the two-step assembly (features to letters to word) of the by-parts (L) process.</p>
<a id="article1.body1.sec3.p7" name="article1.body1.sec3.p7"></a><p>Word shape has been a slippery concept <a href="#pone.0000680-Groff1">[42]</a>, <a href="#pone.0000680-Paap1">[43]</a>. Here, defining word shape by a practical test for holistic equivalence allowed us to isolate and measure the contribution of the process, W, that recognizes words as wholes.</p>
<a id="article1.body1.sec3.p8" name="article1.body1.sec3.p8"></a><p>Past studies have measured effects of substituting letters <a href="#pone.0000680-Underwood1">[28]</a>, <a href="#pone.0000680-Rayner2">[44]</a>, changing case <a href="#pone.0000680-Smith1">[26]</a>, <a href="#pone.0000680-Smith2">[30]</a>–<a href="#pone.0000680-McClelland1">[34]</a>, <a href="#pone.0000680-Smith3">[38]</a>, and shuffling words <a href="#pone.0000680-Chung1">[27]</a>, but, applied separately, these manipulations only assess how much each degradation of the text impairs reading. This says something about effectiveness, but nothing about the specificity of the knockout. Only by combining the manipulations could we discover the additivity. The triple dissociation proves that the text manipulations are selective. We hope the newly-revealed selectivity of these time-worn tools will prove useful in further explorations of reading.</p>
<a id="article1.body1.sec3.p9" name="article1.body1.sec3.p9"></a><p>Our findings challenge the most successful models of reading. Ever since the discovery that reading consists of a series of fixations rather than a continuous sweep of the eyes across the text <a href="#pone.0000680-Huey1">[45]</a>, investigators have looked to the eye movements for clues into how reading works. E-Z Reader <a href="#pone.0000680-Reichle1">[46]</a> and SWIFT <a href="#pone.0000680-Engbert1">[47]</a> model the eye movements of reading. These models include a word-recognition stage whose latency is affected by language properties such as word familiarity and word length, so they do make some use of context and word shape, but they cannot read at all when the letter information is knocked out. Similarly, Mr. Chips <a href="#pone.0000680-Legge1">[48]</a> and the Dual Route Cascaded model <a href="#pone.0000680-Coltheart2">[49]</a> simulate a wide variety of visual and language effects on reading rate and latency, but both models implement only the L process, plan to later add S, and omit W entirely. Our results cry out for implementation of W and S, which, together, account for 38% of reading rate. We suspect that it will be difficult for the models to achieve the additivity of rates that is so robust in the human data presented here.</p>
<a id="article1.body1.sec3.p10" name="article1.body1.sec3.p10"></a><p>Our approach treats reading as serial word recognition. It is surprising that such a simple model succeeds so well in describing how readers benefit from the three sources of information available in reading. Our results affirm the practical emphasis on L in schools, but challenge current computational models, revealing that W and S do contribute and make reading possible when L is knocked out. It has long been known that we recognize objects by parts, wholes, and context. The surprise is that each process contributes the same number of words per minute regardless of whether the others are operating. This is a triple dissociation among parts, wholes, and context.</p>
</div>

<div id="section4" class="section"><a id="s4" name="s4" toc="s4" title="Materials and Methods"></a><h3>Materials and Methods</h3>
<h4>Reading rate</h4>
<a id="article1.body1.sec4.sec1.p1" name="article1.body1.sec4.sec1.p1"></a><p>We measured reading rate in three ways. The results in <a href="#pone-0000680-g001">Figure 1</a> and <a href="#pone-0000680-t001">Tables 1</a>–<a href="#pone-0000680-t002">2</a> used the RSVP method described below. We also collected similar results on one observer (EK) reading printed pages, aloud and silently (<a href="#pone-0000680-t003">Table 3</a>). In the page-reading experiments, the observer was instructed to read each page as quickly as possible while still getting most of the words right. Aloud, she read 80%–100% of the words correctly in every condition. Accuracy could not be measured when she read silently. The LWS model fits well in every case, and the parameters' proportions were very similar for the three methods (<a href="#pone-0000680-t003">Table 3</a>).</p>
<a id="article1.body1.sec4.sec1.p2" name="article1.body1.sec4.sec1.p2"></a><p>Past studies have found that reading rate and comprehension when reading words presented serially (RSVP) are not very different from when reading static words on a page. Yu, Cheung, Legge, and Chung compare reading rate as a function of text size for text presented dynamically, one word at a time (RSVP), or statically, all together (static flashcard with four lines of text) <a href="#pone.0000680-Yu1">[50]</a>. RSVP reading is faster (1.4×) but the log reading rate curves are parallel (one is shifted upward), showing the same dependence on spacing. Masson assessed readers' comprehension of texts presented using RSVP versus statically <a href="#pone.0000680-Masson1">[51]</a>. He found that when 500 ms pauses were inserted between sentences, accuracy in answering specific questions about the text read was the same for 500 word/min RSVP as when the whole passage was displayed for the same total length of time [51, <a href="#pone-0000680-t002">Table 2</a>, page 270].</p>


<h4>Observers</h4>
<a id="article1.body1.sec4.sec2.p1" name="article1.body1.sec4.sec2.p1"></a><p>Eleven observers (ages 17–25) participated. All had normal or corrected-to-normal vision and were fluent in English. All observers gave informed consent and were paid for their participation. Observer KT is an author.</p>


<h4>Stimulus generation</h4>
<a id="article1.body1.sec4.sec3.p1" name="article1.body1.sec4.sec3.p1"></a><p>Our stimuli were generated using MATLAB with the Psychophysics Toolbox [52; 53; <a href="http://www.psychtoolbox.org">http://www.psychtoolbox.org</a>] and were displayed on a Philips FIMI GD402 very high brightness 21” grayscale monitor, sold in the USA by AFP Imaging as the “BrightView”, whose frame rate was 75 Hz. Words were presented as black text on a white background (156 cd/m<sup>2</sup>).</p>


<h4>Text</h4>
<a id="article1.body1.sec4.sec4.p1" name="article1.body1.sec4.sec4.p1"></a><p>The text came from the mystery novel <em>Loves Music, Loves to Dance</em> by Mary Higgins Clark <a href="#pone.0000680-Clark1">[35]</a>, a bestseller written for a broad, popular audience. It is an easy text, with a 7.5 Fog Index and a 5.5 Fleish-Kincaid Index. The text was not edited in any way before the application of the LWS manipulations. All proper nouns, capitalization (except in alternating-case trials), and punctuation were retained. The text was displayed in the Linotype font Helvetica Neue LT 85 Heavy, at an x-height of 0.39 deg and center-to-center letter spacing of 0.53 deg. This imposes uniform center-to-center spacing, overriding the font's ordinary spacing. No observer read the same passage twice.</p>


<h4>RSVP</h4>
<a id="article1.body1.sec4.sec5.p1" name="article1.body1.sec4.sec5.p1"></a><p>Using Rapid Serial Visual Presentation (RSVP), we presented each word, one after the other, in the same place on the screen <a href="#pone.0000680-Potter1">[36]</a>. The reported reading rate (<a href="#pone-0000680-t001">Table 1</a>) is the rate at which words were presented, six per trial, while the observer achieved an accuracy of 80% correct. On each trial, the observer read the six words aloud, taking as long as she liked. Legge notes that “for procedures in which maximum reading speed is computed from the display time of short texts, and oral reading speed is used only to check for accuracy, oral and silent reading speeds are approximately the same” <a href="#pone.0000680-Legge2">[54]</a>.</p>
<a id="article1.body1.sec4.sec5.p2" name="article1.body1.sec4.sec5.p2"></a><p>Our average reading rate of 396 word/min is faster than the typical reading rate of 250 word/min for adults reading a printed page. Our rates are faster partly because RSVP eliminates the need for eye movements <a href="#pone.0000680-Potter1">[36]</a> and partly because of a speed-accuracy trade-off. We use an adaptive procedure that iteratively adjusts the presentation rate to achieve a desired error rate. The error rate of normal silent reading is very low and hard to measure. We used the criterion of 80% correct, i.e. 20% mistaken, which allows faster reading.</p>
<a id="article1.body1.sec4.sec5.p3" name="article1.body1.sec4.sec5.p3"></a><p>The controlled error rate (achieved through the adaptive control of presentation rate) is an important feature of our experiment. We want to know how quickly observers can read when pressed. We are not interested in how their preferred rate might be affected by unfamiliar formatting of the text. Instead of leaving it to the observer's whim, the RSVP presentation controls the presentation rate, and the adaptive procedure (QUEST) finds the rate that yields 80% correct identification of the words <a href="#pone.0000680-Watson1">[55]</a>. The fixed error rate (80% correct) contributes to the specificity of our knockouts. The contribution of the sentence context (S) would be reduced by any condition that reduced the fraction of words identified. Our procedure adjusts presentation rate to maintain a fixed accuracy.</p>
<a id="article1.body1.sec4.sec5.p4" name="article1.body1.sec4.sec5.p4"></a><p>All our reported rates and model fits in <a href="#pone-0000680-g001">Figure 1</a> and <a href="#pone-0000680-t001">Tables 1</a>–<a href="#pone-0000680-t002">2</a> were collected with an 80% accuracy criterion. Pilot results using higher and lower criteria are consistently well-fit by the LWS model. The LWS rates decrease as the accuracy increases.</p>


<h4>Spatial and temporal flankers</h4>
<a id="article1.body1.sec4.sec6.p1" name="article1.body1.sec4.sec6.p1"></a><p>In order to simulate page reading, where each word is preceded and followed by another word, we added a random letter flanker one letter-space away from the beginning and end of each target word: “x word h”. Observers were asked to ignore the flankers.</p>
<a id="article1.body1.sec4.sec6.p2" name="article1.body1.sec4.sec6.p2"></a><p>In order to minimize end-effects in the 6-word sequence of a trial, we added a random letter string before the first word in a trial and another after the last word. These temporal flankers were displayed for the same length of time as the target words in that trial, and observers were asked to ignore them. Without temporal flankers, the first and last words in a trial showed a strong advantage over the middle four. With the temporal flankers, there is no longer any advantage for the last word in a trial. The primacy effect, enhanced performance on the first word, was reduced, but not eliminated, by the addition of temporal flankers.</p>


<h4>Fixation</h4>
<a id="article1.body1.sec4.sec7.p1" name="article1.body1.sec4.sec7.p1"></a><p>Two black squares (0.2 deg) were centered 0.9 deg above and below the center of the word. The observer, seated 200 cm from the screen, was instructed to fixate between the two squares and read the words aloud. Spatial and temporal flankers (as described above) were present on all trials.</p>


<h4>Trial, run, and threshold</h4>
<a id="article1.body1.sec4.sec8.p1" name="article1.body1.sec4.sec8.p1"></a><p>Each trial contained six words, presented one at a time at the same location (between the black squares). Each run consisted of 15 trials. Except for the scrambled condition, explained below, the text for each trial and run began at the point in the novel where the previous trial and run ended. Each observer completed approximately ten practice runs with plain text before data collection began. The 8 reading conditions of <a href="#pone-0000680-g001">Figure 1</a> were tested in random order. Before each run, the observer was told which condition she would be reading. The observer was given unlimited speaking time, and correctly read words were counted regardless of word order. Errors in word order were rare, occurring on less than 10% of the trials, with and without scrambled word order. After each trial, an answer screen showed the correct six words, and the experimenter recorded the number of words missed. Observers were encouraged to look at the correct words on the answer screen. The QUEST adaptive staircase increased or decreased the presentation rate of the words for the next trial, homing in on threshold rate for 80% accuracy <a href="#pone.0000680-Watson1">[55]</a>. Each reading rate recorded in <a href="#pone-0000680-t001">Table 1</a> is the average of two or three runs.</p>


<h4>Word shape and letter substitution</h4>
<a id="article1.body1.sec4.sec9.p1" name="article1.body1.sec4.sec9.p1"></a><p>The approach we are taking to shape is analogous to the scientific approach to color. When multiple mechanisms are potentially involved in a task, it is useful to design stimuli so that one of the mechanisms can't tell them apart. Then, any effects of exchanging the stimuli can be attributed to the other mechanisms, excluding the one that is blind to the difference. This technique is called “silent substitution” <a href="#pone.0000680-Estevez1">[56]</a>. It is common in color vision experiments to equate the luminance of two stimuli and thereby rule out any role of the achromatic channel in accounting for differences in response to these stimuli.</p>
<a id="article1.body1.sec4.sec9.p2" name="article1.body1.sec4.sec9.p2"></a><p>In the perceptual discrimination test used to generate <a href="#pone-0000680-g002">Figure 2</a>, the observer compared the peripherally-presented original letter and candidate substitute, each flanked on both sides by the same pair of flanking letters and judged whether they were discriminable or not. Letter size was 2 deg x-height, center-to-center spacing was 2.5 deg, and the triplets were presented at horizontal eccentricities of ±10 deg. Note that this paper is not about peripheral vision. Nor did we seek out the periphery as an example of bad vision. This paper is about reading. We use peripheral vision as part of an experimental technique that isolates a shape process (W) that is common to both central and peripheral vision. We are using the periphery in a particular way, to knock out the letter-based process, in order to isolate the word-shape channel. We empirically assembled a table of letter substitutions that are invisible to the observer's perception of word-shape when the letter-based identification mechanism is absent.</p>
<a id="article1.body1.sec4.sec9.p3" name="article1.body1.sec4.sec9.p3"></a><p>When Underwood and McConkie <a href="#pone.0000680-Beckmann1">[31]</a> used letter substitution to destroy word shape in their moving window paradigm, they defined “word shape” as the gross outline, and selected substitute letters from the original letter's category: having an ascender (e.g. bh), descender (e.g. pq), or neither (e.g. ac). However, though hallowed by tradition, there is no theoretical or empirical basis for that definition of word shape. We instead define word shape operationally (what can be distinguished when crowded) and choose the letter substitutes so as to be visually indistinguishable from the original when crowded. However, this is less different than it sounds, as our letter substitution table turns out to be similar to theirs.</p>
<a id="article1.body1.sec4.sec9.p4" name="article1.body1.sec4.sec9.p4"></a><p>In the L knockout condition, we explained to observers that letters would only be substituted by other letters that look like them (for example, letters with ascenders would never be substituted by letters without ascenders). Observers were told to say the original word they thought the target had been before substitution, not to try to pronounce nonsense words. Observers were also informed that not all letters have substitutes, and letters with substitutes were not always substituted (i.e., a letter with substitutes could be randomly substituted by itself). Because of these rules, some words in substitution trials were unmodified. Even so, the substitution manipulation was very effective. It slowed reading to a crawl.</p>
<a id="article1.body1.sec4.sec9.p5" name="article1.body1.sec4.sec9.p5"></a><p>The L knockout by substitution is quite effective, but not total. We attribute the residual reading rate (50 word/min) in the triple-knockout condition (see <a href="#pone-0000680-t001">Table 1</a>) to letter decoding, i.e. we think that <em>L</em> as reported here slightly underestimates the true value of the process. We tried making the knockout more severe (by not allowing a same-letter substitution when alternatives were available) but, as one would expect, this makes some conditions untestable because the observer cannot reach 80% correct at any presentation rate.</p>
<a id="article1.body1.sec4.sec9.p6" name="article1.body1.sec4.sec9.p6"></a><p>As you might guess from the demonstrations in the Introduction, most words are not easily identified by word shape. Random words cannot be read reliably based on word shape alone <a href="#pone.0000680-Martelli1">[8]</a>. The W process identifies only some of the words. These are often short, high-frequency words such as “and” (see demo in Introduction).</p>


<h4>Scrambling sentences</h4>
<a id="article1.body1.sec4.sec10.p1" name="article1.body1.sec4.sec10.p1"></a><p>To knock out the S process, word order was scrambled across the 90 words in a run.</p>


<h4>Preserving sentence context</h4>
<a id="article1.body1.sec4.sec11.p1" name="article1.body1.sec4.sec11.p1"></a><p>As mentioned above, each (ordered) trial began where the previous trial left off. This means that only some of the trials began at the beginning of a sentence. Many RSVP reading studies, some of which report extremely high reading rates, begin each trial at the beginning of a sentence <a href="#pone.0000680-Chung1">[e.g., 27]</a>. However, we do not think our method lessened the contribution of the S process in the ordered trials. First of all, sentence context is preserved by our use of an 80% criterion, which insures that readers recognize most of the words. Also, we always presented the correct six words on the response screen at the end of each trial. Therefore, at the beginning of any trial after the first, the observer had the context of the novel up to that point. Granted, we did randomize the order of scrambled and ordered runs. But still, within any given run with S intact, the observer is given much opportunity to make use of the storyline. This is, we think, more like the normal reading experience than using random sentences or artificially restricting our text to only include 6-word sentences.</p>


<h4>ANOVA</h4>
<a id="article1.body1.sec4.sec12.p1" name="article1.body1.sec4.sec12.p1"></a><p>Our analysis of variance (<a href="#pone-0000680-t004">Table 4</a>) used the ANOVAN command in MATLAB.</p>
<div class="figure" id="pone-0000680-t004"><div class="img"><a name="pone-0000680-t004" title="Click for larger image " href="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000680.t004&amp;representation=PNG_M" data-doi="info:doi/10.1371/journal.pone.0000680" data-uri="info:doi/10.1371/journal.pone.0000680.t004"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000680.t004&amp;representation=PNG_I" alt="thumbnail" class="thumbnail"></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000680.t004/powerpoint">
                    PPT
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000680.t004/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000680.t004/largerimage">
                    PNG
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000680.t004/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pone.0000680.t004.PNG_L"></span>)
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000680.t004/originalimage">
                    TIFF
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000680.t004/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pone.0000680.t004.TIF"></span>)
                </a></li></ul></div><p><strong>Table 4.  <span>Results of 3-way Repeated Measures ANOVA.</span></strong></p><span>doi:10.1371/journal.pone.0000680.t004</span></div>
</div>





<div><a id="ack" name="ack" toc="ack" title="Acknowledgments"></a><h3>Acknowledgments</h3>
<a id="article1.back1.ack1.p1" name="article1.back1.ack1.p1"></a><p>This is the third in a series of papers about crowding and its cure, isolating to recognize <a href="#pone.0000680-Martelli1">[8]</a>, <a href="#pone.0000680-Pelli3">[24]</a>, <a href="#pone.0000680-Pelli4">[25]</a>, <a href="#pone.0000680-Levi1">[57]</a>, <a href="#pone.0000680-Freeman1">[58]</a>. Shuang Guo collected pilot data as part of her NYU undergraduate honors thesis. Doris Aaronson, Beth Bauer, Bonnie Golding, Michael Landy, Najib Majaj, Liina Pylkkänen, Patrick Shrout, Barbara Tversky, Brian Wandell, and Beau Watson provided helpful comments. Some of these results were presented at the 2005 Vision Sciences Society conference in Sarasota, Florida and at the 2006 European Conference on Visual Perception in St. Petersburg, Russia.</p>
</div><div class="contributions"><a id="authcontrib" name="authcontrib" toc="authcontrib" title="Author Contributions"></a><h3>Author Contributions</h3><p>Conceived and designed the experiments: DP KT. Performed the experiments: KT. Analyzed the data: DP KT. Contributed reagents/materials/analysis tools: DP. Wrote the paper: DP KT.</p></div><div><a id="references" name="references" toc="references" title="References"></a><h3>References</h3><ol class="references"><li><span class="label">1.
              </span><a name="pone.0000680-Neisser1" id="pone.0000680-Neisser1"></a>Neisser U (1967) Cognitive psychology. New York: Appleton-Century-Croft.   <ul class="find-nolinks"></ul></li><li><span class="label">2.
              </span><a name="pone.0000680-Robson1" id="pone.0000680-Robson1"></a>Robson JG, Graham N (1981) Probability summation and regional variation in contrast sensitivity across the visual field. Vision Res  21: 409–418.  <ul class="find" data-citedArticleID="972118" data-doi="10.1016/0042-6989(81)90169-3"><li><a href="http://dx.doi.org/10.1016/0042-6989(81)90169-3" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Probability+summation+and+regional+variation+in+contrast+sensitivity+across+the+visual+field." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Probability+summation+and+regional+variation+in+contrast+sensitivity+across+the+visual+field.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">3.
              </span><a name="pone.0000680-Pelli1" id="pone.0000680-Pelli1"></a>Pelli DG, Burns CW, Farell B, Moore-Page DC (2006) Feature detection and letter identification. Vision Res  46: 4646–4674.  <ul class="find" data-citedArticleID="972098" data-doi="10.1016/j.visres.2006.04.023"><li><a href="http://dx.doi.org/10.1016/j.visres.2006.04.023" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Feature+detection+and+letter+identification." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Feature+detection+and+letter+identification.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">4.
              </span><a name="pone.0000680-Treisman1" id="pone.0000680-Treisman1"></a>Treisman AM, Gelade G (1980) A feature-integration theory of attention. Cognit Psychol,  12(1): 97–136.  <ul class="find" data-citedArticleID="972138" data-doi="10.1016/0010-0285(80)90005-5"><li><a href="http://dx.doi.org/10.1016/0010-0285(80)90005-5" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=A+feature-integration+theory+of+attention." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22A+feature-integration+theory+of+attention.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">5.
              </span><a name="pone.0000680-Rosch1" id="pone.0000680-Rosch1"></a>Rosch E, Mervis C, Gray W, Johnson D, Boyes-Braem P (1976) Basic objects in natural categories. Cognitive Psychology  8: 382–439.  <ul class="find" data-citedArticleID="972120" data-doi="10.1016/0010-0285(76)90013-x"><li><a href="http://dx.doi.org/10.1016/0010-0285(76)90013-x" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Basic+objects+in+natural+categories." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Basic+objects+in+natural+categories.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">6.
              </span><a name="pone.0000680-Prinzmetal1" id="pone.0000680-Prinzmetal1"></a>Prinzmetal W (1995) Visual feature integration in a world of objects. Current Directions in Psychological Science  4: 90–94.  <ul class="find" data-citedArticleID="972110" data-doi="10.1111/1467-8721.ep10772335"><li><a href="http://dx.doi.org/10.1111/1467-8721.ep10772335" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Visual+feature+integration+in+a+world+of+objects." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Visual+feature+integration+in+a+world+of+objects.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">7.
              </span><a name="pone.0000680-Pelli2" id="pone.0000680-Pelli2"></a>Pelli DG, Farell B, Moore DC (2003) The remarkable inefficiency of word recognition. Nature  423: 752–756.  <ul class="find" data-citedArticleID="972100" data-doi="10.1038/nature01516"><li><a href="http://dx.doi.org/10.1038/nature01516" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=The+remarkable+inefficiency+of+word+recognition." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22The+remarkable+inefficiency+of+word+recognition.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">8.
              </span><a name="pone.0000680-Martelli1" id="pone.0000680-Martelli1"></a>Martelli M, Majaj NJ, Pelli DG (2005) Are faces processed like words? A diagnostic test for recognition by parts. Journal of Vision  5(1): 6, 58–70.  <a href="http://journalofvision.org/5/1/6/">http://journalofvision.org/5/1/6/</a>, doi:10.1167/5.1.6.  <ul class="find" data-citedArticleID="972086" data-doi="10.1167/5.1.6"><li><a href="http://dx.doi.org/10.1167/5.1.6" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Are+faces+processed+like+words%3F+A+diagnostic+test+for+recognition+by+parts." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Are+faces+processed+like+words%3F+A+diagnostic+test+for+recognition+by+parts.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">9.
              </span><a name="pone.0000680-Biederman1" id="pone.0000680-Biederman1"></a>Biederman I (1987) Recognition-by-components: a theory of human image understanding. Psychol Rev  94: 115–147.  <ul class="find" data-citedArticleID="972042" data-doi="10.1037//0033-295x.94.2.115"><li><a href="http://dx.doi.org/10.1037//0033-295x.94.2.115" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Recognition-by-components%3A+a+theory+of+human+image+understanding." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Recognition-by-components%3A+a+theory+of+human+image+understanding.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">10.
              </span><a name="pone.0000680-Tjan1" id="pone.0000680-Tjan1"></a>Tjan BS, Legge GE (1998) The viewpoint complexity of an object-recognition task. Vision Res  38: 2335–2350.  <ul class="find" data-citedArticleID="972136" data-doi="10.1016/s0042-6989(97)00255-1"><li><a href="http://dx.doi.org/10.1016/s0042-6989(97)00255-1" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=The+viewpoint+complexity+of+an+object-recognition+task." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22The+viewpoint+complexity+of+an+object-recognition+task.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">11.
              </span><a name="pone.0000680-Tversky1" id="pone.0000680-Tversky1"></a>Tversky B, Hemenway K (1984) Objects, parts, and categories. J Exp Psychol Gen  113: 169–197.  <ul class="find" data-citedArticleID="972140"><li><a href="http://www.crossref.org/guestquery/?auth2=&amp;atitle2=Objects%2C+parts%2C+and+categories.&amp;auth=&amp;atitle=Objects%2C+parts%2C+and+categories." target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Objects%2C+parts%2C+and+categories." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Objects%2C+parts%2C+and+categories.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">12.
              </span><a name="pone.0000680-Cave1" id="pone.0000680-Cave1"></a>Cave CB, Kosslyn SM (1993) The role of parts and spatial relations in object identification. Perception  22: 229–248.  <ul class="find" data-citedArticleID="972050" data-doi="10.1068/p220229"><li><a href="http://dx.doi.org/10.1068/p220229" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=The+role+of+parts+and+spatial+relations+in+object+identification." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22The+role+of+parts+and+spatial+relations+in+object+identification.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">13.
              </span><a name="pone.0000680-Yin1" id="pone.0000680-Yin1"></a>Yin RK (1969) Looking at upside-down faces. Journal of Experimental Psychology  81(1): 141–145.  <ul class="find" data-citedArticleID="972146" data-doi="10.1037/h0027474"><li><a href="http://dx.doi.org/10.1037/h0027474" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Looking+at+upside-down+faces." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Looking+at+upside-down+faces.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">14.
              </span><a name="pone.0000680-Tanaka1" id="pone.0000680-Tanaka1"></a>Tanaka JW, Farah MJ (1993) Parts and wholes in face recognition. Q J Exp Psychol A  46: 225–245.  <ul class="find" data-citedArticleID="972132" data-doi="10.1080/14640749308401045"><li><a href="http://dx.doi.org/10.1080/14640749308401045" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Parts+and+wholes+in+face+recognition." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Parts+and+wholes+in+face+recognition.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">15.
              </span><a name="pone.0000680-Young1" id="pone.0000680-Young1"></a>Young AW, Hellawell D, Hay DC (1987) Configurational information in face perception. Perception  16: 747–759.  <ul class="find" data-citedArticleID="972148" data-doi="10.1068/p160747"><li><a href="http://dx.doi.org/10.1068/p160747" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Configurational+information+in+face+perception." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Configurational+information+in+face+perception.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">16.
              </span><a name="pone.0000680-Maurer1" id="pone.0000680-Maurer1"></a>Maurer D, Grand RL, Mondloch CJ (2002) The many faces of configural processing. Trends Cogn Sci  6: 255–260.  <ul class="find" data-citedArticleID="972090" data-doi="10.1016/s1364-6613(02)01903-4"><li><a href="http://dx.doi.org/10.1016/s1364-6613(02)01903-4" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=The+many+faces+of+configural+processing." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22The+many+faces+of+configural+processing.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">17.
              </span><a name="pone.0000680-Tanaka2" id="pone.0000680-Tanaka2"></a>Tanaka J, Gauthier I (1997)  Expertise in object and face recognition. In: Goldstone RL, editor. Perceptual learning: The psychology of learning and motivation, Vol 36. San Diego, CA: Academic Press. pp. 83–125.  <ul class="find-nolinks"></ul></li><li><span class="label">18.
              </span><a name="pone.0000680-Chall1" id="pone.0000680-Chall1"></a>Chall JS (1967) Learning to read: the great debate; an inquiry into the science, art, and ideology of old and new methods of teaching children to read, 1910-1965. New York: McGraw-Hill.   <ul class="find-nolinks"></ul></li><li><span class="label">19.
              </span><a name="pone.0000680-Stanovich1" id="pone.0000680-Stanovich1"></a>Stanovich KE, Stanovich PJ (1995) How research might inform the debate about early reading acquisition. Journal of Research in Reading  18: 87–105.  <ul class="find" data-citedArticleID="972128" data-doi="10.1111/j.1467-9817.1995.tb00075.x"><li><a href="http://dx.doi.org/10.1111/j.1467-9817.1995.tb00075.x" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=How+research+might+inform+the+debate+about+early+reading+acquisition." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22How+research+might+inform+the+debate+about+early+reading+acquisition.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">20.
              </span><a name="pone.0000680-Gough1" id="pone.0000680-Gough1"></a>Gough PB (1972)  One second of reading. In: Kavanagh JF, Mattingly IG, editors. Language by ear and by eye; the relationships between speech and reading. Cambridge, MA: MIT Press.   <ul class="find-nolinks"></ul></li><li><span class="label">21.
              </span><a name="pone.0000680-Rayner1" id="pone.0000680-Rayner1"></a>Rayner K, Foorman BR, Perfetti CA, Pesetsky D, Seidenberg MS (2002) How should reading be taught? Sci Am  286: 84–91.  <ul class="find" data-citedArticleID="972112" data-doi="10.1038/scientificamerican0302-84"><li><a href="http://dx.doi.org/10.1038/scientificamerican0302-84" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=How+should+reading+be+taught%3F" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22How+should+reading+be+taught%3F%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">22.
              </span><a name="pone.0000680-Gough2" id="pone.0000680-Gough2"></a>Gough PB, Afford JA, Wilcox PH (1981)  Words and context. In: Tzeng OJ, Singer H, editors. Perception of Print: Reading Research in Experimental Psychology. Hillsdale, NJ: Erlbaum.   <ul class="find-nolinks"></ul></li><li><span class="label">23.
              </span><a name="pone.0000680-Bouma1" id="pone.0000680-Bouma1"></a>Bouma H (1973) Visual interference in the parafoveal recognition of initial and final letters of words. Vision Res  13: 767–782.  <ul class="find" data-citedArticleID="972044" data-doi="10.1016/0042-6989(73)90041-2"><li><a href="http://dx.doi.org/10.1016/0042-6989(73)90041-2" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Visual+interference+in+the+parafoveal+recognition+of+initial+and+final+letters+of+words." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Visual+interference+in+the+parafoveal+recognition+of+initial+and+final+letters+of+words.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">24.
              </span><a name="pone.0000680-Pelli3" id="pone.0000680-Pelli3"></a>Pelli DG, Palomares M, Majaj NJ (2004) Crowding is unlike ordinary masking: Distinguishing feature integration from detection. Journal of Vision  4(12): 12, 1136–1169.  <a href="http://journalofvision.org/4/12/12/">http://journalofvision.org/4/12/12/</a>, doi:10.1167/4.12.12.  <ul class="find" data-citedArticleID="972102" data-doi="10.1167/4.12.12"><li><a href="http://dx.doi.org/10.1167/4.12.12" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Crowding+is+unlike+ordinary+masking%3A+Distinguishing+feature+integration+from+detection." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Crowding+is+unlike+ordinary+masking%3A+Distinguishing+feature+integration+from+detection.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">25.
              </span><a name="pone.0000680-Pelli4" id="pone.0000680-Pelli4"></a>Pelli DG, Tillman KA, Freeman J, Su M, Berger TD, Majaj NJ (2007) Crowding and eccentricity determine reading rate. Journal of Vision  7(2):   In press. <a href="http://journalofvision.org/7/2/">http://journalofvision.org/7/2/</a>.  <ul class="find" data-citedArticleID="972104" data-doi="10.1167/7.2.20"><li><a href="http://dx.doi.org/10.1167/7.2.20" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Crowding+and+eccentricity+determine+reading+rate." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Crowding+and+eccentricity+determine+reading+rate.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">26.
              </span><a name="pone.0000680-Smith1" id="pone.0000680-Smith1"></a>Smith F, Lott D, Cronnell B (1969) The effect of type size and case alternation on word identification. Am J Psychol  82: 248–253.  <ul class="find" data-citedArticleID="972122" data-doi="10.2307/1421250"><li><a href="http://dx.doi.org/10.2307/1421250" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=The+effect+of+type+size+and+case+alternation+on+word+identification." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22The+effect+of+type+size+and+case+alternation+on+word+identification.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">27.
              </span><a name="pone.0000680-Chung1" id="pone.0000680-Chung1"></a>Chung ST, Mansfield JS, Legge GE (1998) Psychophysics of reading. XVIII. The effect of print size on reading speed in normal peripheral vision. Vision Res  38: 2949–2962.  <ul class="find" data-citedArticleID="972054" data-doi="10.1016/s0042-6989(98)00072-8"><li><a href="http://dx.doi.org/10.1016/s0042-6989(98)00072-8" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Psychophysics+of+reading.+XVIII.+The+effect+of+print+size+on+reading+speed+in+normal+peripheral+vision." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Psychophysics+of+reading.+XVIII.+The+effect+of+print+size+on+reading+speed+in+normal+peripheral+vision.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">28.
              </span><a name="pone.0000680-Underwood1" id="pone.0000680-Underwood1"></a>Underwood NR, McConkie GW (1985) Perceptual span for letter distinctions during reading. Reading Research Quarterly  20: 153–162.  <ul class="find" data-citedArticleID="972142"><li><a href="http://www.crossref.org/guestquery/?auth2=&amp;atitle2=Perceptual+span+for+letter+distinctions+during+reading.&amp;auth=&amp;atitle=Perceptual+span+for+letter+distinctions+during+reading." target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Perceptual+span+for+letter+distinctions+during+reading." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Perceptual+span+for+letter+distinctions+during+reading.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">29.
              </span><a name="pone.0000680-Bouma2" id="pone.0000680-Bouma2"></a>Bouma H (1970) Interaction effects in parafoveal letter recognition. Nature  226: 177–178.  <ul class="find" data-citedArticleID="972046" data-doi="10.1038/226177a0"><li><a href="http://dx.doi.org/10.1038/226177a0" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Interaction+effects+in+parafoveal+letter+recognition." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Interaction+effects+in+parafoveal+letter+recognition.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">30.
              </span><a name="pone.0000680-Smith2" id="pone.0000680-Smith2"></a>Smith F (1969) Familiarity of configuration vs discriminability of features in the visual identification of words. Psychonomic Science,  14: 261–3.  <ul class="find" data-citedArticleID="972124" data-doi="10.3758/bf03329112"><li><a href="http://dx.doi.org/10.3758/bf03329112" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Familiarity+of+configuration+vs+discriminability+of+features+in+the+visual+identification+of+words." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Familiarity+of+configuration+vs+discriminability+of+features+in+the+visual+identification+of+words.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">31.
              </span><a name="pone.0000680-Beckmann1" id="pone.0000680-Beckmann1"></a>Beckmann PJ, Legge GE, Luebker A (1991) Reading: letters, words, and their spatial-frequency content. SID 91 Digest, 106–108.  <ul class="find" data-citedArticleID="972040"><li><a href="http://www.crossref.org/guestquery/?auth2=&amp;atitle2=Reading%3A+letters%2C+words%2C+and+their+spatial-frequency+content.&amp;auth=&amp;atitle=Reading%3A+letters%2C+words%2C+and+their+spatial-frequency+content." target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Reading%3A+letters%2C+words%2C+and+their+spatial-frequency+content." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Reading%3A+letters%2C+words%2C+and+their+spatial-frequency+content.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">32.
              </span><a name="pone.0000680-Allen1" id="pone.0000680-Allen1"></a>Allen PA, Wallace B, Sperry S, Vires-Collins H“Influence of case and block type on visual word recognition,” paper presented at the 35th Annual Meeting of the Psychonomics Society, St. Louis, MO, November 12, 1994.   <ul class="find-nolinks"></ul></li><li><span class="label">33.
              </span><a name="pone.0000680-Coltheart1" id="pone.0000680-Coltheart1"></a>Coltheart M, Freeman R (1974) Case alternation impairs word identification. Bull. Psychon. Soc.,  3: 102–104.  <ul class="find" data-citedArticleID="972058" data-doi="10.3758/bf03333407"><li><a href="http://dx.doi.org/10.3758/bf03333407" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Case+alternation+impairs+word+identification.+Bull.+Psychon." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Case+alternation+impairs+word+identification.+Bull.+Psychon.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">34.
              </span><a name="pone.0000680-McClelland1" id="pone.0000680-McClelland1"></a>McClelland JL (1976) Preliminary letter identification in the perception of words and nonwords. J Exp Psychol [Hum Percept],  2: 80–91.  <ul class="find" data-citedArticleID="972092" data-doi="10.1037/0096-1523.2.1.80"><li><a href="http://dx.doi.org/10.1037/0096-1523.2.1.80" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Preliminary+letter+identification+in+the+perception+of+words+and+nonwords." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Preliminary+letter+identification+in+the+perception+of+words+and+nonwords.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">35.
              </span><a name="pone.0000680-Clark1" id="pone.0000680-Clark1"></a>Clark MH (1991) Loves music, loves to dance. New York: Simon&amp;Schuster.   <ul class="find-nolinks"></ul></li><li><span class="label">36.
              </span><a name="pone.0000680-Potter1" id="pone.0000680-Potter1"></a>Potter MC (1984)  Rapid serial visual presentation (RSVP): A method for studying language processing. In: Kieras DE, Just MA, editors. New methods in reading comprehension research. Hillsdale, NJ: Erlbaum. pp. 91–118.  <ul class="find-nolinks"></ul></li><li><span class="label">37.
              </span><a name="pone.0000680-Sternberg1" id="pone.0000680-Sternberg1"></a>Sternberg S (2003) Process decomposition from double dissociation of subprocesses. Cortex  39: 180–182.  <ul class="find" data-citedArticleID="972130" data-doi="10.1016/s0010-9452(08)70097-2"><li><a href="http://dx.doi.org/10.1016/s0010-9452(08)70097-2" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Process+decomposition+from+double+dissociation+of+subprocesses." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Process+decomposition+from+double+dissociation+of+subprocesses.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">38.
              </span><a name="pone.0000680-Smith3" id="pone.0000680-Smith3"></a>Smith F (1969) Familiarity of configuration vs discriminability of features in the visual identification of words. Psychonomic Science,  14: 261–3.  <ul class="find" data-citedArticleID="972126" data-doi="10.3758/bf03329112"><li><a href="http://dx.doi.org/10.3758/bf03329112" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Familiarity+of+configuration+vs+discriminability+of+features+in+the+visual+identification+of+words." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Familiarity+of+configuration+vs+discriminability+of+features+in+the+visual+identification+of+words.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">39.
              </span><a name="pone.0000680-Dikker1" id="pone.0000680-Dikker1"></a>Dikker S, Rabagliati H, Pylkkänen L (submitted) Sensitivity to syntax in visual cortex.   <ul class="find-nolinks"></ul></li><li><span class="label">40.
              </span><a name="pone.0000680-Lau1" id="pone.0000680-Lau1"></a>Lau E, Stroud C, Plesch S, Phillips C (2006) The role of structural prediction in rapid syntactic analysis. Brain and Language  98: 74–88.  <ul class="find" data-citedArticleID="972078" data-doi="10.1016/j.bandl.2006.02.003"><li><a href="http://dx.doi.org/10.1016/j.bandl.2006.02.003" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=The+role+of+structural+prediction+in+rapid+syntactic+analysis." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22The+role+of+structural+prediction+in+rapid+syntactic+analysis.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">41.
              </span><a name="pone.0000680-Zweig1" id="pone.0000680-Zweig1"></a>Zweig E, Pylkkänen L (submitted) A visual M170 effect of morphological complexity. Language and Cognitive Processes.   <ul class="find-nolinks"></ul></li><li><span class="label">42.
              </span><a name="pone.0000680-Groff1" id="pone.0000680-Groff1"></a>Groff P (1975) Shapes as cues to word recognition. Visible Language  9: 67–71.  <ul class="find" data-citedArticleID="972074"><li><a href="http://www.crossref.org/guestquery/?auth2=&amp;atitle2=Shapes+as+cues+to+word+recognition.&amp;auth=&amp;atitle=Shapes+as+cues+to+word+recognition." target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Shapes+as+cues+to+word+recognition." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Shapes+as+cues+to+word+recognition.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">43.
              </span><a name="pone.0000680-Paap1" id="pone.0000680-Paap1"></a>Paap KR, Newsome SL, Noel RW (1984) Word shape's in poor shape for the race to the lexicon. J Exp Psychol Hum Percept Perform  10: 413–428.  <ul class="find" data-citedArticleID="972096" data-doi="10.1037/0096-1523.10.3.413"><li><a href="http://dx.doi.org/10.1037/0096-1523.10.3.413" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Word+shape%27s+in+poor+shape+for+the+race+to+the+lexicon." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Word+shape%27s+in+poor+shape+for+the+race+to+the+lexicon.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">44.
              </span><a name="pone.0000680-Rayner2" id="pone.0000680-Rayner2"></a>Rayner K, Kaiser JS (1975) Reading mutilated text. Journal of Educational Psychology  67: 301–306.  <ul class="find" data-citedArticleID="972114" data-doi="10.1037/h0077015"><li><a href="http://dx.doi.org/10.1037/h0077015" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Reading+mutilated+text." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Reading+mutilated+text.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">45.
              </span><a name="pone.0000680-Huey1" id="pone.0000680-Huey1"></a>Huey EB (1908) The psychology and pedagogy of reading. New York: Macmillan.   <ul class="find-nolinks"></ul></li><li><span class="label">46.
              </span><a name="pone.0000680-Reichle1" id="pone.0000680-Reichle1"></a>Reichle ED, Pollatsek A, Rayner K (2006) E-Z Reader: A cognitive-control, serial-attention model of eye-movement behavior during reading. Cognitive Systems Research,  7: 4–22.  <ul class="find" data-citedArticleID="972116" data-doi="10.1016/j.cogsys.2005.07.002"><li><a href="http://dx.doi.org/10.1016/j.cogsys.2005.07.002" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=E-Z+Reader%3A+A+cognitive-control%2C+serial-attention+model+of+eye-movement+behavior+during+reading." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22E-Z+Reader%3A+A+cognitive-control%2C+serial-attention+model+of+eye-movement+behavior+during+reading.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">47.
              </span><a name="pone.0000680-Engbert1" id="pone.0000680-Engbert1"></a>Engbert R, Nuthmann A, Richter EM, Kliegl R (2005) SWIFT: a dynamical model of saccade generation during reading. Psychol Rev  112: 777–813.  <ul class="find" data-citedArticleID="972064" data-doi="10.1037/0033-295x.112.4.777"><li><a href="http://dx.doi.org/10.1037/0033-295x.112.4.777" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=SWIFT%3A+a+dynamical+model+of+saccade+generation+during+reading." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22SWIFT%3A+a+dynamical+model+of+saccade+generation+during+reading.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">48.
              </span><a name="pone.0000680-Legge1" id="pone.0000680-Legge1"></a>Legge GE, Hooven TA, Klitz TS, Mansfield JS, Tjan BS (2002) Mr. Chips 2002: new insights from an ideal-observer model of reading. Vision Res  42: 2219–2234.  <ul class="find" data-citedArticleID="972080" data-doi="10.1016/s0042-6989(02)00131-1"><li><a href="http://dx.doi.org/10.1016/s0042-6989(02)00131-1" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Mr.+Chips+2002%3A+new+insights+from+an+ideal-observer+model+of+reading." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Mr.+Chips+2002%3A+new+insights+from+an+ideal-observer+model+of+reading.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">49.
              </span><a name="pone.0000680-Coltheart2" id="pone.0000680-Coltheart2"></a>Coltheart M, Rastle K, Perry C, Langdon R, Ziegler J (2001) DRC: a dual route cascaded model of visual word recognition and reading aloud. Psychol Rev  108: 204–256.  <ul class="find" data-citedArticleID="972060" data-doi="10.1037/0033-295x.108.1.204"><li><a href="http://dx.doi.org/10.1037/0033-295x.108.1.204" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=DRC%3A+a+dual+route+cascaded+model+of+visual+word+recognition+and+reading+aloud." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22DRC%3A+a+dual+route+cascaded+model+of+visual+word+recognition+and+reading+aloud.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">50.
              </span><a name="pone.0000680-Yu1" id="pone.0000680-Yu1"></a>Yu D, Cheung S-H, Legge GE, Chung STL (2007) Effect of letter spacing on visual span and reading speed. Journal of Vision  7(2): 2, 1–10.  <a href="http://journalofvision.org/7/2/2/">http://journalofvision.org/7/2/2/</a>, doi:10.1167/7.2.2.  <ul class="find" data-citedArticleID="972150" data-doi="10.1167/7.2.2"><li><a href="http://dx.doi.org/10.1167/7.2.2" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Effect+of+letter+spacing+on+visual+span+and+reading+speed." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Effect+of+letter+spacing+on+visual+span+and+reading+speed.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">51.
              </span><a name="pone.0000680-Masson1" id="pone.0000680-Masson1"></a>Masson ME (1983) Conceptual processing of text during skimming and rapid sequential reading. Memory&amp;Cognition  11: 262–274.  <ul class="find" data-citedArticleID="972088" data-doi="10.3758/bf03196973"><li><a href="http://dx.doi.org/10.3758/bf03196973" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Conceptual+processing+of+text+during+skimming+and+rapid+sequential+reading." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Conceptual+processing+of+text+during+skimming+and+rapid+sequential+reading.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">52.
              </span><a name="pone.0000680-Pelli5" id="pone.0000680-Pelli5"></a>Pelli DG (1997) The VideoToolbox software for visual psychophysics: transforming numbers into movies. Spat Vis  10: 437–442.  <ul class="find" data-citedArticleID="972106" data-doi="10.1163/156856897x00366"><li><a href="http://dx.doi.org/10.1163/156856897x00366" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=The+VideoToolbox+software+for+visual+psychophysics%3A+transforming+numbers+into+movies." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22The+VideoToolbox+software+for+visual+psychophysics%3A+transforming+numbers+into+movies.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">53.
              </span><a name="pone.0000680-Brainard1" id="pone.0000680-Brainard1"></a>Brainard DH (1997) The Psychophysics Toolbox. Spat Vis  10: 433–436.  <ul class="find" data-citedArticleID="972048" data-doi="10.1163/156856897x00357"><li><a href="http://dx.doi.org/10.1163/156856897x00357" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=The+Psychophysics+Toolbox." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22The+Psychophysics+Toolbox.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">54.
              </span><a name="pone.0000680-Legge2" id="pone.0000680-Legge2"></a>Legge GE (2007) Psychophysics of Reading in Normal and Low Vision. Mahwah, NJ: Lawrence Erlbaum Associates.   <ul class="find-nolinks"></ul></li><li><span class="label">55.
              </span><a name="pone.0000680-Watson1" id="pone.0000680-Watson1"></a>Watson AB, Pelli DG (1983) QUEST: a Bayesian adaptive psychometric method. Percept Psychophys  33: 113–120.  <ul class="find" data-citedArticleID="972144" data-doi="10.3758/bf03202828"><li><a href="http://dx.doi.org/10.3758/bf03202828" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=QUEST%3A+a+Bayesian+adaptive+psychometric+method." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22QUEST%3A+a+Bayesian+adaptive+psychometric+method.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">56.
              </span><a name="pone.0000680-Estevez1" id="pone.0000680-Estevez1"></a>Estevez O, Spekreijse H (1982) The “silent substitution” method in visual research. Vision Res  22: 681–691.  <ul class="find" data-citedArticleID="972066" data-doi="10.1016/0042-6989(82)90104-3"><li><a href="http://dx.doi.org/10.1016/0042-6989(82)90104-3" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=The+%26%23x201c%3Bsilent+substitution%26%23x201d%3B+method+in+visual+research." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22The+%26%23x201c%3Bsilent+substitution%26%23x201d%3B+method+in+visual+research.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">57.
              </span><a name="pone.0000680-Levi1" id="pone.0000680-Levi1"></a>Levi DM, Song S, Pelli DG (2007) Amblyopic reading is crowded. Journal of Vision  7(2):   In press. <a href="http://journalofvision.org/7/2/">http://journalofvision.org/7/2/</a>.  <ul class="find" data-citedArticleID="972084" data-doi="10.1167/7.2.21"><li><a href="http://dx.doi.org/10.1167/7.2.21" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Amblyopic+reading+is+crowded." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Amblyopic+reading+is+crowded.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">58.
              </span><a name="pone.0000680-Freeman1" id="pone.0000680-Freeman1"></a>Freeman J, Pelli DG (2007) An escape from crowding. Journal of Vision  7(2):   In press. <a href="http://journalofvision.org/7/2/">http://journalofvision.org/7/2/</a>.  <ul class="find" data-citedArticleID="972068" data-doi="10.1167/7.2.22"><li><a href="http://dx.doi.org/10.1167/7.2.22" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=An+escape+from+crowding." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22An+escape+from+crowding.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li></ol></div>

  </div>

      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000680.XML" value="112584"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000680.PDF" value="227471"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000680.g001.PNG_L" value="600307"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000680.g001.PNG_M" value="37104"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000680.g001.PNG_S" value="6845"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000680.g001.TIF" value="939346"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000680.g001.PNG_I" value="12465"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000680.e001.PNG" value="14502"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000680.e001.TIF" value="31572"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000680.e002.PNG" value="12857"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000680.e002.TIF" value="29680"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000680.e003.PNG" value="13614"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000680.e003.TIF" value="30772"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000680.e004.PNG" value="8821"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000680.e004.TIF" value="9132"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000680.e005.PNG" value="6472"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000680.e005.TIF" value="6886"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000680.g002.PNG_L" value="691701"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000680.g002.PNG_M" value="73890"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000680.g002.PNG_S" value="11807"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000680.g002.TIF" value="1262866"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000680.g002.PNG_I" value="57119"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000680.e006.PNG" value="9198"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000680.e006.TIF" value="25152"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000680.t001.PNG_L" value="133164"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000680.t001.PNG_M" value="81679"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000680.t001.PNG_S" value="11299"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000680.t001.TIF" value="566862"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000680.t001.PNG_I" value="25208"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000680.t002.PNG_L" value="108145"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000680.t002.PNG_M" value="66124"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000680.t002.PNG_S" value="10627"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000680.t002.TIF" value="498014"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000680.t002.PNG_I" value="20200"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000680.t003.PNG_L" value="85196"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000680.t003.PNG_M" value="116388"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000680.t003.PNG_S" value="16539"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000680.t003.TIF" value="380872"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000680.t003.PNG_I" value="38917"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000680.e007.PNG" value="7252"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000680.e007.TIF" value="23160"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000680.t004.PNG_L" value="98389"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000680.t004.PNG_M" value="132403"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000680.t004.PNG_S" value="17690"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000680.t004.TIF" value="432952"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000680.t004.PNG_I" value="44880"/>

</div>
<div class="sidebar">

  <div class="article-actions cf">
      <div class="download">
        <span class="btn"><a href="/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0000680&amp;representation=PDF" title="Download" target="_blank">Download PDF</a></span>
      </div>
      <div class="btn-reveal dropdown">
        <div class="dropdown-icon">
          <span class="btn">&nbsp;</span>
        </div>

        <div class="content">
          <ul class="bullet">
            <li><a href="/article/citationList.action?articleURI=info%3Adoi%2F10.1371%2Fjournal.pone.0000680" title="Download citations">Citation</a></li>
            <li><a href="/article/fetchObjectAttachment.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0000680&amp;representation=XML" title="Download article XML">XML</a></li>
          </ul>
        </div>
      </div> <!-- end btn-reveal dropdown-->


    <div class="btn-reveal flt-l">
        <span class="btn">Print</span>
        <div class="content">
            <ul class="bullet">
                <li id="print-article"><a href="#" onclick="if(typeof(_gaq) != 'undefined'){ _gaq.push(['_trackEvent','Article', 'Print', 'Click']); } window.print(); return false;" title="Print Article">Print article</a></li>
                <li>
                  <a href="https://www.odysseypress.com/onlinehost/reprint_order.php?type=A&page=0&journal=7&doi=10.1371/journal.pone.0000680&volume=&issue=&title=Parts, Wholes, and Context in Reading: A Triple Dissociation&author_name=Denis%20G.%20Pelli%2C%20Katharine%20A.%20Tillman&start_page=1&end_page=8" title="Odyssey Press">EzReprint</a>
                </li>
            </ul>
        </div>
    </div>

    <div class="btn-reveal flt-r">
        <span class="btn">Share</span>
        <div class="content">
            <ul class="social">
                <li><a href="http://www.reddit.com/submit?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0000680" target="_blank" title="Submit to Reddit"><img src="/images/icon.reddit.16.png" width="16" height="16" alt="Reddit">Reddit</a></li>

                <li><a href="https://plus.google.com/share?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0000680" target="_blank" title="Share on Google+"><img src="/images/icon.gplus.16.png" width="16" height="16" alt="Google+">Google+</a></li>

                <li><a href="http://www.stumbleupon.com/submit?url=http%3A%2F%2Fwww.plosone.org%2Farticle%2Finfo%253Adoi%252F10.1371%252Fjournal.pone.0000680" target="_blank" title="Add to StumbleUpon"><img src="/images/icon.stumble.16.png" width="16" height="16" alt="StumbleUpon">StumbleUpon</a></li>

                <li><a href="http://www.facebook.com/share.php?u=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0000680&amp;t=Parts%2C%20Wholes%2C%20and%20Context%20in%20Reading%3A%20A%20Triple%20Dissociation" target="_blank" title="Share on Facebook"><img src="/images/icon.fb.16.png" width="16" height="16" alt="Facebook">Facebook</a></li>

                <li><a href="http://www.linkedin.com/shareArticle?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0000680&title=Parts%2C%20Wholes%2C%20and%20Context%20in%20Reading%3A%20A%20Triple%20Dissociation&summary=Checkout%20this%20article%20I%20found%20at%20PLOS" target="_blank" title="Add to LinkedIn"><img src="/images/icon.linkedin.16.png" width="16" height="16" alt="Mendeley">LinkedIn</a></li>

                <li><a href="http://www.citeulike.org/posturl?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0000680&amp;title=Parts%2C%20Wholes%2C%20and%20Context%20in%20Reading%3A%20A%20Triple%20Dissociation" target="_blank" title="Add to CiteULike"><img src="/images/icon.cul.16.png" width="16" height="16" alt="CiteULike">CiteULike</a></li>

                <li><a href="http://www.mendeley.com/import/?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0000680" target="_blank" title="Add to Mendeley"><img src="/images/icon.mendeley.16.png" width="16" height="16" alt="Mendeley">Mendeley</a></li>

                <li><a href="https://www.pubchase.com/library?add_aid=10.1371%2Fjournal.pone.0000680&amp;source=plos" target="_blank" title="Add to PubChase"><img src="/images/icon.pc.16.png" width="16" height="16" alt="PubChase">PubChase</a></li>


                <script type="text/javascript">
                    // replace tweet with one that's pre-shortened to 140 chars
                    function truncateTweetText() {
                        var twtTitle = 'Parts, Wholes, and Context in Reading: A Triple Dissociation';
                        var twtUrl = 'http://dx.plos.org/10.1371/journal.pone.0000680';
                        // all URLs posted to twitter get auto-shortened to 20 chars.
                        var maxLength = 140 - (20 + 1);
                        // truncate the title to include space for twtTag and ellipsis (here, 10 = tag length + space + ellipsis)
                        if (twtTitle.length > maxLength) { twtTitle = twtTitle.substr(0, (maxLength - 10)) + '...'; }
                        // set the href to use the shortened tweet
                        $('#twitter-share-link').prop('href', 'http://twitter.com/intent/tweet?text=' + encodeURIComponent('#PLOSONE: ' + twtTitle + ' ' + twtUrl));
                    }
                </script>
                <li><a href="http://twitter.com/intent/tweet?text=#PLOSONE%3A%20Parts%2C%20Wholes%2C%20and%20Context%20in%20Reading%3A%20A%20Triple%20Dissociation http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0000680" onclick="truncateTweetText();" target="_blank" title="Share on Twitter" id="twitter-share-link"><img src="/images/icon.twtr.16.png" width="16" height="16" alt="Twitter">Twitter</a></li>

                <li><a href="/article/email/info%3Adoi%2F10.1371%2Fjournal.pone.0000680" title="Email this article"><img src="/images/icon.email.16.png" width="16" height="16" alt="Email">Email</a></li>
            </ul>
        </div>
    </div><!--end btn-reveal flt-r-->
</div><!-- end article-actions-->

<!-- begin Crossmark -->

<a id="open-crossmark" href="#" style="margin-top: -28px; display:block"><img style="border: 0; display: none;
 padding: 10px 0 18px 0;"  id="crossmark-icon" src="/images/logo-crossmark-bw.png" /></a>
<div id="crossmark-dialog" style="display: none;" title="">
    <!-- the external CrossMark data is loaded inside this iframe -->
    <iframe id="crossmark-dialog-frame" frameborder="0"></iframe>
</div>

<!-- end crossmark -->


<div class="block" id="subject-area-sidebar-block">
    <div class="header">
        <h3>Subject Areas</h3><div title="More information" id="subject-area-sidebar-block-help-icon"><img align="right"
                                                                                                           alt="info" src="/images/button_info.png"/><div id="subject-area-sidebar-block-help"><img align="right"
                                                                                                                                                                                                    src="/images/button_info.png"/><p>
        <b>We want your feedback.</b> Do these subject areas make sense for this article? If not, click the flag
        next to the incorrect subject area and we will review it. Thanks for your help!
    </p></div></div>
    </div>


    <ul id="subject-area-sidebar-list">


















          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Analysis+of+variance%22" title="Search for articles in the subject area:'Analysis of variance'"><div class="flagText">Analysis of variance</div></a>
              <div data-categoryid="42193" data-articleid="24198"
                   data-categoryname="Analysis of variance"
                   class="flagImage" title="Flag 'Analysis of variance' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Chemical+dissociation%22" title="Search for articles in the subject area:'Chemical dissociation'"><div class="flagText">Chemical dissociation</div></a>
              <div data-categoryid="19673" data-articleid="24198"
                   data-categoryname="Chemical dissociation"
                   class="flagImage" title="Flag 'Chemical dissociation' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Children%22" title="Search for articles in the subject area:'Children'"><div class="flagText">Children</div></a>
              <div data-categoryid="40327" data-articleid="24198"
                   data-categoryname="Children"
                   class="flagImage" title="Flag 'Children' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Dental+and+oral+procedures%22" title="Search for articles in the subject area:'Dental and oral procedures'"><div class="flagText">Dental and oral procedures</div></a>
              <div data-categoryid="25819" data-articleid="24198"
                   data-categoryname="Dental and oral procedures"
                   class="flagImage" title="Flag 'Dental and oral procedures' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Eye+movements%22" title="Search for articles in the subject area:'Eye movements'"><div class="flagText">Eye movements</div></a>
              <div data-categoryid="48081" data-articleid="24198"
                   data-categoryname="Eye movements"
                   class="flagImage" title="Flag 'Eye movements' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Object+recognition%22" title="Search for articles in the subject area:'Object recognition'"><div class="flagText">Object recognition</div></a>
              <div data-categoryid="20057" data-articleid="24198"
                   data-categoryname="Object recognition"
                   class="flagImage" title="Flag 'Object recognition' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Schools%22" title="Search for articles in the subject area:'Schools'"><div class="flagText">Schools</div></a>
              <div data-categoryid="18361" data-articleid="24198"
                   data-categoryname="Schools"
                   class="flagImage" title="Flag 'Schools' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Word+recognition%22" title="Search for articles in the subject area:'Word recognition'"><div class="flagText">Word recognition</div></a>
              <div data-categoryid="22899" data-articleid="24198"
                   data-categoryname="Word recognition"
                   class="flagImage" title="Flag 'Word recognition' as inappropriate"></div>
          </li>
    </ul>
</div>

<div class="ad">
    <div class="title">Advertisement</div>






  <iframe id='a0852f54' name='a0852f54'
    src='http://ads.plos.org/www/delivery/afr.php?zoneid=381&amp;cb=1262'
    frameborder='0' scrolling='no' width='160' height='600'>
    <a href='http://ads.plos.org/www/delivery/ck.php?n=a0852f54&amp;cb=35'
      target='_top'><img src='http://ads.plos.org/www/delivery/avw.php?zoneid=381&amp;cb=7903&amp;n=a0852f54'
      border='0' alt=''/>
    </a>
  </iframe>



</div>

<div id="twitter-alm-timeline" class="twitter-alm-timeline"></div>

<div class="block sidebar-comments">
    <div class="header">
        <h3>Comments</h3>
    </div>
      <p><a href="/annotation/listThread.action?root=1675">Sheet Music</a><br>Posted by BruceDeitrickPrice</p>
      <p><a href="/annotation/listThread.action?root=1">Asymptote per condition</a><br>Posted by Potsdam_EM_Group</p>
      <p><a href="/annotation/listThread.action?root=263">Mapping word identity and word shape on a single continuum</a><br>Posted by Potsdam_EM_Group</p>
</div>

</div><!-- sidebar -->
    </div>
  </div>
</div>
<script src="http://wl.figshare.com/static/p_widget.js" type="text/javascript"></script><div id="pageftr">
  <div class="ftr-cols cf">
    <div class="col col-1">
      <img src="/images/logo-plos-footer.png" alt="PLOS Logo" class="logo" />
      <p><a href="/static/releaseNotes">Ambra 2.9.16</a> Managed Colocation provided <br />by <a href="http://www.isc.org/">Internet Systems Consortium</a>.<p>
      <div class="nav nav-aux">
        <a href="/static/privacy">Privacy Policy</a> |
        <a href="/static/terms">Terms of Use</a> |
        <a href="http://www.plos.org/advertise/">Advertise</a> |
        <a href="http://www.plos.org/about/media-inquiries/">Media Inquiries</a>
      </div>
    </div>
    <div class="col col-2">
      <p><a href="http://www.plos.org/publications/journals/">Publications</a></p>
      <div class="nav">
        <ul>
          <li><a href="http://www.plosbiology.org">PLOS Biology</a></li>
          <li><a href="http://www.plosmedicine.org">PLOS Medicine</a></li>
          <li><a href="http://www.ploscompbiol.org">PLOS Computational Biology</a></li>
          <li><a href="http://currents.plos.org">PLOS Currents</a></li>
          <li><a href="http://www.plosgenetics.org">PLOS Genetics</a></li>
          <li><a href="http://www.plospathogens.org">PLOS Pathogens</a></li>
          <li><a href="http://www.plosone.org">PLOS ONE</a></li>
          <li><a href="http://www.plosntds.org">PLOS Neglected Tropical Diseases</a></li>
        </ul>
      </div>
    </div>
    <div class="col col-3">
      <div class="nav">
        <p><a href="http://www.plos.org">plos.org</a></p>
        <p><a href="http://blogs.plos.org">Blogs</a></p>
        <p><a href="http://www.ploscollections.org">Collections</a></p>
        <p><a href="/feedback/new">Send us feedback</a></p>

        <p>California (US) corporation #C2354500, based in San Francisco</p>
      </div>
    </div>
  </div>
</div><!-- pageftr -->

</div><!-- end page-wrap, this div is in header.ftl -->
<script type="text/javascript" src="/javascript/jquery-1.8.1-min.js?v=Tm7VCOzZz3lE03ghpkS6SWkHbyI"></script>
<script type="text/javascript" src="/javascript/ga-min.js?v=lNQ4gt8QcPDatjsdOFl_FGpPhLY"></script>
<script type="text/javascript" src="/javascript/jquery.hoverIntent-min.js?v=mRiGNYY9cIXxVb8u0K_MdW7hHnc"></script>
<script type="text/javascript" src="/javascript/jquery.placeholder-min.js?v=21Pn56Ur9h1N4K4VZDa0nqI3Pxo"></script>
<script type="text/javascript" src="/javascript/jquery.jsonp-2.4.0-min.js?v=lqTpzoHfSq3I5Ygo01qq5WankEo"></script>
<script type="text/javascript" src="/javascript/jquery-ui-1.9.2.custom-min.js?v=raSSlfNO0YsV5uUpAKmTB9n5VTc"></script>
<script type="text/javascript" src="/javascript/jquery.tooltip-min.js?v=cw+6Smh+mdryIA25xvqIvHMrnZM"></script>
<script type="text/javascript" src="/javascript/jquery.uniform-min.js?v=kYUAnX6W2W_2fK3RIuQ2m_YFG9U"></script>
<script type="text/javascript" src="/javascript/jquery.pjax-min.js?v=939kLBjL5_YKbx71T1RHjYaD4l8"></script>
<script type="text/javascript" src="/javascript/imagesloaded-min.js?v=XeuAp8Gc3mvQUo+wZCSF8ttPwvw"></script>
<script type="text/javascript" src="/javascript/figviewer-min.js?v=yPUa0sUQ_iHkI+IRv2i9bjyZJFo"></script>
<script type="text/javascript" src="/javascript/global-min.js?v=0Q3PwjeaWtXYDnqIsQvnL_ou0qs"></script>
<script type="text/javascript" src="/javascript/jquery.touchswipe-min.js?v=huaek_e6HqTduvCNAN91dJolTyw"></script>
<script type="text/javascript" src="/javascript/jquery.base64-min.js?v=VwV1zeVqKZj5FCAdlK0q5NRxbBg"></script>
<script type="text/javascript" src="/javascript/alm-min.js?v=Y5gm6B0b4Kx2YHNObNrgEeBgXlY"></script>
<script type="text/javascript" src="/javascript/taxonomy-browser-min.js?v=vBVMuDMYkGJCXIUxLe35GoyiJNw"></script>
<script type="text/javascript" src="/javascript/jquery.filterize-min.js?v=j0ZKVnHyk2nhFy8eIuNJkp7xaM0"></script>
<script type="text/javascript" src="/javascript/plosone-min.js?v=TK4H4arL_XBSwwJq+K1N3kqYfAI"></script>
<script type="text/javascript" src="/javascript/twitter-min.js?v=xKgcxLsQFXy+at1ao1NVke8nFlM"></script>
<script type="text/javascript" src="/javascript/crossmark.1.4-min.js?v=3FO4k0SjwTaGNnKGNSqthar1080"></script>
<script type="text/javascript">
  var _sf_async_config={uid:16579,domain:"plosone.org"};
  (function(){
    function loadChartbeat() {
      window._sf_endpt=(new Date()).getTime();
      var e = document.createElement('script');
      e.setAttribute('language', 'javascript');
      e.setAttribute('type', 'text/javascript');
      e.setAttribute('src',
          (("https:" == document.location.protocol) ? "https://a248.e.akamai.net/chartbeat.download.akamai.com/102508/" : "http://static.chartbeat.com/") +
              "js/chartbeat.js");
      document.body.appendChild(e);
    }
    var oldonload = window.onload;
    window.onload = (typeof window.onload != 'function') ?
        loadChartbeat : function() { oldonload(); loadChartbeat(); };
  })();
</script>
<!-- <script type="application/javascript" src="http://crossmark.crossref.org/javascripts/v1.3/crossmark.min.js"></script> -->

</body>
</html>
