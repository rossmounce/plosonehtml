

 



<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"
      xmlns:foaf="http://xmlns.com/foaf/0.1/"
      xmlns:dc="http://purl.org/dc/terms/"
      xmlns:doi="http://dx.doi.org/"
      xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
      xmlns:xsd="http://www.w3.org/2001/XMLSchema-datatypes#"
      lang="en" xml:lang="en"
      itemscope itemtype="http://schema.org/Article"
      class="no-js">
<head prefix="og: http://ogp.me/ns#">
  <title>PLOS ONE: Speech and Non-Speech Audio-Visual Illusions: A Developmental Study</title>


<link rel="stylesheet" type="text/css"  href="/css/global-min.css?v=izteQ6tu7kgsJZW_xmrYizvKiHM" />


    <!--[if lte IE 7]>
<link rel="stylesheet" type="text/css"  href="/css/lte_ie7-min.css?v=3bykQUyQmReeuobVyPozcJ9LxRc" />
    <![endif]-->


<link rel="stylesheet" type="text/css"  href="/css/jquery-ui-min.css?v=eXDHTEJM0lIAmDe5k0I0Ad4nxNo" />


<link rel="stylesheet" type="text/css"  href="/css/journal.css?v=T7ZVxJfgk9jNxLAJ2qHz1vZpgYU" />


<link rel="stylesheet" type="text/css" media="print" href="/css/print-min.css?v=T5lb0B3q6EXBsuDluc5V5w+AkRc" />


  <link rel="stylesheet" href="http://f.fontdeck.com/s/css/js/www.plosone.org/24557.css" type="text/css"/>

  <!--chartbeat -->
  <script type="text/javascript">var _sf_startpt = (new Date()).getTime()</script>
  <script>document.documentElement.className += ' js';</script>

  <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <meta http-equiv="X-UA-Compatible" content="IE=EmulateIE7; IE=EmulateIE9"/>
  <meta name="description" content="PLOS ONE: an inclusive, peer-reviewed, open-access resource from the PUBLIC LIBRARY OF SCIENCE. Reports of well-performed scientific studies from all disciplines freely available to the whole world."/>
  <meta name="keywords" content="PLOS, Public Library of Science, Open Access, Open-Access, Science, Medicine, Biology, Research, Peer-review, Inclusive, Interdisciplinary, Ante-disciplinary, Physics, Chemistry, Engineering"/>
  <meta name="almHost" content="http://alm.plos.org/api/v3/articles"/>
  <meta name="searchHost" content="http://api.plos.org/search" />
  <meta name="termsHost" content="http://api.plos.org/terms" />
  <meta name="solrApiKey" content="plos"/>
  <meta name="almAPIKey" content="3pezRBRXdyzYW6ztfwft" />
  <meta name="currentJournal" content="PLoSONE" />
  <meta name="almRequestBatchSize" content="" />

  <meta name="citation_publisher" content="Public Library of Science"/>
  <meta name="citation_doi" content="10.1371/journal.pone.0000742"/>
  <meta name="dc.identifier" content="10.1371/journal.pone.0000742" />

    <meta name="citation_title" content="Speech and Non-Speech Audio-Visual Illusions: A Developmental Study"/>
    <meta itemprop="name" content="Speech and Non-Speech Audio-Visual Illusions: A Developmental Study"/>

      <meta name="citation_author" content="Corinne Tremblay"/>
            <meta name="citation_author_institution" content="Department of Psychology, University of Montreal, Montreal, Canada"/>
            <meta name="citation_author_institution" content="Research Center, Sainte-Justine Hospital, Montreal, Canada"/>
      <meta name="citation_author" content="François Champoux"/>
            <meta name="citation_author_institution" content="Speech Language Pathology and Audiology, University of Montreal, Montreal, Canada"/>
      <meta name="citation_author" content="Patrice Voss"/>
            <meta name="citation_author_institution" content="Department of Psychology, University of Montreal, Montreal, Canada"/>
      <meta name="citation_author" content="Benoit A. Bacon"/>
            <meta name="citation_author_institution" content="Department of Psychology, Bishop's University, Sherbrooke, Quebec, Canada"/>
      <meta name="citation_author" content="Franco Lepore"/>
            <meta name="citation_author_institution" content="Department of Psychology, University of Montreal, Montreal, Canada"/>
            <meta name="citation_author_institution" content="Research Center, Sainte-Justine Hospital, Montreal, Canada"/>
      <meta name="citation_author" content="Hugo Théoret"/>
            <meta name="citation_author_institution" content="Department of Psychology, University of Montreal, Montreal, Canada"/>
            <meta name="citation_author_institution" content="Research Center, Sainte-Justine Hospital, Montreal, Canada"/>

    <meta name="citation_date" content="2007/8/15"/>

  <meta name="citation_pdf_url" content="http://dx.plos.org/10.1371/journal.pone.0000742.pdf" />

      <meta name="citation_journal_title" content="PLOS ONE" />
    <meta name="citation_firstpage" content="e742"/>
    <meta name="citation_issue" content="8"/>
    <meta name="citation_volume" content="2"/>
    <meta name="citation_issn" content="1932-6203"/>

    <meta name="citation_journal_abbrev" content="PLoS ONE" />

      <meta name="citation_reference" content="citation_title=Enhancement of visual perception by crossmodal visuo-auditory interaction.; citation_author=F Frassinetti; citation_author=N Bolognini; citation_author=E Ladavas; citation_journal_title=Exp Brain Res; citation_volume=147; citation_number=1; citation_pages=332-343; citation_date=2002; " />
      <meta name="citation_reference" content="citation_title=An analysis of audio-visual crossmodal integration by means of event-related potential (ERP) recordings.; citation_author=WA Teder-Sälejarvi; citation_author=JJ McDonald; citation_author=F Di Russo; citation_author=SA Hillyard; citation_journal_title=Brain Res Cogn Brain Res; citation_volume=14; citation_number=2; citation_pages=106-114; citation_date=2002; " />
      <meta name="citation_reference" content="citation_title=Heterogeneity and heterochrony in the development of intersensory perception.; citation_author=DJ Lewkowicz; citation_journal_title=Brain Res Cogn Brain Res; citation_volume=14; citation_number=3; citation_pages=41-63; citation_date=2002; " />
      <meta name="citation_reference" content="citation_title=Hearing lips and seeing voices.; citation_author=H McGurk; citation_author=J McDonald; citation_journal_title=Nature; citation_volume=264; citation_number=4; citation_pages=746-748; citation_date=1976; " />
      <meta name="citation_reference" content="citation_title=Perception of synthesized audible and visible speech.; citation_author=DW Massaro; citation_author=MM Cohen; citation_journal_title=Psychol Sci; citation_volume=1; citation_number=5; citation_pages=55-63; citation_date=1990; " />
      <meta name="citation_reference" content="citation_title=An audiovisual test of kinematic primitives for visual speech perception.; citation_author=LD Rosenblum; citation_author=HM Saldana; citation_journal_title=J Exp Psychol Hum Percept Perform; citation_volume=22; citation_number=6; citation_pages=318-331; citation_date=1996; " />
      <meta name="citation_reference" content="citation_title=How can we determine if the sense of presence affects task performance?; citation_author=RB Welch; citation_journal_title=Presence Teleoper Virtual Environ; citation_volume=8; citation_number=7; citation_pages=574-577; citation_date=1999; " />
      <meta name="citation_reference" content="citation_title=What you see is what you hear.; citation_author=L Shams; citation_author=Y Kamitani; citation_author=S Shimojo; citation_journal_title=Nature; citation_volume=408; citation_number=8; citation_pages=788; citation_date=2000; " />
      <meta name="citation_reference" content="citation_title=Factors influencing audio-visual fission and fusion illusions.; citation_author=TS Andersen; citation_author=K Tiippana; citation_author=M Sams; citation_journal_title=Brain Res Cogn Brain Res; citation_volume=2; citation_number=9; citation_pages=301-308; citation_date=2004; " />
      <meta name="citation_reference" content="citation_title=Sensory modalities are not separate modalities: plasticity and interactions.; citation_author=S Shimojo; citation_author=L Shams; citation_journal_title=Curr Opin Neurobiol; citation_volume=11; citation_number=10; citation_pages=505-509; citation_date=2001; " />
      <meta name="citation_reference" content="citation_title=Audio-visual speech perception is special.; citation_author=J Tuomainen; citation_author=TS Andersen; citation_author=K Tiippana; citation_author=M Sams; citation_journal_title=Cognition; citation_volume=96; citation_number=11; citation_pages=13-22; citation_date=2005; " />
      <meta name="citation_reference" content="citation_title=Hearing lips: gamma-band activity during audio-visual speech perception.; citation_author=J Kaiser; citation_author=I Hertrich; citation_author=H Ackermann; citation_author=K Mathiak; citation_author=W Lutzenberger; citation_journal_title=Cereb Cortex; citation_volume=15; citation_number=12; citation_pages=646-653; citation_date=2005; " />
      <meta name="citation_reference" content="citation_title=Sound alters visual evoked potentials in humans.; citation_author=L Shams; citation_author=Y Kamitani; citation_author=S Thompson; citation_author=S Shimojo; citation_journal_title=Neuroreport; citation_volume=12; citation_number=13; citation_pages=3849-3852; citation_date=2001; " />
      <meta name="citation_reference" content="citation_title=Crossmodal processing in the human brain: insights from functional neuroimaging studies.; citation_author=GA Calvert; citation_journal_title=Cereb Cortex; citation_volume=11; citation_number=14; citation_pages=1110-1123; citation_date=2001; " />
      <meta name="citation_reference" content="citation_title=Evidence from functional magnetic resonance imaging of crossmodal binding in the human heteromodal cortex.; citation_author=GA Calvert; citation_author=R Campbell; citation_author=MJ Brammer; citation_journal_title=Curr Biol; citation_volume=10; citation_number=15; citation_pages=649-657; citation_date=2000; " />
      <meta name="citation_reference" content="citation_title=Spatial and temporal factors during processing of audiovisual speech: a PET study.; citation_author=E Macaluso; citation_author=N George; citation_author=R Dolan; citation_author=C Spence; citation_author=J Driver; citation_journal_title=Neuroimage; citation_volume=21; citation_number=16; citation_pages=725-732; citation_date=2004; " />
      <meta name="citation_reference" content="citation_title=An fMRI study of the binding of audio-visual information: the dissociation between object and space processing.; citation_author=C Sestieri; citation_author=R Di Matteo; citation_author=A Ferretti; citation_author=C Del Gratta; citation_author=M Caulo; citation_journal_title=Cogn Process; citation_volume=7; citation_number=17; citation_pages=138-139; citation_date=2006; " />
      <meta name="citation_reference" content="citation_title=The McGurk effect in infants.; citation_author=LD Rosenblum; citation_author=MA Schmuckler; citation_author=JA Johnson; citation_journal_title=Percept Psychophys; citation_volume=59; citation_number=18; citation_pages=347-357; citation_date=1997; " />
      <meta name="citation_reference" content="citation_title=Auditory-visual speech integration by prelinguistic infants: perception of an emergent consonant in the McGurk effect.; citation_author=D Burnham; citation_author=B Dodd; citation_journal_title=Dev Psychobiol; citation_volume=45; citation_number=19; citation_pages=204-220; citation_date=2004; " />
      <meta name="citation_reference" content="citation_title=Is the integration of heard and seen speech mandatory for infants?; citation_author=RN Desjardins; citation_author=JF Werker; citation_journal_title=Dev Psychobiol; citation_volume=45; citation_number=20; citation_pages=187-203; citation_date=2004; " />
      <meta name="citation_reference" content="citation_title=Children's perception of visual and auditory speech.; citation_author=DW Massaro; citation_journal_title=Child Dev; citation_volume=55; citation_number=21; citation_pages=1777-1788; citation_date=1984; " />
      <meta name="citation_reference" content="citation_title=Developmental changes in visual and auditory contributions to speech perception.; citation_author=DW Massaro; citation_author=LA Thompson; citation_author=B Barron; citation_author=E Laren; citation_journal_title=J Exp Child Psychol; citation_volume=1; citation_number=22; citation_pages=93-113; citation_date=1986; " />
      <meta name="citation_reference" content="citation_title=Visual illusion induced by sound.; citation_author=L Shams; citation_author=Y Kamitani; citation_author=S Shimojo; citation_journal_title=Cogn Brain Res; citation_volume=14; citation_number=23; citation_pages=147-152; citation_date=2002; " />
      <meta name="citation_reference" content="citation_title=Age-related changes on a children's test of sensory-level speech perception capacity.; citation_author=TE Hnath-Chisolm; citation_author=E Laipply; citation_author=A Boothroyd; citation_journal_title=J Speech Lang Hear Res; citation_volume=41; citation_number=24; citation_pages=94-106; " />
      <meta name="citation_reference" content="citation_title=Sound alters visual motion perception.; citation_author=R Sekuler; citation_author=AB Sekuler; citation_author=R Lau; citation_journal_title=Nature; citation_volume=385; citation_number=25; citation_pages=308; citation_date=1997; " />
      <meta name="citation_reference" content="citation_title=Sound induces perceptual reorganization of an ambiguous motion display in human infants.; citation_author=C Scheier; citation_author=DJ Lewkowicz; citation_author=S Shimojo; citation_journal_title=Dev Science; citation_volume=6; citation_number=26; citation_pages=233-244; citation_date=2003; " />
      <meta name="citation_reference" content="citation_title=Immediate perceptual response to intersensory discrepancy.; citation_author=RB Welch; citation_author=DH Warren; citation_journal_title=Psychol Bull; citation_volume=88; citation_number=27; citation_pages=638-667; citation_date=1980; " />
      <meta name="citation_reference" content="citation_title=Mismatch negativity evoked by the McGurk-MacDonald effect: a phonetic representation within short-term memory.; citation_author=C Colin; citation_author=M Radeau; citation_author=A Soquet; citation_author=D Demolin; citation_author=F Colin; citation_author=P Deltenre; citation_journal_title=Clin Neurophysiol; citation_volume=113; citation_number=28; citation_pages=495-506; citation_date=2002; " />
      <meta name="citation_reference" content="citation_title=Visual attention modulates audiovisual speech perception.; citation_author=K Tiippana; citation_author=TS Andersen; citation_author=M Sams; citation_journal_title=Eur J Cogn Psychol; citation_volume=16; citation_number=29; citation_pages=457-472; citation_date=2004; " />
      <meta name="citation_reference" content="citation_title=Audiovisual integration in perception of real words.; citation_author=DJ Dekle; citation_author=CA Fowler; citation_author=MG Funnell; citation_journal_title=Percept Psychophys; citation_volume=51; citation_number=30; citation_pages=355-362; citation_date=1992; " />
      <meta name="citation_reference" content="citation_title=Perceiving talking faces.; citation_author=DW Massaro; citation_number=31; citation_date=1998; citation_publisher=MIT Press; " />
      <meta name="citation_reference" content="citation_title=Seeing speech affects acoustic information processing in the human brainstem.; citation_author=G Musacchia; citation_author=M Sams; citation_author=T Nicol; citation_author=N Kraus; citation_journal_title=Exp Brain Res; citation_volume=168; citation_number=32; citation_pages=1-10; citation_date=2006; " />
      <meta name="citation_reference" content="citation_title=A role for the inferior colliculus in multisensory integration.; citation_author=F Champoux; citation_author=C Tremblay; citation_author=C Mercier; citation_author=M Lassonde; citation_author=F Lepore; citation_journal_title=Neuroreport; citation_volume=17; citation_number=33; citation_pages=1607-1610; citation_date=2006; " />
      <meta name="citation_reference" content="citation_title=Multisensory integration: space, time and superadditivity.; citation_author=NP Holmes; citation_author=C Spence; citation_journal_title=Curr Biol; citation_volume=15; citation_number=34; citation_pages=762-764; citation_date=2005; " />
      <meta name="citation_reference" content="citation_title=The development of a dialogue between cortex and midbrain to integrate multisensory information.; citation_author=BE Stein; citation_journal_title=Exp Brain Res; citation_volume=166; citation_number=35; citation_pages=305-315; citation_date=2005; " />

  <link rel="canonical" href="http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0000742" />

    <meta name="twitter:card" content="summary"/>
    <meta name="twitter:site" content="@plosone"/>
    <meta name="twitter:title" content="Speech and Non-Speech Audio-Visual Illusions: A Developmental Study"/>
    <meta name="twitter:description" content="It is well known that simultaneous presentation of incongruent audio and visual stimuli can lead to illusory percepts. Recent data suggest that distinct processes underlie non-specific intersensory speech as opposed to non-speech perception. However, the development of both speech and non-speech intersensory perception across childhood and adolescence remains poorly defined. Thirty-eight observers aged 5 to 19 were tested on the McGurk effect (an audio-visual illusion involving speech), the Illusory Flash effect and the Fusion effect (two audio-visual illusions not involving speech) to investigate the development of audio-visual interactions and contrast speech vs. non-speech developmental patterns. Whereas the strength of audio-visual speech illusions varied as a direct function of maturational level, performance on non-speech illusory tasks appeared to be homogeneous across all ages. These data support the existence of independent maturational processes underlying speech and non-speech audio-visual illusory effects."/>
      <meta name="twitter:image" content="http://dx.plos.org/10.1371/journal.pone.0000742.g004"/>

  <meta property="og:title" content="Speech and Non-Speech Audio-Visual Illusions: A Developmental Study" />
  <meta property="og:type" content="article" />
  <meta property="og:url" content="http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0000742" />

 <!--end articleInfoX-->

  <link rel="pingback" href="http://www.plosone.org/pingback" />


  <link rel="shortcut icon" href="/images/favicon.ico" type="image/x-icon"/>
  <link rel="home" title="home" href="/"/>
  <link rel="alternate" type="application/rss+xml"
        title="PLOS ONE: New Articles"
        href="http://www.plosone.org/article/feed"/>
</head>
<body>

  <div id="page-wrap">
    <div id="topbanner" class="cf">

<!-- Div for the ad at the top of journal home page-->
<div class="center">
  <div class="title">Advertisement</div>
  <iframe id='a3ac9da4' name='a3ac9da4'
    src='http://ads.plos.org/www/delivery/afr.php?zoneid=345&amp;cb=6839'
    frameborder='0' scrolling='no' width='730' height='90'>
    <a href='http://ads.plos.org/www/delivery/ck.php?n=a3ac9da4&amp;cb=4277'
      target='_top'><img src='http://ads.plos.org/www/delivery/avw.php?zoneid=345&amp;cb=3585&amp;n=a3ac9da4'
      border='0' alt=''/>
    </a>
  </iframe>
</div>    </div>

    <div id="pagehdr-wrap">
      <div id="pagehdr">
        <div id="user" class="nav">
          <ul>
            <li><a href="http://www.plos.org">plos.org</a></li>
            <li><a href="https://register.plos.org/ambra-registration/register.action">create account</a></li>
            <li class="btn-style"><a
              href="/user/secure/secureRedirect.action?goTo=%2Farticle%2FfetchArticle.action%3FarticleURI%3Dinfo%253Adoi%252F10.1371%252Fjournal.pone.0000742">sign in</a>
            </li>
          </ul>
        </div>
        <div class="logo">
          <a href="/"><img src="/images/logo.png" alt="PLOS ONE"></a>
        </div>

<div id="nav-main" class="nav">
  <ul>
        <li id="mn-01"><a href="/taxonomy" class="areas-link">Subject Areas</a></li>
    <li id="mn-02"><a href="javascript:void(0);">For Authors</a>
      <div class="submenu" style="width: 540px; margin-left: -300px;">
        <div class="block">
          <div class="submit-script">
            <h3>Submit your Manuscript</h3>
            <ul>
              <li>Fair, rigorous peer review</li>
              <li>Broad scope and wide reach</li>
            </ul>
            <a href="/static/submissionInstructions" class="btn">get started</a>
          </div>
        </div>
        <div class="menu">
          <ul>
            <li><a href="/static/publish">Why Publish with PLOS ONE</a></li>
            <li><a href="/static/publication">Publication Criteria</a></li>
            <li><a href="/static/editorial">Editorial Policies</a></li>
            <li><a href="/static/guidelines">Preparing A Manuscript</a></li>
            <li><a href="/static/figureGuidelines">Figure and Table Guidelines</a></li>
          <li><a href="/static/supportingInformation">Supporting Information Guidelines</a></li>
            <li><a href="/static/submissionInstructions">Submitting a Manuscript</a></li>
          </ul>
        </div>
      </div>
    </li>

    <li id="mn-03"><a href="javascript:void(0);">About Us</a>
      <div class="submenu" style="width:248px; margin-left:-30px;">
        <div class="menu">
          <ul>
            <li><a href="/static/information">Journal Information</a></li>
            <li><a href="/static/edboard">Editorial Board</a></li>
            <li><a href="/static/reviewerGuidelines">Reviewer Guidelines</a></li>
            <li><a href="/static/almInfo">Article-Level Metrics</a></li>
            <li><a href="/static/license">Open-Access License</a></li>
            <li><a href="/static/downloads">Media Downloads</a></li>
            <li><a href="/static/commentGuidelines">Guidelines for Comments</a></li>
            <li><a href="/static/corrections">Corrections</a></li>
            <li><a href="/static/help">Help Using this Site</a></li>
            <li><a href="/static/contact">Contact Us</a></li>
          </ul>
        </div>
      </div>
    </li>
  </ul>
<div id="db">
  <form name="searchForm" action="/search/simple?noSearchFlag=true&amp;query=&amp;articleURI=info%3Adoi%2F10.1371%2Fjournal.pone.0000742" method="get" >
<input type="hidden" name="from" value="globalSimpleSearch" id="from"/><input type="hidden" name="filterJournals" value="PLoSONE" id="filterJournals"/>    <fieldset>
      <legend>Search</legend>
      <label for="search">Search</label>
      <div class="wrap">
        <input id="search" type="text" name="query" placeholder="Search">
        <input type="image" alt="SEARCH" src="/images/icon.search.gif">
      </div>
    </fieldset>
  </form>
    <a id="advSearch" href="/search/advanced?noSearchFlag=true&amp;query=&amp;articleURI=info%3Adoi%2F10.1371%2Fjournal.pone.0000742&filterJournals=PLoSONE">advanced search</a>
</div></div>

      </div>
      <!-- pagehdr-->
    </div>
    <!-- pagehdr-wrap -->

  <!--body and html tags gets closed in global_footer.ftl-->
<script type="text/javascript" src="/javascript/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<div id="pagebdy-wrap">
  <div id="pagebdy">

    <div id="article-block" class="cf">

<div class="article-meta cf">
  <ul id="almSignPost" style="display: none;"></ul>
  <div class="article-type">
    <span class="type oa">Open Access</span>
      <span class="type pr">Peer-Reviewed</span>
  </div>
</div>

<div class="header" id="hdr-article">

<div class="article-kicker">
      <span id="article-type-heading">
        Research Article
      </span>
</div>  <h1 property="dc:title" datatype="" rel="dc:type" href="http://purl.org/dc/dcmitype/Text">
    Speech and Non-Speech Audio-Visual Illusions: A Developmental Study
  </h1>

  <ul class="authors">
      <li>


        <span rel="dc:creator" class="author">
          <span class="person" property="foaf:name" typeof="foaf:Person">
            Corinne Tremblay, 
          </span>
        </span>

          <div class="author_meta">
            <div class="author_inner">


              
              

                <p>Affiliations:
                  Department of Psychology, University of Montreal, Montreal, Canada, 
                  Research Center, Sainte-Justine Hospital, Montreal, Canada
                </p>


              <span class="close">X</span>

            </div>
          </div>
      </li>
      <li>


        <span rel="dc:creator" class="author">
          <span class="person" property="foaf:name" typeof="foaf:Person">
            François Champoux, 
          </span>
        </span>

          <div class="author_meta">
            <div class="author_inner">


              
              

                <p>Affiliation:
                  Speech Language Pathology and Audiology, University of Montreal, Montreal, Canada
                </p>


              <span class="close">X</span>

            </div>
          </div>
      </li>
      <li>


        <span rel="dc:creator" class="author">
          <span class="person" property="foaf:name" typeof="foaf:Person">
            Patrice Voss, 
          </span>
        </span>

          <div class="author_meta">
            <div class="author_inner">


              
              

                <p>Affiliation:
                  Department of Psychology, University of Montreal, Montreal, Canada
                </p>


              <span class="close">X</span>

            </div>
          </div>
      </li>
      <li>


        <span rel="dc:creator" class="author">
          <span class="person" property="foaf:name" typeof="foaf:Person">
            Benoit A. Bacon, 
          </span>
        </span>

          <div class="author_meta">
            <div class="author_inner">


              
              

                <p>Affiliation:
                  Department of Psychology, Bishop's University, Sherbrooke, Quebec, Canada
                </p>


              <span class="close">X</span>

            </div>
          </div>
      </li>
      <li>


        <span rel="dc:creator" class="author">
          <span class="person" property="foaf:name" typeof="foaf:Person">
            Franco Lepore, 
          </span>
        </span>

          <div class="author_meta">
            <div class="author_inner">


              
              

                <p>Affiliations:
                  Department of Psychology, University of Montreal, Montreal, Canada, 
                  Research Center, Sainte-Justine Hospital, Montreal, Canada
                </p>


              <span class="close">X</span>

            </div>
          </div>
      </li>
      <li>


        <span rel="dc:creator" class="author">
          <span class="person" property="foaf:name" typeof="foaf:Person">
            Hugo Théoret
              <span class="corresponding">mail</span>
          </span>
        </span>

          <div class="author_meta">
            <div class="author_inner">


              
              <p><span class="email">*</span>To whom correspondence should be addressed. E-mail: <a href="mailto:hugo.theoret@umontreal.ca">hugo.theoret@umontreal.ca</a></p>

                <p>Affiliations:
                  Department of Psychology, University of Montreal, Montreal, Canada, 
                  Research Center, Sainte-Justine Hospital, Montreal, Canada
                </p>


              <span class="close">X</span>

            </div>
          </div>
      </li>
  </ul>
  <ul class="date-doi-line">
    <li>Published: August 15, 2007</li>
    <li>DOI: 10.1371/journal.pone.0000742</li>
  </ul>


</div><!--end header-->
<div class="main cf" id="pjax-container">
  

<div class="nav items-5" id="nav-article">
  <ul>
  <li>
        <span class="active" name="article">Article</span>
  </li>
  <li>
      <a href="/article/authors/info%3Adoi%2F10.1371%2Fjournal.pone.0000742" name="authors">About the Authors</a>
  </li>
  <li>
      <a href="/article/metrics/info%3Adoi%2F10.1371%2Fjournal.pone.0000742" name="metrics">Metrics</a>
  </li>
  <li>
      <a href="/article/comments/info%3Adoi%2F10.1371%2Fjournal.pone.0000742" name="comments">Comments</a>
  </li>
  <li>
      <a href="/article/related/info%3Adoi%2F10.1371%2Fjournal.pone.0000742" name="related">Related Content</a>
  </li>
  </ul>
</div>

<script type="text/javascript">
  var selected_tab = "article";
</script>
  <div id="figure-thmbs" class="carousel cf">
    <div class="wrapper">
      <div class="slider">
              <div class="item">
                <a href="#pone-0000742-g001" data-doi="info:doi/10.1371/journal.pone.0000742" data-uri="info:doi/10.1371/journal.pone.0000742.g001" title="Figure 1">
                  <span class="thmb-wrap">
                    <img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000742.g001&representation=PNG_I" alt="">
                  </span>
                </a>
              </div>
              <div class="item">
                <a href="#pone-0000742-g002" data-doi="info:doi/10.1371/journal.pone.0000742" data-uri="info:doi/10.1371/journal.pone.0000742.g002" title="Figure 2">
                  <span class="thmb-wrap">
                    <img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000742.g002&representation=PNG_I" alt="">
                  </span>
                </a>
              </div>
              <div class="item">
                <a href="#pone-0000742-g003" data-doi="info:doi/10.1371/journal.pone.0000742" data-uri="info:doi/10.1371/journal.pone.0000742.g003" title="Figure 3">
                  <span class="thmb-wrap">
                    <img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000742.g003&representation=PNG_I" alt="">
                  </span>
                </a>
              </div>
              <div class="item">
                <a href="#pone-0000742-g004" data-doi="info:doi/10.1371/journal.pone.0000742" data-uri="info:doi/10.1371/journal.pone.0000742.g004" title="Figure 4">
                  <span class="thmb-wrap">
                    <img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000742.g004&representation=PNG_I" alt="">
                  </span>
                </a>
              </div>
      </div>
    </div>
  </div>

  <div class="nav-col">
    <div class="nav" id="nav-article-page">
      <ul>
        <li class="nav-col-comments"><a href="/article/comments/info%3Adoi%2F10.1371%2Fjournal.pone.0000742">Reader Comments (0)</a></li>
          <li id="nav-figures"><a data-doi="info:doi/10.1371/journal.pone.0000742" >Figures</a></li>
      </ul>
    </div>
  </div>

  <div class="article">







<div class="abstract"><a id="abstract0" name="abstract0" toc="abstract0" title="Abstract"></a><h2>Abstract</h2><a id="article1.front1.article-meta1.abstract1.p1" name="article1.front1.article-meta1.abstract1.p1"></a><p>It is well known that simultaneous presentation of incongruent audio and visual stimuli can lead to illusory percepts. Recent data suggest that distinct processes underlie non-specific intersensory speech as opposed to non-speech perception. However, the development of both speech and non-speech intersensory perception across childhood and adolescence remains poorly defined. Thirty-eight observers aged 5 to 19 were tested on the McGurk effect (an audio-visual illusion involving speech), the Illusory Flash effect and the Fusion effect (two audio-visual illusions not involving speech) to investigate the development of audio-visual interactions and contrast speech vs. non-speech developmental patterns. Whereas the strength of audio-visual speech illusions varied as a direct function of maturational level, performance on non-speech illusory tasks appeared to be homogeneous across all ages. These data support the existence of independent maturational processes underlying speech and non-speech audio-visual illusory effects.</p>
</div>


<div class="articleinfo"><p><strong>Citation: </strong>Tremblay C, Champoux F, Voss P, Bacon BA, Lepore F, et al.  (2007) Speech and Non-Speech Audio-Visual Illusions: A Developmental Study. PLoS ONE 2(8):
          e742.
            doi:10.1371/journal.pone.0000742</p><p><strong>Academic Editor: </strong>Justin Harris, University of Sydney, Australia</p><p><strong>Received:</strong> June 14, 2007; <strong>Accepted:</strong> July 16, 2007; <strong>Published:</strong> August 15, 2007</p><p>This is an open-access article distributed under the terms of the Creative Commons Public Domain declaration which stipulates that, once placed in the public domain, this work may be freely reproduced, distributed, transmitted, modified, built upon, or otherwise used by anyone for any lawful purpose.</p><p><strong>Funding: </strong>This study was funded by the Canadian Institutes of Health Research, Fonds de Recherche en Santé du Québec. Funders did not contribute to the study or to the mansucript.</p><p><strong>Competing interests:</strong> The authors have declared that no competing interests exist.</p></div>





<div id="section1" class="section"><a id="s1" name="s1" toc="s1" title="Introduction"></a><h3>Introduction</h3><a id="article1.body1.sec1.p1" name="article1.body1.sec1.p1"></a><p>It has repeatedly been shown that intersensory redundancy, the congruent bimodal presentation of stimuli over two sensory modalities, can enhance perception in both modalities (e.g. <a href="#pone.0000742-Frassinetti1">[1]</a>, <a href="#pone.0000742-TederSlejarvi1">[2]</a>). It is also well established that when two sensory modalities convey incongruent information (i.e. non-specific intersensory effects; <a href="#pone.0000742-Lewkowicz1">[3]</a>), accuracy of perception can suffer. In the McGurk effect <a href="#pone.0000742-McGurk1">[4]</a>, vision biases audition. In this classic demonstration based on the perception of spoken syllables, incongrent lip movements induce the misperception of auditory inputs. For example, upon hearing/baba/but seeing/gaga/, most subjects will report hearing the fused percept/dada/<a href="#pone.0000742-McGurk1">[4]</a>. Subsequent studies have confirmed that the McGurk effect is a very robust illusion <a href="#pone.0000742-Massaro1">[5]</a>, <a href="#pone.0000742-Rosenblum1">[6]</a>. Although vision was first thought to dominate audio-visual interactions <a href="#pone.0000742-Welch1">[7]</a>, more recent findings suggest that auditory inputs can also bias visual perception. In the “Illusory Flash effect” or “sound-induced flashing” <a href="#pone.0000742-Shams1">[8]</a> a single visual flash can be perceived as two flashes if it is accompanied by two (rather than one) successive sounds. Conversely, in the “Fusion effect” <a href="#pone.0000742-Andersen1">[9]</a> two physical flashes can be fused as one if they are accompanied by a single auditory signal.</p>
<a id="article1.body1.sec1.p2" name="article1.body1.sec1.p2"></a><p>Based on these findings, theoretical accounts relating how the senses interact to create a unified percept have emerged <a href="#pone.0000742-Lewkowicz1">[3]</a>, <a href="#pone.0000742-Shimojo1">[10]</a>. It has recently been suggested that different mechanisms could underlie speech as opposed to non-speech interaction effects. Indeed, in adult observers, audio-visual interaction is stronger when a set of identical stimuli is treated as speech rather than non-speech; this supports a “speech-specific mode of perception” <a href="#pone.0000742-Tuomainen1">[11]</a>. At the physiological level, intersensory speech and non-speech interactions also appear to rely, at least in part, on distinct mechanisms. McGurk-type illusory effects recruit the posterior parietal cortex around 150 ms before activating occipital areas at around 270 ms <a href="#pone.0000742-Kaiser1">[12]</a>. In the Illusory Flash effect, modulation of the visual cortex occurs much earlier (~150 ms; <a href="#pone.0000742-Shams2">[13]</a>). Functional imaging data also show that intersensory interactions rely on multiple brain areas that are differentially involved in the intersensory process (for a review, see <a href="#pone.0000742-Calvert1">[14]</a>). For example, parts of the superior temporal sulcus have been repeatedly shown to play an important role in object recognition, including recognition of audio-visual speech information, whereas audio-visual spatial processing has predominantly been associated with activation of the intraparietal sulcus <a href="#pone.0000742-Calvert2">[15]</a>–<a href="#pone.0000742-Sestieri1">[17]</a>.</p>
<a id="article1.body1.sec1.p3" name="article1.body1.sec1.p3"></a><p>Although speech and non-speech intersensory effects have been well characterized in adult observers, developmental patterns remain poorly understood. McGurk-type illusory phenomena have been studied in infants <a href="#pone.0000742-Rosenblum2">[18]</a>–<a href="#pone.0000742-Desjardins1">[20]</a> and children <a href="#pone.0000742-McGurk1">[4]</a>, <a href="#pone.0000742-Massaro2">[21]</a>, <a href="#pone.0000742-Massaro3">[22]</a> but no study has used an age range sufficiently broad to map the developmental course of this phenomenon. Moreover, to our knowledge no study has attempted to map the developmental course of non-specific, non-speech intersensory effects in childhood and adolescence. Indeed, the few studies that touched on intersensory perception in children have centered on their ability to perceive intersensory <em>equivalence</em> (see <a href="#pone.0000742-Lewkowicz1">[3]</a>). Finally, to our knowledge, no study has yet simultaneously assessed both speech and non-speech intersensory illusions in children and adolescents.</p>
<a id="article1.body1.sec1.p4" name="article1.body1.sec1.p4"></a><p>In the present study, speech (McGurk effect) and non-speech (Illusory Flash effect and Fusion) illusions were presented to the same observers across three age categories (5–9, 10–14 and 15–19 years old). Hence, we aimed at <em>i</em>) determining the presence of non-specific, non-speech intersensory effects at different developmental stages; and <em>ii</em>) describing and contrasting the developmental course of non-specific speech/non-speech illusory effects.</p>
</div>

<div id="section2" class="section"><a id="s2" name="s2" toc="s2" title="Methods"></a><h3>Methods</h3><a id="article1.body1.sec2.p1" name="article1.body1.sec2.p1"></a><p>Thirty-eight French-speaking subjects (15 males, 23 females) aged 5 to 19 years participated in the study. Each age (e.g. 9 years old) was represented by at least two participants. Three age groups were defined <em>a priori</em>: 5–9 (11 subjects), 10–14 (16 subjects), and 15–19 (11 subjects) years of age. The study was approved by the institutional Research Ethics Board of Hôpital Sainte-Justine and written informed consent was obtained from all participants and their parents. Individuals with a diagnosed or suspected neurodevelopmental disorder, attention deficit and hyperactivity disorder or learning disorder were excluded from the study. All participant had normal or corrected-to-normal vision as well as normal hearing.</p>
<a id="article1.body1.sec2.p2" name="article1.body1.sec2.p2"></a><p>Participants were seated in a semi-dark room with the head on a chin rest located 57 cm from the computer screen (and speakers) where the stimuli were presented. The McGurk effect, the Illusory Flash effect and the Fusion effect tasks were performed in a single session, in counterbalanced order. In all tasks, visual stimuli were presented either at fixation or 5 degrees below fixation. This procedure was implemented because the strength of at least one of the illusions used in the present study has been shown to be greater for parafoveal presentations (the Illusory Flash effect; <a href="#pone.0000742-Shams3">[23]</a>). Stimuli were presented on a 17-inch Viewsonic computer screen using a Powermac G4 computer (Apple Inc., Cuppertino, CA, USA). Stimuli were delivered with Psyscope for the McGurk effect and Matlab (The Mathworks Inc., Natick, MA, USA) for the Illusory Flash effect and Fusion effect. To ensure fixation and reject the trials in which fixation did not occur, eye movements were monitored on-line (EyeLink, SR Research, Mississauga, Canada).</p>

<h4>The McGurk effect</h4>
<a id="article1.body1.sec2.sec1.p1" name="article1.body1.sec2.sec1.p1"></a><p>In the McGurk effect task, the voice of an adult male articulating syllables was presented in either a unimodal (auditory only) or bimodal manner. In bimodal trials, the auditory stimulus and the video of the articulatring face (subtending 5 degrees of visual angle) were presented simultaneously. In congruent trials, the auditory (voice) and visual (face) signals carried the same information whereas in incongruent trials, they did not. Five different experimental conditions were used: 1) unimodal auditory/va/; 2) unimodal auditory/ba/; 3) bimodal congruent/va/; 4) bimodal congruent/ba/; and 5) bimodal incongruent auditory/ba/and visual/va/. The bimodal and unimodal trials were repeated ten times each in random order.</p>
<a id="article1.body1.sec2.sec1.p2" name="article1.body1.sec2.sec1.p2"></a><p>Participants were instructed to look at a fixation cross that was presented at the center of the screen for 1000 ms before each trial. Immediately following the disappearance of the cross, a stimulus was presented. Observers were told to simply repeat the syllable they had heard as clearly and precisely as possible. A break was systematically offered at 3 different times during the experiment, but participants could also take a break at any moment if needed. All incorrect responses in the incongruent bimodal condition (anything other than/ba/) were considered manifestations of the McGurk effect.</p>
<a id="article1.body1.sec2.sec1.p3" name="article1.body1.sec2.sec1.p3"></a><p>After the McGurk effect task, a mute control task was performed in order to assess the participants' lip-reading abilities. In this task, the stimuli were unimodal visual/ba/and/va/lip movements. Again, the stimuli were presented at fixation and 5 degrees below fixation. Each condition was repeated 10 times for a total of 40 trials (2 stimuli × 2 locations × 10 trials).</p>


<h4>Illusory Flash effect and Fusion effect</h4>
<a id="article1.body1.sec2.sec2.p1" name="article1.body1.sec2.sec2.p1"></a><p>The characteristics of the stimuli used in the Illusory Flash effect task and Fusion effect were similar to those used in Shams et al. <a href="#pone.0000742-Shams1">[8]</a>, <a href="#pone.0000742-Shams2">[13]</a>. The flash was a white circle subtending 2 degrees of visual angle. It had a luminance of 0.02 cd/m. The auditory signal was made of one or two 7 ms beeps with a frequency of 3500 Hz.</p>
<a id="article1.body1.sec2.sec2.p2" name="article1.body1.sec2.sec2.p2"></a><p>Pilot trials revealed that the inter-flash delay of 67 ms used by Shams et al. <a href="#pone.0000742-Shams1">[8]</a> was too short for many children to be able to visually distinguish one from two flashes. A pre-experimental task was therefore conducted to determine the optimal inter-flash delay for each participant. The fastest delay between flashes in which the participant reached an efficiency score of at least 93% (15/16) was used in the experimental task. Eight conditions (number of flashes (2) X number of beeps (2) X location (2)) were presented in randomized order. Ten trials per condition were presented. Subjects were simply asked to judge the number of flashes that appeared on the screen (one or two).</p>

</div>

<div id="section3" class="section"><a id="s3" name="s3" toc="s3" title="Results"></a><h3>Results</h3>
<h4>McGurk effect</h4>
<a id="article1.body1.sec3.sec1.p1" name="article1.body1.sec3.sec1.p1"></a><p>For visual-only trials (lip-reading), a 3×2 repeated measures ANOVA with <em>age</em> (5–9, 10–14, 15–19) as a between-subjects factor and <em>position</em> (center, periphery) as a within-subjects factor indicated that performance in control trials was homogeneous across age groups (F = 1.9, p = 0.15; <a href="#pone-0000742-g001">Figure 1a</a>). For auditory trials and congruent audiovisual trials, a one-way ANOVA with <em>age</em> as a between-subjects factor was conducted. Performance was similar across age groups for both auditory (F = 0.60, p = 0.45; <a href="#pone-0000742-g001">Figure 1b</a>) and congruent audiovisual (F = 1.17, p = 0.32; <a href="#pone-0000742-g001">Figure 1c</a>) conditions.</p>
<div class="figure" id="pone-0000742-g001"><div class="img"><a name="pone-0000742-g001" title="Click for larger image " href="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000742.g001&amp;representation=PNG_M" data-doi="info:doi/10.1371/journal.pone.0000742" data-uri="info:doi/10.1371/journal.pone.0000742.g001"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000742.g001&amp;representation=PNG_I" alt="thumbnail" class="thumbnail"></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000742.g001/powerpoint">
                    PPT
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000742.g001/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000742.g001/largerimage">
                    PNG
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000742.g001/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pone.0000742.g001.PNG_L"></span>)
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000742.g001/originalimage">
                    TIFF
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000742.g001/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pone.0000742.g001.TIF"></span>)
                </a></li></ul></div><p><strong>Figure 1.  <span>Subjects' performance on the McGurk effect.</span></strong></p><a id="article1.body1.sec3.sec1.fig1.caption1.p1" name="article1.body1.sec3.sec1.fig1.caption1.p1"></a><p>For visual trials (A), auditory trials (B) and congruent audiovisual trials (C), performance was similar across age groups. Performance in the incongruent trials (D) revealed that the 5–9 year-old group perceived significantly fewer McGurk illusions than the two older groups of children. Dark bars: peripheral visual presentation; Light bars: central visual presentation. Error bars represent between-subject SEM. * : p&lt;0.05.</p>
<span>doi:10.1371/journal.pone.0000742.g001</span></div><a id="article1.body1.sec3.sec1.p2" name="article1.body1.sec3.sec1.p2"></a><p>To determine the robustness of the McGurk effect across age groups, a 3×2 repeated measures ANOVA with <em>age</em> as a between-subjects factor and <em>position</em> as a within-subjects factor was performed on bimodal incongruent trials. There were main effects of <em>age</em> (F = 5.10, p = 0.01) and <em>position</em> (F = 4.11, p = 0.05) . The interaction between factors was not significant (F = 0.67, p = 0.52). Post hoc t-tests revealed that the 5–9 year-old group perceived significantly fewer McGurk illusions than the 10–14 (p = 0.02) and the 15–19 year-old groups (p = 0.04) (<a href="#pone-0000742-g001">Figure 1d</a>). In addition, more McGurk illusions were perceived when the visual stimuli were presented at fixation (p = 0.03).</p>
<a id="article1.body1.sec3.sec1.p3" name="article1.body1.sec3.sec1.p3"></a><p>To further test the effect of age on the McGurk effect, individual subjects' ages were correlated with the number of trials in which a McGurk illusion was perceived. A two-tailed Pearson correlation revealed significant effects in both central (r = −0.475, p = 0.003) and peripheral (r = −0.459, p = 0.004) locations, as well as when both these conditions were collapsed (r = −0.49, p = 0.002; <a href="#pone-0000742-g002">Figure 2</a>). Finally, to determine the influence of lip-reading ability on the integration of audio-visual speech cues, a correlation between participants' correct responses in the mute control task and the number of McGurk illusions was computed. The correlation was not significant (r = −0.2, p = 0.23; <a href="#pone-0000742-g003">Figure 3</a>).</p>
<div class="figure" id="pone-0000742-g002"><div class="img"><a name="pone-0000742-g002" title="Click for larger image " href="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000742.g002&amp;representation=PNG_M" data-doi="info:doi/10.1371/journal.pone.0000742" data-uri="info:doi/10.1371/journal.pone.0000742.g002"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000742.g002&amp;representation=PNG_I" alt="thumbnail" class="thumbnail"></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000742.g002/powerpoint">
                    PPT
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000742.g002/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000742.g002/largerimage">
                    PNG
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000742.g002/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pone.0000742.g002.PNG_L"></span>)
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000742.g002/originalimage">
                    TIFF
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000742.g002/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pone.0000742.g002.TIF"></span>)
                </a></li></ul></div><p><strong>Figure 2.  <span>Percent of correct (non-biased) responses in the incongruent condition McGurk effect plotted as a function of age.</span></strong></p><span>doi:10.1371/journal.pone.0000742.g002</span></div><div class="figure" id="pone-0000742-g003"><div class="img"><a name="pone-0000742-g003" title="Click for larger image " href="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000742.g003&amp;representation=PNG_M" data-doi="info:doi/10.1371/journal.pone.0000742" data-uri="info:doi/10.1371/journal.pone.0000742.g003"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000742.g003&amp;representation=PNG_I" alt="thumbnail" class="thumbnail"></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000742.g003/powerpoint">
                    PPT
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000742.g003/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000742.g003/largerimage">
                    PNG
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000742.g003/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pone.0000742.g003.PNG_L"></span>)
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000742.g003/originalimage">
                    TIFF
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000742.g003/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pone.0000742.g003.TIF"></span>)
                </a></li></ul></div><p><strong>Figure 3.  <span>Percent of correct (non-biased) responses in the incongruent condition McGurk effect plotted as a function of lipreading ability.</span></strong></p><span>doi:10.1371/journal.pone.0000742.g003</span></div>

<h4>Illusory Flash effect and Fusion effect</h4>
<a id="article1.body1.sec3.sec2.p1" name="article1.body1.sec3.sec2.p1"></a><p>The original illusion (Shams, 2000) was replicated as the number of correct responses in the 1 flash/2 beeps condition was drastically reduced (<a href="#pone-0000742-g004">Figure 4a</a>). A 3×2 repeated measures ANOVA with <em>age</em> (5–9, 10–14, 15–19) as a between-subjects factor and <em>position</em> (center, periphery) as a within-subjects factor revealed a main effect for <em>position</em> (F = 10.64, p = 0.002), but no main effect for <em>age</em> (F = 0.52, p = 0.60). The interaction was also non-significant (F = 0.74, p = 0.49). This is in line with previous work, where the Illusory Flash effect has been shown to be more robust at a perifoveal location (Shams et al., 2002). The strength of the illusion was not correlated with participant age (center: r = 0.12, p = 0.456; periphery: r = 0.25, p = 0.12). As for the Fusion effect (<a href="#pone-0000742-g004">Figure 4b</a>), there were no significant effects for either <em>age</em> (F = 1.81, p = 0.18) or <em>position</em> (F = 1.76, p = 0.19) and the interaction was non-significant (F = 0.22, p = 0.80).</p>
<div class="figure" id="pone-0000742-g004"><div class="img"><a name="pone-0000742-g004" title="Click for larger image " href="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000742.g004&amp;representation=PNG_M" data-doi="info:doi/10.1371/journal.pone.0000742" data-uri="info:doi/10.1371/journal.pone.0000742.g004"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000742.g004&amp;representation=PNG_I" alt="thumbnail" class="thumbnail"></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000742.g004/powerpoint">
                    PPT
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000742.g004/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000742.g004/largerimage">
                    PNG
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000742.g004/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pone.0000742.g004.PNG_L"></span>)
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000742.g004/originalimage">
                    TIFF
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000742.g004/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pone.0000742.g004.TIF"></span>)
                </a></li></ul></div><p><strong>Figure 4.  <span>Subjects' performance on the Illusory Flash (A) and the Fusion (B) effects. For both illusory percepts, there was no effect of age.</span></strong></p><a id="article1.body1.sec3.sec2.fig1.caption1.p1" name="article1.body1.sec3.sec2.fig1.caption1.p1"></a><p> Error bars represent between-subject SEM.</p>
<span>doi:10.1371/journal.pone.0000742.g004</span></div><a id="article1.body1.sec3.sec2.p2" name="article1.body1.sec3.sec2.p2"></a><p>There was no correlation between the Illusory Flash effect and the McGurk effect (center: r = −0.167, p = 0.32; periphery: r = −0.22, p = 0.182) or the Fusion effect and the McGurk effect (center: r = −0.28; periphery: r = −0.206, p = −0.21).</p>

</div>

<div id="section4" class="section"><a id="s4" name="s4" toc="s4" title="Discussion"></a><h3>Discussion</h3><a id="article1.body1.sec4.p1" name="article1.body1.sec4.p1"></a><p>The purpose of this study was to investigate the developmental course of non-specific audio-visual effects on a maturational continuum. Our main finding is a discrepancy in the maturational patterns of speech and non-speech audio-visual effects.</p>
<a id="article1.body1.sec4.p2" name="article1.body1.sec4.p2"></a><p>Illusory percepts of audio-visual speech elements have been shown to occur in infants <a href="#pone.0000742-Rosenblum2">[18]</a>–<a href="#pone.0000742-Desjardins1">[20]</a> but these are weaker and more inconsistent than what is observed in adults, suggesting that experience with speech may be an important component of audio-visual speech perception <a href="#pone.0000742-Desjardins1">[20]</a>. In pre-school and school-aged children, previous findings indicate that incongruent visual input has less influence on the final percept resulting from a McGurk illusion <a href="#pone.0000742-McGurk1">[4]</a>, <a href="#pone.0000742-Massaro2">[21]</a>, <a href="#pone.0000742-Massaro3">[22]</a> and that when a single modality is chosen for the final bimodal percept in a McGurk illusion, children choose the auditory modality whereas adults choose vision <a href="#pone.0000742-McGurk1">[4]</a>, <a href="#pone.0000742-Massaro2">[21]</a>, <a href="#pone.0000742-Massaro3">[22]</a>. Our results are consistent with and extend previous findings by showing that an important proportion of the maturational processes underlying speech intersensory effects is not completely developed before 10 years of age, since 5–9 year-olds presented a different pattern of intersensory speech effect in comparison with the two older groups. Indeed, the significant correlation between age and the frequency of illusory percepts suggests that audio-visual speech perception continues to evolve during childhood. Massaro et al. <a href="#pone.0000742-Massaro3">[22]</a> have suggested that the weaker McGurk effect observed in young children is due to poorer lip-reading abilities. We found no significant difference in lip-reading abilities across the three age-groups. Although a ceiling effect in the older group of children may have prevented small lip-reading differences from being revealed, the absence of a significant correlation between lip-reading ability and the frequency of McGurk illusions argues against this explanation. In addition, Massaro and collaborators have suggested that lip-reading performance becomes similar to adults “sometime after the child's 6<sup>th</sup> year” <a href="#pone.0000742-Massaro3">[22]</a>, a notion that is supported by a study showing that speech reading abilities become stable near 7 years of age <a href="#pone.0000742-HnathChisolm1">[24]</a>. Our data are in line with this interpretation and suggest that the weaker influence of visual input on bimodal speech perception in children that are more than 6 years old may be explained by the degree to which visual and speech cues are integrated.</p>
<a id="article1.body1.sec4.p3" name="article1.body1.sec4.p3"></a><p>To our knowledge, a single study has shown that non-speech illusions can occur in infants. In the “Streaming-Bouncing” effect <a href="#pone.0000742-Sekuler1">[25]</a>, two disks move towards the centre of a screen. When the two disks cross in silence, they are perceived as passing through one another. However, when a sound is emitted as the disks meet they appear to bounce off each other. Using this effect, Scheier et al. <a href="#pone.0000742-Scheier1">[26]</a> have shown that this non-specific intersensory capability emerges halfway through the first year of life. Thus, prior to the present investigation, non-speech audio-visual illusions have only been observed in a spatiotemporal task where audition biases vision. The developmental course of non-speech illusory percept remains uncharted. Our findings reveal a homogeneous profile for all ages for the two non-verbal tasks. Therefore, all age groups performed equally on both the Illusory Flash effect and the Fusion effect. These findings are consistent with the suggestion that audio-visual non-speech integration appears very early in life <a href="#pone.0000742-Scheier1">[26]</a>.</p>
<a id="article1.body1.sec4.p4" name="article1.body1.sec4.p4"></a><p>It is important to note that both illusion categories not only differ with respect to the speech/non speech content but also in the way participants respond. In the McGurk effect, children must report what they hear whereas in the two non-speech illusions they report what they see. Some have suggested that the strength of a single modality on perceptual judgment depends on the attention it is given <a href="#pone.0000742-Welch2">[27]</a>, which in the present case could explain the different pattern of age-related differences in the two illusory categories. In a study of bimodal speech perception in 6 year old children, however, Massaro <a href="#pone.0000742-Massaro2">[21]</a> showed that directing attention to the speaker's mouth did not modify the proportion of incorrect responses in a McGurk-like task. Electrophysiological data also support the idea that audiovisual integration is a preattentive phenomenon since a mismatch negativity can be evoked by McGurk-like stimuli <a href="#pone.0000742-Colin1">[28]</a>. As such, some authors have suggested that audiovisual speech perception is an automatic process (see <a href="#pone.0000742-Tiippana1">[29]</a> for a review). Conversely, it has been shown that responses to McGurk stimuli differ when participants are asked to respond to the visual or auditory cue <a href="#pone.0000742-Dekle1">[30]</a> and directing attention away from the mouth area significantly reduces the strength of the McGurk effect <a href="#pone.0000742-Tiippana1">[29]</a>. Interestingly, contrary to audiovisual stimuli, unisensory responses in the McGurk task do not appear to be influenced by attentional shifts, suggesting that it is integration <em>per se</em> that varies with attention <a href="#pone.0000742-Tiippana1">[29]</a>. However, when data are fitted in a model of perception (Fuzzy Logical Model of Perception; <a href="#pone.0000742-Massaro4">[31]</a>), predictions are that it is not the integration level that is affected by attention but unisensory processing <a href="#pone.0000742-Tiippana1">[29]</a>. These discrepancies highlight the fact it is still premature to ascertain whether it is only the speech/non speech distinction that separates performance on both types of illusions tested here. In addition to attention and modality of response, it may be that the different pattern of results reflects the fact that in young children vision may have less impact on hearing than in older children, whereas hearing has comparable effects on vision across all ages. In this case, the fact that vision biases audition in the McGurk effect and that audition biases vision in the illusory flash effect may also explain parts of the data. Nevertheless, our results clearly show that the McGurk illusion, which involves speech material, does not follow the same developmental rules than the illusory flash and fusion effects. Further studies are needed to specifically address which factors contribute to this difference, and to what extent.</p>
<a id="article1.body1.sec4.p5" name="article1.body1.sec4.p5"></a><p>Finally, the suggestion that speech and non-speech integration follow different developmental time courses does not exclude the possibility that they share common mechanisms. Indeed, it may be hypothesized that both illusory phenomena are subtended similarly at low hierarchical levels whereas audio-visual integration of speech elements requires supplementary processing. For example, it has been shown that brainstem structures are involved in <em>both</em> audio-visual speech <a href="#pone.0000742-Musacchia1">[32]</a>, <a href="#pone.0000742-Champoux1">[33]</a> and non-speech integration <a href="#pone.0000742-Holmes1">[34]</a>, <a href="#pone.0000742-Stein1">[35]</a>, suggesting the existence of common substrates.</p>
</div>



<div class="contributions"><a id="authcontrib" name="authcontrib" toc="authcontrib" title="Author Contributions"></a><h3>Author Contributions</h3><p>Conceived and designed the experiments: FL HT CT FC BB. Performed the experiments: PV CT FC. Analyzed the data: PV CT FC. Contributed reagents/materials/analysis tools: FC. Wrote the paper: FL HT CT FC BB.</p></div><div><a id="references" name="references" toc="references" title="References"></a><h3>References</h3><ol class="references"><li><span class="label">1.
              </span><a name="pone.0000742-Frassinetti1" id="pone.0000742-Frassinetti1"></a>Frassinetti F, Bolognini N, Ladavas E (2002) Enhancement of visual perception by crossmodal visuo-auditory interaction. Exp Brain Res  147: 332–343.  <ul class="find" data-citedArticleID="977768" data-doi="10.1007/s00221-002-1262-y"><li><a href="http://dx.doi.org/10.1007/s00221-002-1262-y" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Enhancement+of+visual+perception+by+crossmodal+visuo-auditory+interaction." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Enhancement+of+visual+perception+by+crossmodal+visuo-auditory+interaction.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">2.
              </span><a name="pone.0000742-TederSlejarvi1" id="pone.0000742-TederSlejarvi1"></a>Teder-Sälejarvi WA, McDonald JJ, Di Russo F, Hillyard SA (2002) An analysis of audio-visual crossmodal integration by means of event-related potential (ERP) recordings. Brain Res Cogn Brain Res  14: 106–114.  <ul class="find" data-citedArticleID="977812" data-doi="10.1016/s0926-6410(02)00065-4"><li><a href="http://dx.doi.org/10.1016/s0926-6410(02)00065-4" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=An+analysis+of+audio-visual+crossmodal+integration+by+means+of+event-related+potential+%28ERP%29+recordings." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22An+analysis+of+audio-visual+crossmodal+integration+by+means+of+event-related+potential+%28ERP%29+recordings.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">3.
              </span><a name="pone.0000742-Lewkowicz1" id="pone.0000742-Lewkowicz1"></a>Lewkowicz DJ (2002) Heterogeneity and heterochrony in the development of intersensory perception. Brain Res Cogn Brain Res  14: 41–63.  <ul class="find" data-citedArticleID="977776" data-doi="10.1016/s0926-6410(02)00060-5"><li><a href="http://dx.doi.org/10.1016/s0926-6410(02)00060-5" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Heterogeneity+and+heterochrony+in+the+development+of+intersensory+perception." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Heterogeneity+and+heterochrony+in+the+development+of+intersensory+perception.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">4.
              </span><a name="pone.0000742-McGurk1" id="pone.0000742-McGurk1"></a>McGurk H, McDonald J (1976) Hearing lips and seeing voices. Nature  264: 746–748.  <ul class="find" data-citedArticleID="977788" data-doi="10.1038/264746a0"><li><a href="http://dx.doi.org/10.1038/264746a0" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Hearing+lips+and+seeing+voices." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Hearing+lips+and+seeing+voices.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">5.
              </span><a name="pone.0000742-Massaro1" id="pone.0000742-Massaro1"></a>Massaro DW, Cohen MM (1990) Perception of synthesized audible and visible speech. Psychol Sci  1: 55–63.  <ul class="find" data-citedArticleID="977780" data-doi="10.1111/j.1467-9280.1990.tb00068.x"><li><a href="http://dx.doi.org/10.1111/j.1467-9280.1990.tb00068.x" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Perception+of+synthesized+audible+and+visible+speech." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Perception+of+synthesized+audible+and+visible+speech.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">6.
              </span><a name="pone.0000742-Rosenblum1" id="pone.0000742-Rosenblum1"></a>Rosenblum LD, Saldana HM (1996) An audiovisual test of kinematic primitives for visual speech perception. J Exp Psychol Hum Percept Perform  22: 318–331.  <ul class="find" data-citedArticleID="977792" data-doi="10.1037/0096-1523.22.2.318"><li><a href="http://dx.doi.org/10.1037/0096-1523.22.2.318" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=An+audiovisual+test+of+kinematic+primitives+for+visual+speech+perception." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22An+audiovisual+test+of+kinematic+primitives+for+visual+speech+perception.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">7.
              </span><a name="pone.0000742-Welch1" id="pone.0000742-Welch1"></a>Welch RB (1999) How can we determine if the sense of presence affects task performance? Presence Teleoper Virtual Environ  8: 574–577.  <ul class="find" data-citedArticleID="977818" data-doi="10.1162/105474699566387"><li><a href="http://dx.doi.org/10.1162/105474699566387" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=How+can+we+determine+if+the+sense+of+presence+affects+task+performance%3F" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22How+can+we+determine+if+the+sense+of+presence+affects+task+performance%3F%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">8.
              </span><a name="pone.0000742-Shams1" id="pone.0000742-Shams1"></a>Shams L, Kamitani Y, Shimojo S (2000) What you see is what you hear. Nature  408: 788.  <ul class="find" data-citedArticleID="977802" data-doi="10.1038/35048669"><li><a href="http://dx.doi.org/10.1038/35048669" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=What+you+see+is+what+you+hear." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22What+you+see+is+what+you+hear.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">9.
              </span><a name="pone.0000742-Andersen1" id="pone.0000742-Andersen1"></a>Andersen TS, Tiippana K, Sams M (2004) Factors influencing audio-visual fission and fusion illusions. Brain Res Cogn Brain Res  2: 301–308.  <ul class="find" data-citedArticleID="977752" data-doi="10.1016/j.cogbrainres.2004.06.004"><li><a href="http://dx.doi.org/10.1016/j.cogbrainres.2004.06.004" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Factors+influencing+audio-visual+fission+and+fusion+illusions." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Factors+influencing+audio-visual+fission+and+fusion+illusions.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">10.
              </span><a name="pone.0000742-Shimojo1" id="pone.0000742-Shimojo1"></a>Shimojo S, Shams L (2001) Sensory modalities are not separate modalities: plasticity and interactions. Curr Opin Neurobiol  11: 505–509.  <ul class="find" data-citedArticleID="977808" data-doi="10.1016/s0959-4388(00)00241-5"><li><a href="http://dx.doi.org/10.1016/s0959-4388(00)00241-5" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Sensory+modalities+are+not+separate+modalities%3A+plasticity+and+interactions." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Sensory+modalities+are+not+separate+modalities%3A+plasticity+and+interactions.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">11.
              </span><a name="pone.0000742-Tuomainen1" id="pone.0000742-Tuomainen1"></a>Tuomainen J, Andersen TS, Tiippana K, Sams M (2005) Audio-visual speech perception is special. Cognition  96: 13–22.  <ul class="find" data-citedArticleID="977816" data-doi="10.1016/j.cognition.2004.10.004"><li><a href="http://dx.doi.org/10.1016/j.cognition.2004.10.004" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Audio-visual+speech+perception+is+special." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Audio-visual+speech+perception+is+special.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">12.
              </span><a name="pone.0000742-Kaiser1" id="pone.0000742-Kaiser1"></a>Kaiser J, Hertrich I, Ackermann H, Mathiak K, Lutzenberger W (2005) Hearing lips: gamma-band activity during audio-visual speech perception. Cereb Cortex  15: 646–653.  <ul class="find" data-citedArticleID="977774" data-doi="10.1093/cercor/bhh166"><li><a href="http://dx.doi.org/10.1093/cercor/bhh166" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Hearing+lips%3A+gamma-band+activity+during+audio-visual+speech+perception." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Hearing+lips%3A+gamma-band+activity+during+audio-visual+speech+perception.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">13.
              </span><a name="pone.0000742-Shams2" id="pone.0000742-Shams2"></a>Shams L, Kamitani Y, Thompson S, Shimojo S (2001) Sound alters visual evoked potentials in humans. Neuroreport  12: 3849–3852.  <ul class="find" data-citedArticleID="977804" data-doi="10.1097/00001756-200112040-00049"><li><a href="http://dx.doi.org/10.1097/00001756-200112040-00049" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Sound+alters+visual+evoked+potentials+in+humans." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Sound+alters+visual+evoked+potentials+in+humans.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">14.
              </span><a name="pone.0000742-Calvert1" id="pone.0000742-Calvert1"></a>Calvert GA (2001) Crossmodal processing in the human brain: insights from functional neuroimaging studies. Cereb Cortex  11: 1110–1123.  <ul class="find" data-citedArticleID="977756" data-doi="10.1093/cercor/11.12.1110"><li><a href="http://dx.doi.org/10.1093/cercor/11.12.1110" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Crossmodal+processing+in+the+human+brain%3A+insights+from+functional+neuroimaging+studies." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Crossmodal+processing+in+the+human+brain%3A+insights+from+functional+neuroimaging+studies.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">15.
              </span><a name="pone.0000742-Calvert2" id="pone.0000742-Calvert2"></a>Calvert GA, Campbell R, Brammer MJ (2000) Evidence from functional magnetic resonance imaging of crossmodal binding in the human heteromodal cortex. Curr Biol  10: 649–657.  <ul class="find" data-citedArticleID="977758" data-doi="10.1016/s0960-9822(00)00513-3"><li><a href="http://dx.doi.org/10.1016/s0960-9822(00)00513-3" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Evidence+from+functional+magnetic+resonance+imaging+of+crossmodal+binding+in+the+human+heteromodal+cortex." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Evidence+from+functional+magnetic+resonance+imaging+of+crossmodal+binding+in+the+human+heteromodal+cortex.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">16.
              </span><a name="pone.0000742-Macaluso1" id="pone.0000742-Macaluso1"></a>Macaluso E, George N, Dolan R, Spence C, Driver J (2004) Spatial and temporal factors during processing of audiovisual speech: a PET study. Neuroimage  21: 725–732.  <ul class="find" data-citedArticleID="977778" data-doi="10.1016/j.neuroimage.2003.09.049"><li><a href="http://dx.doi.org/10.1016/j.neuroimage.2003.09.049" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Spatial+and+temporal+factors+during+processing+of+audiovisual+speech%3A+a+PET+study." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Spatial+and+temporal+factors+during+processing+of+audiovisual+speech%3A+a+PET+study.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">17.
              </span><a name="pone.0000742-Sestieri1" id="pone.0000742-Sestieri1"></a>Sestieri C, Di Matteo R, Ferretti A, Del Gratta C, Caulo M, et al.  (2006) An fMRI study of the binding of audio-visual information: the dissociation between object and space processing. Cogn Process  7: 138–139.  <ul class="find" data-citedArticleID="977800" data-doi="10.1007/s10339-006-0105-3"><li><a href="http://dx.doi.org/10.1007/s10339-006-0105-3" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=An+fMRI+study+of+the+binding+of+audio-visual+information%3A+the+dissociation+between+object+and+space+processing." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22An+fMRI+study+of+the+binding+of+audio-visual+information%3A+the+dissociation+between+object+and+space+processing.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">18.
              </span><a name="pone.0000742-Rosenblum2" id="pone.0000742-Rosenblum2"></a>Rosenblum LD, Schmuckler MA, Johnson JA (1997) The McGurk effect in infants. Percept Psychophys  59: 347–357.  <ul class="find" data-citedArticleID="977794" data-doi="10.3758/bf03211902"><li><a href="http://dx.doi.org/10.3758/bf03211902" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=The+McGurk+effect+in+infants." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22The+McGurk+effect+in+infants.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">19.
              </span><a name="pone.0000742-Burnham1" id="pone.0000742-Burnham1"></a>Burnham D, Dodd B (2004) Auditory-visual speech integration by prelinguistic infants: perception of an emergent consonant in the McGurk effect. Dev Psychobiol  45: 204–220.  <ul class="find" data-citedArticleID="977754" data-doi="10.1002/dev.20032"><li><a href="http://dx.doi.org/10.1002/dev.20032" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Auditory-visual+speech+integration+by+prelinguistic+infants%3A+perception+of+an+emergent+consonant+in+the+McGurk+effect." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Auditory-visual+speech+integration+by+prelinguistic+infants%3A+perception+of+an+emergent+consonant+in+the+McGurk+effect.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">20.
              </span><a name="pone.0000742-Desjardins1" id="pone.0000742-Desjardins1"></a>Desjardins RN, Werker JF (2004) Is the integration of heard and seen speech mandatory for infants? Dev Psychobiol  45: 187–203.  <ul class="find" data-citedArticleID="977766" data-doi="10.1002/dev.20033"><li><a href="http://dx.doi.org/10.1002/dev.20033" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Is+the+integration+of+heard+and+seen+speech+mandatory+for+infants%3F" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Is+the+integration+of+heard+and+seen+speech+mandatory+for+infants%3F%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">21.
              </span><a name="pone.0000742-Massaro2" id="pone.0000742-Massaro2"></a>Massaro DW (1984) Children's perception of visual and auditory speech. Child Dev  55: 1777–1788.  <ul class="find" data-citedArticleID="977782" data-doi="10.1111/j.1467-8624.1984.tb00420.x"><li><a href="http://dx.doi.org/10.1111/j.1467-8624.1984.tb00420.x" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Children%27s+perception+of+visual+and+auditory+speech." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Children%27s+perception+of+visual+and+auditory+speech.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">22.
              </span><a name="pone.0000742-Massaro3" id="pone.0000742-Massaro3"></a>Massaro DW, Thompson LA, Barron B, Laren E (1986) Developmental changes in visual and auditory contributions to speech perception. J Exp Child Psychol  1: 93–113.  <ul class="find" data-citedArticleID="977784" data-doi="10.1016/0022-0965(86)90053-6"><li><a href="http://dx.doi.org/10.1016/0022-0965(86)90053-6" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Developmental+changes+in+visual+and+auditory+contributions+to+speech+perception." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Developmental+changes+in+visual+and+auditory+contributions+to+speech+perception.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">23.
              </span><a name="pone.0000742-Shams3" id="pone.0000742-Shams3"></a>Shams L, Kamitani Y, Shimojo S (2002) Visual illusion induced by sound. Cogn Brain Res  14: 147–152.  <ul class="find" data-citedArticleID="977806" data-doi="10.1016/s0926-6410(02)00069-1"><li><a href="http://dx.doi.org/10.1016/s0926-6410(02)00069-1" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Visual+illusion+induced+by+sound." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Visual+illusion+induced+by+sound.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">24.
              </span><a name="pone.0000742-HnathChisolm1" id="pone.0000742-HnathChisolm1"></a>Hnath-Chisolm TE, Laipply E, Boothroyd AAge-related changes on a children's test of sensory-level speech perception capacity. J Speech Lang Hear Res  41: 94–106.  <ul class="find" data-citedArticleID="977770"><li><a href="http://www.crossref.org/guestquery/?auth2=&amp;atitle2=Age-related+changes+on+a+children%27s+test+of+sensory-level+speech+perception+capacity.&amp;auth=&amp;atitle=Age-related+changes+on+a+children%27s+test+of+sensory-level+speech+perception+capacity." target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Age-related+changes+on+a+children%27s+test+of+sensory-level+speech+perception+capacity." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Age-related+changes+on+a+children%27s+test+of+sensory-level+speech+perception+capacity.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">25.
              </span><a name="pone.0000742-Sekuler1" id="pone.0000742-Sekuler1"></a>Sekuler R, Sekuler AB, Lau R (1997) Sound alters visual motion perception. Nature  385: 308.  <ul class="find" data-citedArticleID="977798" data-doi="10.1038/385308a0"><li><a href="http://dx.doi.org/10.1038/385308a0" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Sound+alters+visual+motion+perception." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Sound+alters+visual+motion+perception.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">26.
              </span><a name="pone.0000742-Scheier1" id="pone.0000742-Scheier1"></a>Scheier C, Lewkowicz DJ, Shimojo S (2003) Sound induces perceptual reorganization of an ambiguous motion display in human infants. Dev Science  6: 233–244.  <ul class="find" data-citedArticleID="977796" data-doi="10.1111/1467-7687.00276"><li><a href="http://dx.doi.org/10.1111/1467-7687.00276" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Sound+induces+perceptual+reorganization+of+an+ambiguous+motion+display+in+human+infants." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Sound+induces+perceptual+reorganization+of+an+ambiguous+motion+display+in+human+infants.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">27.
              </span><a name="pone.0000742-Welch2" id="pone.0000742-Welch2"></a>Welch RB, Warren DH (1980) Immediate perceptual response to intersensory discrepancy. Psychol Bull  88: 638–667.  <ul class="find" data-citedArticleID="977820" data-doi="10.1037/0033-2909.88.3.638"><li><a href="http://dx.doi.org/10.1037/0033-2909.88.3.638" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Immediate+perceptual+response+to+intersensory+discrepancy." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Immediate+perceptual+response+to+intersensory+discrepancy.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">28.
              </span><a name="pone.0000742-Colin1" id="pone.0000742-Colin1"></a>Colin C, Radeau M, Soquet A, Demolin D, Colin F, Deltenre P (2002) Mismatch negativity evoked by the McGurk-MacDonald effect: a phonetic representation within short-term memory. Clin Neurophysiol  113: 495–506.  <ul class="find" data-citedArticleID="977762" data-doi="10.1016/s1388-2457(02)00024-x"><li><a href="http://dx.doi.org/10.1016/s1388-2457(02)00024-x" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Mismatch+negativity+evoked+by+the+McGurk-MacDonald+effect%3A+a+phonetic+representation+within+short-term+memory." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Mismatch+negativity+evoked+by+the+McGurk-MacDonald+effect%3A+a+phonetic+representation+within+short-term+memory.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">29.
              </span><a name="pone.0000742-Tiippana1" id="pone.0000742-Tiippana1"></a>Tiippana K, Andersen TS, Sams M (2004) Visual attention modulates audiovisual speech perception. Eur J Cogn Psychol  16: 457–472.  <ul class="find" data-citedArticleID="977814" data-doi="10.1080/09541440340000268"><li><a href="http://dx.doi.org/10.1080/09541440340000268" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Visual+attention+modulates+audiovisual+speech+perception." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Visual+attention+modulates+audiovisual+speech+perception.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">30.
              </span><a name="pone.0000742-Dekle1" id="pone.0000742-Dekle1"></a>Dekle DJ, Fowler CA, Funnell MG (1992) Audiovisual integration in perception of real words. Percept Psychophys  51: 355–362.  <ul class="find" data-citedArticleID="977764" data-doi="10.3758/bf03211629"><li><a href="http://dx.doi.org/10.3758/bf03211629" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Audiovisual+integration+in+perception+of+real+words." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Audiovisual+integration+in+perception+of+real+words.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">31.
              </span><a name="pone.0000742-Massaro4" id="pone.0000742-Massaro4"></a>Massaro DW (1998) Perceiving talking faces. Cambridge: MIT Press.   <ul class="find-nolinks"></ul></li><li><span class="label">32.
              </span><a name="pone.0000742-Musacchia1" id="pone.0000742-Musacchia1"></a>Musacchia G, Sams M, Nicol T, Kraus N (2006) Seeing speech affects acoustic information processing in the human brainstem. Exp Brain Res  168: 1–10.  <ul class="find" data-citedArticleID="977790" data-doi="10.1007/s00221-005-0071-5"><li><a href="http://dx.doi.org/10.1007/s00221-005-0071-5" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Seeing+speech+affects+acoustic+information+processing+in+the+human+brainstem." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Seeing+speech+affects+acoustic+information+processing+in+the+human+brainstem.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">33.
              </span><a name="pone.0000742-Champoux1" id="pone.0000742-Champoux1"></a>Champoux F, Tremblay C, Mercier C, Lassonde M, Lepore F, et al.  (2006) A role for the inferior colliculus in multisensory integration. Neuroreport  17: 1607–1610.  <ul class="find" data-citedArticleID="977760" data-doi="10.1097/01.wnr.0000236856.93586.94"><li><a href="http://dx.doi.org/10.1097/01.wnr.0000236856.93586.94" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=A+role+for+the+inferior+colliculus+in+multisensory+integration." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22A+role+for+the+inferior+colliculus+in+multisensory+integration.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">34.
              </span><a name="pone.0000742-Holmes1" id="pone.0000742-Holmes1"></a>Holmes NP, Spence C (2005) Multisensory integration: space, time and superadditivity. Curr Biol  15: 762–764.  <ul class="find" data-citedArticleID="977772" data-doi="10.1016/j.cub.2005.08.058"><li><a href="http://dx.doi.org/10.1016/j.cub.2005.08.058" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Multisensory+integration%3A+space%2C+time+and+superadditivity." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Multisensory+integration%3A+space%2C+time+and+superadditivity.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">35.
              </span><a name="pone.0000742-Stein1" id="pone.0000742-Stein1"></a>Stein BE (2005) The development of a dialogue between cortex and midbrain to integrate multisensory information. Exp Brain Res  166: 305–315.  <ul class="find" data-citedArticleID="977810" data-doi="10.1007/s00221-005-2372-0"><li><a href="http://dx.doi.org/10.1007/s00221-005-2372-0" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=The+development+of+a+dialogue+between+cortex+and+midbrain+to+integrate+multisensory+information." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22The+development+of+a+dialogue+between+cortex+and+midbrain+to+integrate+multisensory+information.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li></ol></div>

  </div>

      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000742.XML" value="57961"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000742.PDF" value="225404"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000742.g001.PNG_L" value="697431"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000742.g001.PNG_M" value="124824"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000742.g001.PNG_S" value="13123"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000742.g001.TIF" value="1558260"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000742.g001.PNG_I" value="51080"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000742.g002.PNG_L" value="124070"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000742.g002.PNG_M" value="97315"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000742.g002.PNG_S" value="11168"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000742.g002.TIF" value="177962"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000742.g002.PNG_I" value="32390"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000742.g003.PNG_L" value="120562"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000742.g003.PNG_M" value="95659"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000742.g003.PNG_S" value="10051"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000742.g003.TIF" value="171950"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000742.g003.PNG_I" value="32409"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000742.g004.PNG_L" value="328370"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000742.g004.PNG_M" value="74116"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000742.g004.PNG_S" value="15102"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000742.g004.TIF" value="622204"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000742.g004.PNG_I" value="95652"/>

</div>
<div class="sidebar">

  <div class="article-actions cf">
      <div class="download">
        <span class="btn"><a href="/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0000742&amp;representation=PDF" title="Download" target="_blank">Download PDF</a></span>
      </div>
      <div class="btn-reveal dropdown">
        <div class="dropdown-icon">
          <span class="btn">&nbsp;</span>
        </div>

        <div class="content">
          <ul class="bullet">
            <li><a href="/article/citationList.action?articleURI=info%3Adoi%2F10.1371%2Fjournal.pone.0000742" title="Download citations">Citation</a></li>
            <li><a href="/article/fetchObjectAttachment.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0000742&amp;representation=XML" title="Download article XML">XML</a></li>
          </ul>
        </div>
      </div> <!-- end btn-reveal dropdown-->


    <div class="btn-reveal flt-l">
        <span class="btn">Print</span>
        <div class="content">
            <ul class="bullet">
                <li id="print-article"><a href="#" onclick="if(typeof(_gaq) != 'undefined'){ _gaq.push(['_trackEvent','Article', 'Print', 'Click']); } window.print(); return false;" title="Print Article">Print article</a></li>
                <li>
                  <a href="https://www.odysseypress.com/onlinehost/reprint_order.php?type=A&page=0&journal=7&doi=10.1371/journal.pone.0000742&volume=&issue=&title=Speech and Non-Speech Audio-Visual Illusions: A Developmental Study&author_name=Corinne%20Tremblay%2C%20Fran%C3%A7ois%20Champoux%2C%20Patrice%20Voss%2C%20Benoit%20A.%20Bacon%2C%20Franco%20Lepore%2C%20Hugo%20Th%C3%A9oret&start_page=1&end_page=5" title="Odyssey Press">EzReprint</a>
                </li>
            </ul>
        </div>
    </div>

    <div class="btn-reveal flt-r">
        <span class="btn">Share</span>
        <div class="content">
            <ul class="social">
                <li><a href="http://www.reddit.com/submit?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0000742" target="_blank" title="Submit to Reddit"><img src="/images/icon.reddit.16.png" width="16" height="16" alt="Reddit">Reddit</a></li>

                <li><a href="https://plus.google.com/share?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0000742" target="_blank" title="Share on Google+"><img src="/images/icon.gplus.16.png" width="16" height="16" alt="Google+">Google+</a></li>

                <li><a href="http://www.stumbleupon.com/submit?url=http%3A%2F%2Fwww.plosone.org%2Farticle%2Finfo%253Adoi%252F10.1371%252Fjournal.pone.0000742" target="_blank" title="Add to StumbleUpon"><img src="/images/icon.stumble.16.png" width="16" height="16" alt="StumbleUpon">StumbleUpon</a></li>

                <li><a href="http://www.facebook.com/share.php?u=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0000742&amp;t=Speech%20and%20Non-Speech%20Audio-Visual%20Illusions%3A%20A%20Developmental%20Study" target="_blank" title="Share on Facebook"><img src="/images/icon.fb.16.png" width="16" height="16" alt="Facebook">Facebook</a></li>

                <li><a href="http://www.linkedin.com/shareArticle?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0000742&title=Speech%20and%20Non-Speech%20Audio-Visual%20Illusions%3A%20A%20Developmental%20Study&summary=Checkout%20this%20article%20I%20found%20at%20PLOS" target="_blank" title="Add to LinkedIn"><img src="/images/icon.linkedin.16.png" width="16" height="16" alt="Mendeley">LinkedIn</a></li>

                <li><a href="http://www.citeulike.org/posturl?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0000742&amp;title=Speech%20and%20Non-Speech%20Audio-Visual%20Illusions%3A%20A%20Developmental%20Study" target="_blank" title="Add to CiteULike"><img src="/images/icon.cul.16.png" width="16" height="16" alt="CiteULike">CiteULike</a></li>

                <li><a href="http://www.mendeley.com/import/?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0000742" target="_blank" title="Add to Mendeley"><img src="/images/icon.mendeley.16.png" width="16" height="16" alt="Mendeley">Mendeley</a></li>

                <li><a href="https://www.pubchase.com/library?add_aid=10.1371%2Fjournal.pone.0000742&amp;source=plos" target="_blank" title="Add to PubChase"><img src="/images/icon.pc.16.png" width="16" height="16" alt="PubChase">PubChase</a></li>


                <script type="text/javascript">
                    // replace tweet with one that's pre-shortened to 140 chars
                    function truncateTweetText() {
                        var twtTitle = 'Speech and Non-Speech Audio-Visual Illusions: A Developmental Study';
                        var twtUrl = 'http://dx.plos.org/10.1371/journal.pone.0000742';
                        // all URLs posted to twitter get auto-shortened to 20 chars.
                        var maxLength = 140 - (20 + 1);
                        // truncate the title to include space for twtTag and ellipsis (here, 10 = tag length + space + ellipsis)
                        if (twtTitle.length > maxLength) { twtTitle = twtTitle.substr(0, (maxLength - 10)) + '...'; }
                        // set the href to use the shortened tweet
                        $('#twitter-share-link').prop('href', 'http://twitter.com/intent/tweet?text=' + encodeURIComponent('#PLOSONE: ' + twtTitle + ' ' + twtUrl));
                    }
                </script>
                <li><a href="http://twitter.com/intent/tweet?text=#PLOSONE%3A%20Speech%20and%20Non-Speech%20Audio-Visual%20Illusions%3A%20A%20Developmental%20Study http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0000742" onclick="truncateTweetText();" target="_blank" title="Share on Twitter" id="twitter-share-link"><img src="/images/icon.twtr.16.png" width="16" height="16" alt="Twitter">Twitter</a></li>

                <li><a href="/article/email/info%3Adoi%2F10.1371%2Fjournal.pone.0000742" title="Email this article"><img src="/images/icon.email.16.png" width="16" height="16" alt="Email">Email</a></li>
            </ul>
        </div>
    </div><!--end btn-reveal flt-r-->
</div><!-- end article-actions-->

<!-- begin Crossmark -->

<a id="open-crossmark" href="#" style="margin-top: -28px; display:block"><img style="border: 0; display: none;
 padding: 10px 0 18px 0;"  id="crossmark-icon" src="/images/logo-crossmark-bw.png" /></a>
<div id="crossmark-dialog" style="display: none;" title="">
    <!-- the external CrossMark data is loaded inside this iframe -->
    <iframe id="crossmark-dialog-frame" frameborder="0"></iframe>
</div>

<!-- end crossmark -->


<div class="block" id="subject-area-sidebar-block">
    <div class="header">
        <h3>Subject Areas</h3><div title="More information" id="subject-area-sidebar-block-help-icon"><img align="right"
                                                                                                           alt="info" src="/images/button_info.png"/><div id="subject-area-sidebar-block-help"><img align="right"
                                                                                                                                                                                                    src="/images/button_info.png"/><p>
        <b>We want your feedback.</b> Do these subject areas make sense for this article? If not, click the flag
        next to the incorrect subject area and we will review it. Thanks for your help!
    </p></div></div>
    </div>


    <ul id="subject-area-sidebar-list">


















          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Age+groups%22" title="Search for articles in the subject area:'Age groups'"><div class="flagText">Age groups</div></a>
              <div data-categoryid="41041" data-articleid="24322"
                   data-categoryname="Age groups"
                   class="flagImage" title="Flag 'Age groups' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Attention%22" title="Search for articles in the subject area:'Attention'"><div class="flagText">Attention</div></a>
              <div data-categoryid="33353" data-articleid="24322"
                   data-categoryname="Attention"
                   class="flagImage" title="Flag 'Attention' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Children%22" title="Search for articles in the subject area:'Children'"><div class="flagText">Children</div></a>
              <div data-categoryid="40327" data-articleid="24322"
                   data-categoryname="Children"
                   class="flagImage" title="Flag 'Children' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Data+processing%22" title="Search for articles in the subject area:'Data processing'"><div class="flagText">Data processing</div></a>
              <div data-categoryid="18073" data-articleid="24322"
                   data-categoryname="Data processing"
                   class="flagImage" title="Flag 'Data processing' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Human+performance%22" title="Search for articles in the subject area:'Human performance'"><div class="flagText">Human performance</div></a>
              <div data-categoryid="18145" data-articleid="24322"
                   data-categoryname="Human performance"
                   class="flagImage" title="Flag 'Human performance' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Perception%22" title="Search for articles in the subject area:'Perception'"><div class="flagText">Perception</div></a>
              <div data-categoryid="21209" data-articleid="24322"
                   data-categoryname="Perception"
                   class="flagImage" title="Flag 'Perception' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Sensory+perception%22" title="Search for articles in the subject area:'Sensory perception'"><div class="flagText">Sensory perception</div></a>
              <div data-categoryid="46099" data-articleid="24322"
                   data-categoryname="Sensory perception"
                   class="flagImage" title="Flag 'Sensory perception' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Vision%22" title="Search for articles in the subject area:'Vision'"><div class="flagText">Vision</div></a>
              <div data-categoryid="32965" data-articleid="24322"
                   data-categoryname="Vision"
                   class="flagImage" title="Flag 'Vision' as inappropriate"></div>
          </li>
    </ul>
</div>

<div class="ad">
    <div class="title">Advertisement</div>






  <iframe id='a0852f54' name='a0852f54'
    src='http://ads.plos.org/www/delivery/afr.php?zoneid=381&amp;cb=4995'
    frameborder='0' scrolling='no' width='160' height='600'>
    <a href='http://ads.plos.org/www/delivery/ck.php?n=a0852f54&amp;cb=1128'
      target='_top'><img src='http://ads.plos.org/www/delivery/avw.php?zoneid=381&amp;cb=4455&amp;n=a0852f54'
      border='0' alt=''/>
    </a>
  </iframe>



</div>

<div id="twitter-alm-timeline" class="twitter-alm-timeline"></div>


</div><!-- sidebar -->
    </div>
  </div>
</div>
<script src="http://wl.figshare.com/static/p_widget.js" type="text/javascript"></script><div id="pageftr">
  <div class="ftr-cols cf">
    <div class="col col-1">
      <img src="/images/logo-plos-footer.png" alt="PLOS Logo" class="logo" />
      <p><a href="/static/releaseNotes">Ambra 2.9.16</a> Managed Colocation provided <br />by <a href="http://www.isc.org/">Internet Systems Consortium</a>.<p>
      <div class="nav nav-aux">
        <a href="/static/privacy">Privacy Policy</a> |
        <a href="/static/terms">Terms of Use</a> |
        <a href="http://www.plos.org/advertise/">Advertise</a> |
        <a href="http://www.plos.org/about/media-inquiries/">Media Inquiries</a>
      </div>
    </div>
    <div class="col col-2">
      <p><a href="http://www.plos.org/publications/journals/">Publications</a></p>
      <div class="nav">
        <ul>
          <li><a href="http://www.plosbiology.org">PLOS Biology</a></li>
          <li><a href="http://www.plosmedicine.org">PLOS Medicine</a></li>
          <li><a href="http://www.ploscompbiol.org">PLOS Computational Biology</a></li>
          <li><a href="http://currents.plos.org">PLOS Currents</a></li>
          <li><a href="http://www.plosgenetics.org">PLOS Genetics</a></li>
          <li><a href="http://www.plospathogens.org">PLOS Pathogens</a></li>
          <li><a href="http://www.plosone.org">PLOS ONE</a></li>
          <li><a href="http://www.plosntds.org">PLOS Neglected Tropical Diseases</a></li>
        </ul>
      </div>
    </div>
    <div class="col col-3">
      <div class="nav">
        <p><a href="http://www.plos.org">plos.org</a></p>
        <p><a href="http://blogs.plos.org">Blogs</a></p>
        <p><a href="http://www.ploscollections.org">Collections</a></p>
        <p><a href="/feedback/new">Send us feedback</a></p>

        <p>California (US) corporation #C2354500, based in San Francisco</p>
      </div>
    </div>
  </div>
</div><!-- pageftr -->

</div><!-- end page-wrap, this div is in header.ftl -->
<script type="text/javascript" src="/javascript/jquery-1.8.1-min.js?v=Tm7VCOzZz3lE03ghpkS6SWkHbyI"></script>
<script type="text/javascript" src="/javascript/ga-min.js?v=lNQ4gt8QcPDatjsdOFl_FGpPhLY"></script>
<script type="text/javascript" src="/javascript/jquery.hoverIntent-min.js?v=mRiGNYY9cIXxVb8u0K_MdW7hHnc"></script>
<script type="text/javascript" src="/javascript/jquery.placeholder-min.js?v=21Pn56Ur9h1N4K4VZDa0nqI3Pxo"></script>
<script type="text/javascript" src="/javascript/jquery.jsonp-2.4.0-min.js?v=lqTpzoHfSq3I5Ygo01qq5WankEo"></script>
<script type="text/javascript" src="/javascript/jquery-ui-1.9.2.custom-min.js?v=raSSlfNO0YsV5uUpAKmTB9n5VTc"></script>
<script type="text/javascript" src="/javascript/jquery.tooltip-min.js?v=cw+6Smh+mdryIA25xvqIvHMrnZM"></script>
<script type="text/javascript" src="/javascript/jquery.uniform-min.js?v=kYUAnX6W2W_2fK3RIuQ2m_YFG9U"></script>
<script type="text/javascript" src="/javascript/jquery.pjax-min.js?v=939kLBjL5_YKbx71T1RHjYaD4l8"></script>
<script type="text/javascript" src="/javascript/imagesloaded-min.js?v=XeuAp8Gc3mvQUo+wZCSF8ttPwvw"></script>
<script type="text/javascript" src="/javascript/figviewer-min.js?v=yPUa0sUQ_iHkI+IRv2i9bjyZJFo"></script>
<script type="text/javascript" src="/javascript/global-min.js?v=0Q3PwjeaWtXYDnqIsQvnL_ou0qs"></script>
<script type="text/javascript" src="/javascript/jquery.touchswipe-min.js?v=huaek_e6HqTduvCNAN91dJolTyw"></script>
<script type="text/javascript" src="/javascript/jquery.base64-min.js?v=VwV1zeVqKZj5FCAdlK0q5NRxbBg"></script>
<script type="text/javascript" src="/javascript/alm-min.js?v=Y5gm6B0b4Kx2YHNObNrgEeBgXlY"></script>
<script type="text/javascript" src="/javascript/taxonomy-browser-min.js?v=vBVMuDMYkGJCXIUxLe35GoyiJNw"></script>
<script type="text/javascript" src="/javascript/jquery.filterize-min.js?v=j0ZKVnHyk2nhFy8eIuNJkp7xaM0"></script>
<script type="text/javascript" src="/javascript/plosone-min.js?v=TK4H4arL_XBSwwJq+K1N3kqYfAI"></script>
<script type="text/javascript" src="/javascript/twitter-min.js?v=xKgcxLsQFXy+at1ao1NVke8nFlM"></script>
<script type="text/javascript" src="/javascript/crossmark.1.4-min.js?v=3FO4k0SjwTaGNnKGNSqthar1080"></script>
<script type="text/javascript">
  var _sf_async_config={uid:16579,domain:"plosone.org"};
  (function(){
    function loadChartbeat() {
      window._sf_endpt=(new Date()).getTime();
      var e = document.createElement('script');
      e.setAttribute('language', 'javascript');
      e.setAttribute('type', 'text/javascript');
      e.setAttribute('src',
          (("https:" == document.location.protocol) ? "https://a248.e.akamai.net/chartbeat.download.akamai.com/102508/" : "http://static.chartbeat.com/") +
              "js/chartbeat.js");
      document.body.appendChild(e);
    }
    var oldonload = window.onload;
    window.onload = (typeof window.onload != 'function') ?
        loadChartbeat : function() { oldonload(); loadChartbeat(); };
  })();
</script>
<!-- <script type="application/javascript" src="http://crossmark.crossref.org/javascripts/v1.3/crossmark.min.js"></script> -->

</body>
</html>
