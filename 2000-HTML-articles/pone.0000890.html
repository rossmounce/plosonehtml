

 



<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"
      xmlns:foaf="http://xmlns.com/foaf/0.1/"
      xmlns:dc="http://purl.org/dc/terms/"
      xmlns:doi="http://dx.doi.org/"
      xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
      xmlns:xsd="http://www.w3.org/2001/XMLSchema-datatypes#"
      lang="en" xml:lang="en"
      itemscope itemtype="http://schema.org/Article"
      class="no-js">
<head prefix="og: http://ogp.me/ns#">
  <title>PLOS ONE: Cross-Modal Object Recognition Is Viewpoint-Independent</title>


<link rel="stylesheet" type="text/css"  href="/css/global-min.css?v=izteQ6tu7kgsJZW_xmrYizvKiHM" />


    <!--[if lte IE 7]>
<link rel="stylesheet" type="text/css"  href="/css/lte_ie7-min.css?v=3bykQUyQmReeuobVyPozcJ9LxRc" />
    <![endif]-->


<link rel="stylesheet" type="text/css"  href="/css/jquery-ui-min.css?v=eXDHTEJM0lIAmDe5k0I0Ad4nxNo" />


<link rel="stylesheet" type="text/css"  href="/css/journal.css?v=T7ZVxJfgk9jNxLAJ2qHz1vZpgYU" />


<link rel="stylesheet" type="text/css" media="print" href="/css/print-min.css?v=T5lb0B3q6EXBsuDluc5V5w+AkRc" />


  <link rel="stylesheet" href="http://f.fontdeck.com/s/css/js/www.plosone.org/24557.css" type="text/css"/>

  <!--chartbeat -->
  <script type="text/javascript">var _sf_startpt = (new Date()).getTime()</script>
  <script>document.documentElement.className += ' js';</script>

  <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <meta http-equiv="X-UA-Compatible" content="IE=EmulateIE7; IE=EmulateIE9"/>
  <meta name="description" content="PLOS ONE: an inclusive, peer-reviewed, open-access resource from the PUBLIC LIBRARY OF SCIENCE. Reports of well-performed scientific studies from all disciplines freely available to the whole world."/>
  <meta name="keywords" content="PLOS, Public Library of Science, Open Access, Open-Access, Science, Medicine, Biology, Research, Peer-review, Inclusive, Interdisciplinary, Ante-disciplinary, Physics, Chemistry, Engineering"/>
  <meta name="almHost" content="http://alm.plos.org/api/v3/articles"/>
  <meta name="searchHost" content="http://api.plos.org/search" />
  <meta name="termsHost" content="http://api.plos.org/terms" />
  <meta name="solrApiKey" content="plos"/>
  <meta name="almAPIKey" content="3pezRBRXdyzYW6ztfwft" />
  <meta name="currentJournal" content="PLoSONE" />
  <meta name="almRequestBatchSize" content="" />

  <meta name="citation_publisher" content="Public Library of Science"/>
  <meta name="citation_doi" content="10.1371/journal.pone.0000890"/>
  <meta name="dc.identifier" content="10.1371/journal.pone.0000890" />

    <meta name="citation_title" content="Cross-Modal Object Recognition Is Viewpoint-Independent"/>
    <meta itemprop="name" content="Cross-Modal Object Recognition Is Viewpoint-Independent"/>

      <meta name="citation_author" content="Simon Lacey"/>
            <meta name="citation_author_institution" content="Department of Neurology, Emory University, Atlanta, Georgia, United States of America"/>
      <meta name="citation_author" content="Andrew Peters"/>
            <meta name="citation_author_institution" content="Department of Neurology, Emory University, Atlanta, Georgia, United States of America"/>
      <meta name="citation_author" content="K. Sathian"/>
            <meta name="citation_author_institution" content="Department of Neurology, Emory University, Atlanta, Georgia, United States of America"/>
            <meta name="citation_author_institution" content="Department of Rehabilitation Medicine, Emory University, Atlanta, Georgia, United States of America"/>
            <meta name="citation_author_institution" content="Department of Psychology, Emory University, Atlanta, Georgia, United States of America"/>
            <meta name="citation_author_institution" content="Atlanta Veterans Affairs Medical Center, Rehabilitation Research and Development Center of Excellence, Decatur, Georgia, United States of America"/>

    <meta name="citation_date" content="2007/9/12"/>

  <meta name="citation_pdf_url" content="http://dx.plos.org/10.1371/journal.pone.0000890.pdf" />

      <meta name="citation_journal_title" content="PLOS ONE" />
    <meta name="citation_firstpage" content="e890"/>
    <meta name="citation_issue" content="9"/>
    <meta name="citation_volume" content="2"/>
    <meta name="citation_issn" content="1932-6203"/>

    <meta name="citation_journal_abbrev" content="PLoS ONE" />

      <meta name="citation_reference" content="citation_title=The time to name disoriented objects.; citation_author=P Jolicoeur; citation_journal_title=Mem Cognition,; citation_volume=13; citation_number=1; citation_pages=289-303; citation_date=1985; " />
      <meta name="citation_reference" content="citation_title=Viewpoint dependence in visual and haptic object recognition.; citation_author=FN Newell; citation_author=MO Ernst; citation_author=BS Tjan; citation_author=HH Bulthoff; citation_journal_title=Psychol Sci,; citation_volume=12; citation_number=2; citation_pages=37-42; citation_date=2001; " />
      <meta name="citation_reference" content="citation_title=There's more to touch than meets the eye: The salience of object attributes for haptics with and without vision.; citation_author=RL Klatzky; citation_author=S Lederman; citation_author=C Reed; citation_journal_title=J Exp Psychol: Gen,; citation_volume=116; citation_number=3; citation_pages=356-369; citation_date=1987; " />
      <meta name="citation_reference" content="citation_title=Implicit and explicit memory for visual and haptic objects: Cross-modal priming depends on structural descriptions.; citation_author=JM Reales; citation_author=S Ballesteros; citation_journal_title=J Exp Psychol: Learn,; citation_volume=25; citation_number=4; citation_pages=644-663; citation_date=1999; " />
      <meta name="citation_reference" content="citation_title=Tangible pictures: Viewpoint effects and linear perspective in visually impaired people.; citation_author=MA Heller; citation_author=DD Brackett; citation_author=E Scroggs; citation_author=H Steffen; citation_author=K Heatherly; citation_journal_title=Perception,; citation_volume=31; citation_number=5; citation_pages=747-769; citation_date=2002; " />
      <meta name="citation_reference" content="citation_title=BOLD activity during mental rotation and viewpoint-dependent object recognition.; citation_author=I Gauthier; citation_author=WG Hayward; citation_author=MJ Tarr; citation_author=AW Anderson; citation_author=P Skudlarski; citation_journal_title=Neuron,; citation_volume=34; citation_number=6; citation_pages=161-171; citation_date=2002; " />
      <meta name="citation_reference" content="citation_title=Object-spatial imagery: A new self-report questionnaire.; citation_author=O Blajenkova; citation_author=M Kozhevnikov; citation_author=MA Motes; citation_journal_title=Appl Cognit Psychol,; citation_volume=20; citation_number=7; citation_pages=239-263; citation_date=2006; " />
      <meta name="citation_reference" content="citation_title=Spatial versus object visualizers: A new characterization of cognitive style.; citation_author=M Kozhevnikov; citation_author=S Kosslyn; citation_author=J Shephard; citation_journal_title=Mem Cognition,; citation_volume=33; citation_number=8; citation_pages=710-726; citation_date=2005; " />
      <meta name="citation_reference" content="citation_title=Mental representation in visual/haptic crossmodal memory: Evidence from interference effects.; citation_author=S Lacey; citation_author=C Campbell; citation_journal_title=Q J Exp Psychol,; citation_volume=59; citation_number=9; citation_pages=361-376; citation_date=2006; " />
      <meta name="citation_reference" content="citation_title=Human information processing and sensory modality: Cross-modal functions, information complexity, memory, and deficit.; citation_author=D Freides; citation_journal_title=Psychol Bull,; citation_volume=8; citation_number=10; citation_pages=284-310; citation_date=1974; " />
      <meta name="citation_reference" content="citation_title=The role of long-term and short-term familiarity in visual and haptic face recognition.; citation_author=SJ Casey; citation_author=FN Newell; citation_journal_title=Exp Brain Res,; citation_volume=166; citation_number=11; citation_pages=583-591; citation_date=2005; " />
      <meta name="citation_reference" content="citation_title=Visual, haptic and crossmodal recognition of scenes.; citation_author=FN Newell; citation_author=AT Woods; citation_author=M Mernagh; citation_author=HH Bulthoff; citation_journal_title=Exp Brain Res,; citation_volume=161; citation_number=12; citation_pages=233-242; citation_date=2005; " />
      <meta name="citation_reference" content="citation_title=The visual and haptic perception of natural object shape.; citation_author=JF Norman; citation_author=HF Norman; citation_author=AM Clayton; citation_author=J Lianekhammy; citation_author=G Zielke; citation_journal_title=Percept Psychophys,; citation_volume=66; citation_number=13; citation_pages=342-351; citation_date=2004; " />
      <meta name="citation_reference" content="citation_title=Children's haptic and cross-modal recognition with familiar and unfamiliar objects.; citation_author=EW Bushnell; citation_author=C Baxt; citation_journal_title=J Exp Psychol Human,; citation_volume=25; citation_number=14; citation_pages=1867-1881; citation_date=1999; " />
      <meta name="citation_reference" content="citation_title=Coordinating visual and kinaesthetic memory codes.; citation_author=KM Newell; citation_author=DC Shapiro; citation_author=MJ Carlton; citation_journal_title=Brit J Psychol,; citation_volume=70; citation_number=15; citation_pages=87-96; citation_date=1979; " />
      <meta name="citation_reference" content="citation_title=Hierarchical models of object recognition in cortex.; citation_author=M Riesenhuber; citation_author=T Poggio; citation_journal_title=Nature Neurosci,; citation_volume=2; citation_number=16; citation_pages=1019-1025; citation_date=1999; " />
      <meta name="citation_reference" content="citation_title=Is human object recognition better described by geon structural descriptions or by multiple views: Comment on Biederman and Gerhardstein (1993).; citation_author=MJ Tarr; citation_author=HH Bulthoff; citation_journal_title=J Exp Psychol Human,; citation_volume=21; citation_number=17; citation_pages=1494-1505; citation_date=1995; " />
      <meta name="citation_reference" content="citation_title=Recognition-by-components: A theory of human image understanding.; citation_author=I Biederman; citation_journal_title=Psychol Rev,; citation_volume=94; citation_number=18; citation_pages=115-147; citation_date=1987; " />
      <meta name="citation_reference" content="citation_title=Visuo-haptic object-related activation in the ventral pathway.; citation_author=A Amedi; citation_author=R Malach; citation_author=T Hendler; citation_author=S Peled; citation_author=E Zohary; citation_journal_title=Nature Neurosci,; citation_volume=4; citation_number=19; citation_pages=324-330; citation_date=2001; " />
      <meta name="citation_reference" content="citation_title=Haptic study of three-dimensional objects activates extrastriate visual areas.; citation_author=TW James; citation_author=GK Humphrey; citation_author=JS Gati; citation_author=P Servos; citation_author=RS Menon; citation_journal_title=Neuropsychologia,; citation_volume=40; citation_number=20; citation_pages=1706-1714; citation_date=2002; " />
      <meta name="citation_reference" content="citation_title=Multisensory cortical processing of shape and its relation to mental imagery.; citation_author=M Zhang; citation_author=VD Weisser; citation_author=R Stilla; citation_author=SC Prather; citation_author=K Sathian; citation_journal_title=Cogn Affect Behav Ne,; citation_volume=4; citation_number=21; citation_pages=251-259; citation_date=2004; " />
      <meta name="citation_reference" content="citation_title=Activity and effective connectivity of parietal and occipital cortical regions during haptic shape perception.; citation_author=S Peltier; citation_author=R Stilla; citation_author=E Mariola; citation_author=S LaConte; citation_author=X Hu; citation_journal_title=Neuropsychologia,; citation_volume=45; citation_number=22; citation_pages=476-483; citation_date=2007; " />
      <meta name="citation_reference" content="citation_title=Differential effects of viewpoint on object-driven activation in dorsal and ventral streams.; citation_author=TW James; citation_author=GK Humphrey; citation_author=JS Gati; citation_author=RS Menon; citation_author=MA Goodale; citation_journal_title=Neuron,; citation_volume=35; citation_number=23; citation_pages=793-801; citation_date=2002; " />
      <meta name="citation_reference" content="citation_title=Differential processing of objects under various viewing conditions in the human lateral occipital complex.; citation_author=K Grill-Spector; citation_author=T Kushnir; citation_author=S Edelman; citation_author=G Avidan; citation_author=Y Itzchak; citation_journal_title=Neuron,; citation_volume=24; citation_number=24; citation_pages=187-203; citation_date=1999; " />
      <meta name="citation_reference" content="citation_title=Visual and haptic representations of scenes are updated with observer movement.; citation_author=A Pasqualotto; citation_author=C Finucane; citation_author=FN Newell; citation_journal_title=Exp Brain Res,; citation_volume=166; citation_number=25; citation_pages=481-488; citation_date=2005; " />
      <meta name="citation_reference" content="citation_title=Shape representation in the inferior temporal cortex of monkeys.; citation_author=NK Logothetis; citation_author=J Pauls; citation_author=T Poggio; citation_journal_title=Curr Biol,; citation_volume=5; citation_number=26; citation_pages=552-563; citation_date=1995; " />
      <meta name="citation_reference" content="citation_title=Visual cells in the temporal cortex sensitive to face view and gaze direction.; citation_author=DI Perrett; citation_author=PAJ Smith; citation_author=DD Potter; citation_author=AJ Mistlin; citation_author=AS Head; citation_journal_title=Proc R Soc Lond B,; citation_volume=223; citation_number=27; citation_pages=293-317; citation_date=1985; " />

  <link rel="canonical" href="http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0000890" />

    <meta name="twitter:card" content="summary"/>
    <meta name="twitter:site" content="@plosone"/>
    <meta name="twitter:title" content="Cross-Modal Object Recognition Is Viewpoint-Independent"/>
    <meta name="twitter:description" content="BackgroundPrevious research suggests that visual and haptic object recognition are viewpoint-dependent both within- and cross-modally. However, this conclusion may not be generally valid as it was reached using objects oriented along their extended y-axis, resulting in differential surface processing in vision and touch. In the present study, we removed this differential by presenting objects along the z-axis, thus making all object surfaces more equally available to vision and touch.Methodology/Principal FindingsParticipants studied previously unfamiliar objects, in groups of four, using either vision or touch. Subsequently, they performed a four-alternative forced-choice object identification task with the studied objects presented in both unrotated and rotated (180&deg; about the x-, y-, and z-axes) orientations. Rotation impaired within-modal recognition accuracy in both vision and touch, but not cross-modal recognition accuracy. Within-modally, visual recognition accuracy was reduced by rotation about the x- and y-axes more than the z-axis, whilst haptic recognition was equally affected by rotation about all three axes. Cross-modal (but not within-modal) accuracy correlated with spatial (but not object) imagery scores.Conclusions/SignificanceThe viewpoint-independence of cross-modal object identification points to its mediation by a high-level abstract representation. The correlation between spatial imagery scores and cross-modal performance suggest that construction of this high-level representation is linked to the ability to perform spatial transformations. Within-modal viewpoint-dependence appears to have a different basis in vision than in touch, possibly due to surface occlusion being important in vision but not touch."/>
      <meta name="twitter:image" content="http://dx.plos.org/10.1371/journal.pone.0000890.g005"/>

  <meta property="og:title" content="Cross-Modal Object Recognition Is Viewpoint-Independent" />
  <meta property="og:type" content="article" />
  <meta property="og:url" content="http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0000890" />

 <!--end articleInfoX-->

  <link rel="pingback" href="http://www.plosone.org/pingback" />


  <link rel="shortcut icon" href="/images/favicon.ico" type="image/x-icon"/>
  <link rel="home" title="home" href="/"/>
  <link rel="alternate" type="application/rss+xml"
        title="PLOS ONE: New Articles"
        href="http://www.plosone.org/article/feed"/>
</head>
<body>

  <div id="page-wrap">
    <div id="topbanner" class="cf">

<!-- Div for the ad at the top of journal home page-->
<div class="center">
  <div class="title">Advertisement</div>
  <iframe id='a3ac9da4' name='a3ac9da4'
    src='http://ads.plos.org/www/delivery/afr.php?zoneid=345&amp;cb=6745'
    frameborder='0' scrolling='no' width='730' height='90'>
    <a href='http://ads.plos.org/www/delivery/ck.php?n=a3ac9da4&amp;cb=8807'
      target='_top'><img src='http://ads.plos.org/www/delivery/avw.php?zoneid=345&amp;cb=7521&amp;n=a3ac9da4'
      border='0' alt=''/>
    </a>
  </iframe>
</div>    </div>

    <div id="pagehdr-wrap">
      <div id="pagehdr">
        <div id="user" class="nav">
          <ul>
            <li><a href="http://www.plos.org">plos.org</a></li>
            <li><a href="https://register.plos.org/ambra-registration/register.action">create account</a></li>
            <li class="btn-style"><a
              href="/user/secure/secureRedirect.action?goTo=%2Farticle%2FfetchArticle.action%3FarticleURI%3Dinfo%253Adoi%252F10.1371%252Fjournal.pone.0000890">sign in</a>
            </li>
          </ul>
        </div>
        <div class="logo">
          <a href="/"><img src="/images/logo.png" alt="PLOS ONE"></a>
        </div>

<div id="nav-main" class="nav">
  <ul>
        <li id="mn-01"><a href="/taxonomy" class="areas-link">Subject Areas</a></li>
    <li id="mn-02"><a href="javascript:void(0);">For Authors</a>
      <div class="submenu" style="width: 540px; margin-left: -300px;">
        <div class="block">
          <div class="submit-script">
            <h3>Submit your Manuscript</h3>
            <ul>
              <li>Fair, rigorous peer review</li>
              <li>Broad scope and wide reach</li>
            </ul>
            <a href="/static/submissionInstructions" class="btn">get started</a>
          </div>
        </div>
        <div class="menu">
          <ul>
            <li><a href="/static/publish">Why Publish with PLOS ONE</a></li>
            <li><a href="/static/publication">Publication Criteria</a></li>
            <li><a href="/static/editorial">Editorial Policies</a></li>
            <li><a href="/static/guidelines">Preparing A Manuscript</a></li>
            <li><a href="/static/figureGuidelines">Figure and Table Guidelines</a></li>
          <li><a href="/static/supportingInformation">Supporting Information Guidelines</a></li>
            <li><a href="/static/submissionInstructions">Submitting a Manuscript</a></li>
          </ul>
        </div>
      </div>
    </li>

    <li id="mn-03"><a href="javascript:void(0);">About Us</a>
      <div class="submenu" style="width:248px; margin-left:-30px;">
        <div class="menu">
          <ul>
            <li><a href="/static/information">Journal Information</a></li>
            <li><a href="/static/edboard">Editorial Board</a></li>
            <li><a href="/static/reviewerGuidelines">Reviewer Guidelines</a></li>
            <li><a href="/static/almInfo">Article-Level Metrics</a></li>
            <li><a href="/static/license">Open-Access License</a></li>
            <li><a href="/static/downloads">Media Downloads</a></li>
            <li><a href="/static/commentGuidelines">Guidelines for Comments</a></li>
            <li><a href="/static/corrections">Corrections</a></li>
            <li><a href="/static/help">Help Using this Site</a></li>
            <li><a href="/static/contact">Contact Us</a></li>
          </ul>
        </div>
      </div>
    </li>
  </ul>
<div id="db">
  <form name="searchForm" action="/search/simple?noSearchFlag=true&amp;query=&amp;articleURI=info%3Adoi%2F10.1371%2Fjournal.pone.0000890" method="get" >
<input type="hidden" name="from" value="globalSimpleSearch" id="from"/><input type="hidden" name="filterJournals" value="PLoSONE" id="filterJournals"/>    <fieldset>
      <legend>Search</legend>
      <label for="search">Search</label>
      <div class="wrap">
        <input id="search" type="text" name="query" placeholder="Search">
        <input type="image" alt="SEARCH" src="/images/icon.search.gif">
      </div>
    </fieldset>
  </form>
    <a id="advSearch" href="/search/advanced?noSearchFlag=true&amp;query=&amp;articleURI=info%3Adoi%2F10.1371%2Fjournal.pone.0000890&filterJournals=PLoSONE">advanced search</a>
</div></div>

      </div>
      <!-- pagehdr-->
    </div>
    <!-- pagehdr-wrap -->

  <!--body and html tags gets closed in global_footer.ftl-->
<script type="text/javascript" src="/javascript/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<div id="pagebdy-wrap">
  <div id="pagebdy">

    <div id="article-block" class="cf">

<div class="article-meta cf">
  <ul id="almSignPost" style="display: none;"></ul>
  <div class="article-type">
    <span class="type oa">Open Access</span>
      <span class="type pr">Peer-Reviewed</span>
  </div>
</div>

<div class="header" id="hdr-article">

<div class="article-kicker">
      <span id="article-type-heading">
        Research Article
      </span>
</div>  <h1 property="dc:title" datatype="" rel="dc:type" href="http://purl.org/dc/dcmitype/Text">
    Cross-Modal Object Recognition Is Viewpoint-Independent
  </h1>

  <ul class="authors">
      <li>


        <span rel="dc:creator" class="author">
          <span class="person" property="foaf:name" typeof="foaf:Person">
            Simon Lacey, 
          </span>
        </span>

          <div class="author_meta">
            <div class="author_inner">


              
              

                <p>Affiliation:
                  Department of Neurology, Emory University, Atlanta, Georgia, United States of America
                </p>


              <span class="close">X</span>

            </div>
          </div>
      </li>
      <li>


        <span rel="dc:creator" class="author">
          <span class="person" property="foaf:name" typeof="foaf:Person">
            Andrew Peters, 
          </span>
        </span>

          <div class="author_meta">
            <div class="author_inner">


              
              

                <p>Affiliation:
                  Department of Neurology, Emory University, Atlanta, Georgia, United States of America
                </p>


              <span class="close">X</span>

            </div>
          </div>
      </li>
      <li>


        <span rel="dc:creator" class="author">
          <span class="person" property="foaf:name" typeof="foaf:Person">
            K. Sathian
              <span class="corresponding">mail</span>
          </span>
        </span>

          <div class="author_meta">
            <div class="author_inner">


              
              <p><span class="email">*</span>To whom correspondence should be addressed. E-mail: <a href="mailto:krish.sathian@emory.edu">krish.sathian@emory.edu</a></p>

                <p>Affiliations:
                  Department of Neurology, Emory University, Atlanta, Georgia, United States of America, 
                  Department of Rehabilitation Medicine, Emory University, Atlanta, Georgia, United States of America, 
                  Department of Psychology, Emory University, Atlanta, Georgia, United States of America, 
                  Atlanta Veterans Affairs Medical Center, Rehabilitation Research and Development Center of Excellence, Decatur, Georgia, United States of America
                </p>


              <span class="close">X</span>

            </div>
          </div>
      </li>
  </ul>
  <ul class="date-doi-line">
    <li>Published: September 12, 2007</li>
    <li>DOI: 10.1371/journal.pone.0000890</li>
  </ul>


</div><!--end header-->
<div class="main cf" id="pjax-container">
  

<div class="nav items-5" id="nav-article">
  <ul>
  <li>
        <span class="active" name="article">Article</span>
  </li>
  <li>
      <a href="/article/authors/info%3Adoi%2F10.1371%2Fjournal.pone.0000890" name="authors">About the Authors</a>
  </li>
  <li>
      <a href="/article/metrics/info%3Adoi%2F10.1371%2Fjournal.pone.0000890" name="metrics">Metrics</a>
  </li>
  <li>
      <a href="/article/comments/info%3Adoi%2F10.1371%2Fjournal.pone.0000890" name="comments">Comments</a>
  </li>
  <li>
      <a href="/article/related/info%3Adoi%2F10.1371%2Fjournal.pone.0000890" name="related">Related Content</a>
  </li>
  </ul>
</div>

<script type="text/javascript">
  var selected_tab = "article";
</script>
  <div id="figure-thmbs" class="carousel cf">
    <div class="wrapper">
      <div class="slider">
              <div class="item">
                <a href="#pone-0000890-g001" data-doi="info:doi/10.1371/journal.pone.0000890" data-uri="info:doi/10.1371/journal.pone.0000890.g001" title="Figure 1">
                  <span class="thmb-wrap">
                    <img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000890.g001&representation=PNG_I" alt="">
                  </span>
                </a>
              </div>
              <div class="item">
                <a href="#pone-0000890-g002" data-doi="info:doi/10.1371/journal.pone.0000890" data-uri="info:doi/10.1371/journal.pone.0000890.g002" title="Figure 2">
                  <span class="thmb-wrap">
                    <img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000890.g002&representation=PNG_I" alt="">
                  </span>
                </a>
              </div>
              <div class="item">
                <a href="#pone-0000890-g003" data-doi="info:doi/10.1371/journal.pone.0000890" data-uri="info:doi/10.1371/journal.pone.0000890.g003" title="Figure 3">
                  <span class="thmb-wrap">
                    <img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000890.g003&representation=PNG_I" alt="">
                  </span>
                </a>
              </div>
              <div class="item">
                <a href="#pone-0000890-g004" data-doi="info:doi/10.1371/journal.pone.0000890" data-uri="info:doi/10.1371/journal.pone.0000890.g004" title="Figure 4">
                  <span class="thmb-wrap">
                    <img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000890.g004&representation=PNG_I" alt="">
                  </span>
                </a>
              </div>
              <div class="item">
                <a href="#pone-0000890-g005" data-doi="info:doi/10.1371/journal.pone.0000890" data-uri="info:doi/10.1371/journal.pone.0000890.g005" title="Figure 5">
                  <span class="thmb-wrap">
                    <img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000890.g005&representation=PNG_I" alt="">
                  </span>
                </a>
              </div>
      </div>
    </div>
  </div>

  <div class="nav-col">
    <div class="nav" id="nav-article-page">
      <ul>
        <li class="nav-col-comments"><a href="/article/comments/info%3Adoi%2F10.1371%2Fjournal.pone.0000890">Reader Comments (0)</a></li>
          <li id="nav-figures"><a data-doi="info:doi/10.1371/journal.pone.0000890" >Figures</a></li>
      </ul>
    </div>
  </div>

  <div class="article">







<div class="abstract"><a id="abstract0" name="abstract0" toc="abstract0" title="Abstract"></a><h2>Abstract</h2>
<h3>Background</h3>
<a id="article1.front1.article-meta1.abstract1.sec1.p1" name="article1.front1.article-meta1.abstract1.sec1.p1"></a><p>Previous research suggests that visual and haptic object recognition are viewpoint-dependent both within- and cross-modally. However, this conclusion may not be generally valid as it was reached using objects oriented along their extended y-axis, resulting in differential surface processing in vision and touch. In the present study, we removed this differential by presenting objects along the z-axis, thus making all object surfaces more equally available to vision and touch.</p>


<h3>Methodology/Principal Findings</h3>
<a id="article1.front1.article-meta1.abstract1.sec2.p1" name="article1.front1.article-meta1.abstract1.sec2.p1"></a><p>Participants studied previously unfamiliar objects, in groups of four, using either vision or touch. Subsequently, they performed a four-alternative forced-choice object identification task with the studied objects presented in both unrotated and rotated (180° about the x-, y-, and z-axes) orientations. Rotation impaired within-modal recognition accuracy in both vision and touch, but not cross-modal recognition accuracy. Within-modally, visual recognition accuracy was reduced by rotation about the x- and y-axes more than the z-axis, whilst haptic recognition was equally affected by rotation about all three axes. Cross-modal (but not within-modal) accuracy correlated with spatial (but not object) imagery scores.</p>


<h3>Conclusions/Significance</h3>
<a id="article1.front1.article-meta1.abstract1.sec3.p1" name="article1.front1.article-meta1.abstract1.sec3.p1"></a><p>The viewpoint-independence of cross-modal object identification points to its mediation by a high-level abstract representation. The correlation between spatial imagery scores and cross-modal performance suggest that construction of this high-level representation is linked to the ability to perform spatial transformations. Within-modal viewpoint-dependence appears to have a different basis in vision than in touch, possibly due to surface occlusion being important in vision but not touch.</p>

</div>


<div class="articleinfo"><p><strong>Citation: </strong>Lacey S, Peters A, Sathian K (2007) Cross-Modal Object Recognition Is Viewpoint-Independent. PLoS ONE 2(9):
          e890.
            doi:10.1371/journal.pone.0000890</p><p><strong>Academic Editor: </strong>Justin Harris, University of Sydney, Australia</p><p><strong>Received:</strong> June 18, 2007; <strong>Accepted:</strong> August 24, 2007; <strong>Published:</strong> September 12, 2007</p><p><strong>Copyright:</strong> © 2007 Lacey et al. This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p><p><strong>Funding: </strong>This work was supported by grants to KS from the National Eye Institute at NIH (R01 EY012440 and K24 EY017332) and the National Science Foundation (BCS 0519417). Support to KS by the Veterans Administration is also gratefully acknowledged.</p><p><strong>Competing interests:</strong> The authors have declared that no competing interests exist.</p></div>





<div id="section1" class="section"><a id="s1" name="s1" toc="s1" title="Introduction"></a><h3>Introduction</h3><a id="article1.body1.sec1.p1" name="article1.body1.sec1.p1"></a><p>Previous research suggests that object recognition is viewpoint-dependent within both the visual <a href="#pone.0000890-Jolicoeur1">[1]</a> and haptic <a href="#pone.0000890-Newell1">[2]</a> modalities, since recognition accuracy is degraded if objects are rotated between encoding and test presentations. However, what happens for visuo-haptic <strong>cross-modal</strong> object recognition is less clear, since differences in the perceptual salience of particular object properties between vision and touch suggest qualitatively different unisensory representations <a href="#pone.0000890-Klatzky1">[3]</a>, whereas cross-modal priming studies suggest a common representation <a href="#pone.0000890-Reales1">[4]</a>. <em>A priori</em>, one would expect that when touch is involved, representations should be viewpoint-independent because the hands can move freely over the object, collecting information from all surfaces. However, cross-modal recognition was reported to be viewpoint-dependent, improving when objects with an elongated vertical (y-) axis were rotated away from the learned view about the x- and y-axes, and degrading when rotated about the z-axis <a href="#pone.0000890-Newell1">[2]</a>. The explanation suggested for these findings was that haptic exploration naturally favors the far surface of objects, and vision, the near surface <a href="#pone.0000890-Newell1">[2]</a>. When objects are rotated about the x- and y-axes, the near and far surfaces are exchanged, the haptic far surface becoming the visual near surface. In contrast, rotation about the z-axis does not involve such a surface exchange. But the haptic preference for the far surface may only be true for objects extended along the y-axis: encoding the near surface of these objects haptically is difficult, given the biomechanical constraints of the hand <a href="#pone.0000890-Newell1">[2]</a>, <a href="#pone.0000890-Heller1">[5]</a>. If this is true, the observed cross-modal effects might simply reflect the particular experimental design. Here we used multi-part objects extended along the z-axis (<a href="#pone-0000890-g001">Figure 1</a>): this removed the near/far asymmetry since these surfaces were identical facets, making all object surfaces that carried shape information more equally available to haptic exploration. We reasoned that this would allow a truer understanding of the effect of object rotation on cross-modal recognition.</p>
<div class="figure" id="pone-0000890-g001"><div class="img"><a name="pone-0000890-g001" title="Click for larger image " href="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000890.g001&amp;representation=PNG_M" data-doi="info:doi/10.1371/journal.pone.0000890" data-uri="info:doi/10.1371/journal.pone.0000890.g001"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000890.g001&amp;representation=PNG_I" alt="thumbnail" class="thumbnail"></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000890.g001/powerpoint">
                    PPT
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000890.g001/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000890.g001/largerimage">
                    PNG
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000890.g001/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pone.0000890.g001.PNG_L"></span>)
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000890.g001/originalimage">
                    TIFF
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000890.g001/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pone.0000890.g001.TIF"></span>)
                </a></li></ul></div><p><strong>Figure 1.  <span>An example object used in the present study in the original orientation (A) and rotated 180° about the z-axis (B), x-axis (C) and y-axis (D).</span></strong></p><span>doi:10.1371/journal.pone.0000890.g001</span></div><a id="article1.body1.sec1.p2" name="article1.body1.sec1.p2"></a><p>Recognition of rotated objects involves complex mental spatial transformations. In visual within-modal object recognition, mental rotation and recognition of rotated objects have behaviorally similar signatures (in both, errors and latencies increase with angle of rotation) but rely on different neural networks <a href="#pone.0000890-Gauthier1">[6]</a>. The relationships between the spatial transformations underlying mental rotation and cross-modal recognition of rotated objects are unclear. As a preliminary step to exploring these relationships further, participants completed the Object-Spatial Imagery Questionnaire (OSIQ) <a href="#pone.0000890-Blajenkova1">[7]</a> which measures individual preference for both ‘object imagery’ (pictorial object representations primarily concerned with the visual appearance of an object) and ‘spatial imagery’ (abstract spatial representations primarily concerned with the spatial relations between objects, object parts, and complex spatial transformations) <a href="#pone.0000890-Blajenkova1">[7]</a>, <a href="#pone.0000890-Kozhevnikov1">[8]</a>. We predicted that performance with our multi-part objects would correlate with the spatial imagery ability reflected in OSIQ-spatial scores, but not with the pictorial imagery ability indexed by OSIQ-object scores.</p>
</div>

<div id="section2" class="section"><a id="s2" name="s2" toc="s2" title="Materials and Methods"></a><h3>Materials and Methods</h3><a id="article1.body1.sec2.p1" name="article1.body1.sec2.p1"></a><p>Forty-eight objects were constructed, each made from six smooth wooden blocks measuring 1.6 cm high, 3.6 cm long and 2.2 cm wide. The resulting objects were 9.5 cm high, the other dimensions varying according to the arrangement of the component blocks. Constructing the objects from smooth wooden component blocks avoided the textural difference between the top and bottom surfaces of Lego™ bricks used by Newell et al. <a href="#pone.0000890-Newell1">[2]</a>. This was important to obviate undesirable cues to rotation around the x- and y-axes. The objects were painted medium grey to remove visual cues from variations in the natural wood color and grain. Each object had a small (&lt;1 mm) grey pencil dot on one facet that was used to guide presentation of the object by the experimenter to the participant in a particular orientation. Pilot testing showed that participants were never aware of these small dots and debriefing confirmed that this was so in the main experiment also.</p>
<a id="article1.body1.sec2.p2" name="article1.body1.sec2.p2"></a><p>The 48 objects were divided into three sets of sixteen, one for each axis of rotation. Each set was further divided into four subsets of four, with one subset for each modality condition. These subsets were checked to ensure that they contained no ‘mirror-image’ pairs. Difference matrices were calculated for the twelve subsets based on the number of differences in the position (three possibilities: in the middle or at either end of the preceding block along the z-axis) and orientation (two possibilities: either the same as, or orthogonal to, the preceding block along the z-axis) of each component block. These values could range from 0 (identical) to 6 (completely different) and were used to calculate the mean difference between objects. The mean difference between objects within a subset ranged from 5.2 to 5.7; the mean of these subset scores within a set was taken as the score for the set and these ranged from 5.4 to 5.5. Paired t-tests on these scores showed no significant differences between subsets or sets (all p values &gt;.05) and the objects were therefore considered equally discriminable.</p>
<a id="article1.body1.sec2.p3" name="article1.body1.sec2.p3"></a><p>The procedures were approved by the Institutional Review Board of Emory University. Twenty-four undergraduates (12 male and 12 female, mean age 20 years 3 months) participated after giving informed written consent. Participants performed a four-alternative forced-choice object identification task in two within-modal (visual-visual; haptic-haptic) and two cross-modal (visual-haptic; haptic-visual) conditions. Objects were either unrotated between encoding and test presentations, or rotated by 180° about the x-, y-, and z-axes (<a href="#pone-0000890-g001">Figure 1</a>). In each encoding-recognition sequence, participants learned four objects, identified by numbers, either visually or haptically. Each object was presented for 30 seconds haptically or 15 seconds visually; these times were determined by a pilot experiment. The 2:1 haptic:visual ratio of presentation times reflects that used in previous studies <a href="#pone.0000890-Newell1">[2]</a>, <a href="#pone.0000890-Lacey1">[9]</a>, <a href="#pone.0000890-Freides1">[10]</a>. During visual presentation, participants sat at a table on which the objects were placed. The table was 86 cm high so that the initial viewing distance was 30–40 cm and the initial viewing angle as the participants looked down on the objects was approximately 35–45°. As in the earlier study of Newell et al. <a href="#pone.0000890-Newell1">[2]</a>, the seated participants were free to move their head and eyes when looking at the objects but were not allowed to get up and walk around them.</p>
<a id="article1.body1.sec2.p4" name="article1.body1.sec2.p4"></a><p>During haptic presentation, participants felt the objects behind an opaque cloth screen and were free to move their hands around the objects. Unlike the study of Newell et al. <a href="#pone.0000890-Newell1">[2]</a>, the objects were not fixed to a surface but placed in the participants' hands: participants were instructed to keep the objects in exactly the same orientation as presented and not to rotate or otherwise manipulate them. On subsequent recognition trials, the four objects were presented both unrotated and rotated by 180°, about a specific axis from the initial orientation, providing blocks of eight trials. Participants were asked to identify each object by its number. Objects were rotated about each axis in turn, all the modality conditions being completed for a given axis before moving on to the next axis of rotation. The order of the modality conditions, axes of rotation and object sets was fully counterbalanced across subjects.</p>
</div>

<div id="section3" class="section"><a id="s3" name="s3" toc="s3" title="Results"></a><h3>Results</h3><a id="article1.body1.sec3.p1" name="article1.body1.sec3.p1"></a><p><a href="#pone-0000890-g002">Figure 2</a> shows that object rotation substantially degraded recognition accuracy in the within-modal conditions, but only slightly decreased cross-modal recognition accuracy. A two-way (within- vs. cross-modal, unrotated vs. rotated) repeated-measures analysis of variance (RM-ANOVA) showed that object rotation significantly reduced recognition accuracy (F<sub>1,23</sub> = 30.04, p = &lt;.001) and that overall within-modal recognition accuracy was marginally better than overall cross-modal recognition (F<sub>1,23</sub> = 4.23, p = .051). These two factors interacted (F<sub>1,23</sub> = 12.58, p = .002) and post-hoc t-tests showed that this was because within-modal recognition accuracy was highly significantly reduced by rotation (t = 7.25, p &lt;.001) while cross-modal recognition accuracy was not (t = 1.66, p = .11) (<a href="#pone-0000890-g002">Figure 2</a>).</p>
<div class="figure" id="pone-0000890-g002"><div class="img"><a name="pone-0000890-g002" title="Click for larger image " href="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000890.g002&amp;representation=PNG_M" data-doi="info:doi/10.1371/journal.pone.0000890" data-uri="info:doi/10.1371/journal.pone.0000890.g002"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000890.g002&amp;representation=PNG_I" alt="thumbnail" class="thumbnail"></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000890.g002/powerpoint">
                    PPT
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000890.g002/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000890.g002/largerimage">
                    PNG
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000890.g002/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pone.0000890.g002.PNG_L"></span>)
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000890.g002/originalimage">
                    TIFF
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000890.g002/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pone.0000890.g002.TIF"></span>)
                </a></li></ul></div><p><strong>Figure 2.  <span>The effect on recognition accuracy of rotating objects away from the learned orientation was confined to the within-modal conditions, with no effect in the cross-modal conditions.</span></strong></p><a id="article1.body1.sec3.fig1.caption1.p1" name="article1.body1.sec3.fig1.caption1.p1"></a><p>(Error bars = s.e.m.; asterisk = significant difference; horizontal line = chance performance at 25% in the four-alternative forced-choice task used).</p>
<span>doi:10.1371/journal.pone.0000890.g002</span></div><a id="article1.body1.sec3.p2" name="article1.body1.sec3.p2"></a><p>Analyzing this further, a three-way (modality: within-modal visual, within-modal haptic, cross-modal visual-haptic and cross-modal haptic-visual; rotation; axis) RM-ANOVA again showed a main effect of object rotation (F<sub>1,23</sub> = 30.04, p = .001) but the axis of rotation was unimportant (F<sub>2,46</sub> = .39, p = .68), and the main effect of modality fell short of significance (F<sub>3,69</sub> = 2.49, p = .07). However, modality and rotation again interacted (F<sub>2,46</sub> = 4.82, p = .004). Three-way (separate within- and cross-modal, rotation, axis) RM-ANOVAs showed again that this was because rotation had an effect in the within-modal conditions (F<sub>1,23</sub> = 52.57, p &lt;.001) but not the cross-modal conditions (F<sub>1,23</sub> = 2.74, p = .11). There were no other significant effects or interactions in the cross-modal conditions. <a href="#pone-0000890-g003">Figure 3</a> illustrates that the two within-modal conditions were similar to each other, as were the two cross-modal conditions.</p>
<div class="figure" id="pone-0000890-g003"><div class="img"><a name="pone-0000890-g003" title="Click for larger image " href="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000890.g003&amp;representation=PNG_M" data-doi="info:doi/10.1371/journal.pone.0000890" data-uri="info:doi/10.1371/journal.pone.0000890.g003"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000890.g003&amp;representation=PNG_I" alt="thumbnail" class="thumbnail"></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000890.g003/powerpoint">
                    PPT
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000890.g003/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000890.g003/largerimage">
                    PNG
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000890.g003/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pone.0000890.g003.PNG_L"></span>)
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000890.g003/originalimage">
                    TIFF
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000890.g003/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pone.0000890.g003.TIF"></span>)
                </a></li></ul></div><p><strong>Figure 3.  <span>Interaction between modality and rotation.</span></strong></p><a id="article1.body1.sec3.fig2.caption1.p1" name="article1.body1.sec3.fig2.caption1.p1"></a><p>Rotation away from the learned orientation only affected within-modal, not cross-modal, recognition accuracy. (Error bars = s.e.m.; asterisk = significant difference; horizontal line = chance performance at 25% in the four-alternative forced-choice task used).</p>
<span>doi:10.1371/journal.pone.0000890.g003</span></div><a id="article1.body1.sec3.p3" name="article1.body1.sec3.p3"></a><p>In the within-modal conditions, visual and haptic recognition were not significantly different (F<sub>1,23</sub> = 2.66, p = .12) but modality and axis interacted (F<sub>2,46</sub> = 4.37, p = .02). To investigate this, we ran separate two-way (axis, rotation) RM-ANOVAs for each modality. While rotation reduced both visual (F<sub>1,23</sub> = 36.36, p = .001) and haptic (F<sub>1,23</sub> = 13.54, p = .001) recognition accuracy, there was an effect of axis in vision (F<sub>2,46</sub> = 3.93, p = .03) but not touch (F<sub>2,46</sub> = .56, p = .58). To examine this further, we compared the percentage reduction in accuracy for each axis in vision and touch. This was computed using the formula {[unrotated score–rotated score]/unrotated score}*100. (Four observations (2.7% of the total) could not be calculated because the formula required division by zero as there were no correct responses for unrotated objects in these cases; these instances were set to zero). Paired t-tests on these difference scores showed that visual recognition accuracy after z-rotation was significantly better than after x-rotation (t = −2.97, p = .007) or y-rotation (t = −2.19, p = .04): the x- and y-rotations were not different (t = .49, p = .63). In contrast, haptic recognition accuracy was equally disrupted by each axis of rotation (z-x: t = .71, p = .48; z-y: t = .48, p = .63; x-y: t = −.34, p = .73) (<a href="#pone-0000890-g004">Figure 4</a>).</p>
<div class="figure" id="pone-0000890-g004"><div class="img"><a name="pone-0000890-g004" title="Click for larger image " href="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000890.g004&amp;representation=PNG_M" data-doi="info:doi/10.1371/journal.pone.0000890" data-uri="info:doi/10.1371/journal.pone.0000890.g004"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000890.g004&amp;representation=PNG_I" alt="thumbnail" class="thumbnail"></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000890.g004/powerpoint">
                    PPT
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000890.g004/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000890.g004/largerimage">
                    PNG
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000890.g004/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pone.0000890.g004.PNG_L"></span>)
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000890.g004/originalimage">
                    TIFF
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000890.g004/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pone.0000890.g004.TIF"></span>)
                </a></li></ul></div><p><strong>Figure 4.  <span>Interaction between the within-modal conditions and the axis of rotation.</span></strong></p><a id="article1.body1.sec3.fig3.caption1.p1" name="article1.body1.sec3.fig3.caption1.p1"></a><p>Haptic within-modal recognition accuracy was equally disrupted by rotation about each axis whereas visual within-modal recognition was disrupted by the x- and y-rotations more than the z-rotation. The graph shows the percentage decrease in accuracy due to rotating the object away from the learned view. (Error bars = s.e.m.; asterisk = significant difference).</p>
<span>doi:10.1371/journal.pone.0000890.g004</span></div><a id="article1.body1.sec3.p4" name="article1.body1.sec3.p4"></a><p>A three-way (rotation, axis, modality) ANOVA of the cross-modal conditions alone showed that there was no main effect of object rotation (F<sub>1,23</sub> = 2.74, p = .11) or the axis of rotation (F<sub>2,46</sub> = .03, p = .97), and no significant difference between the two cross-modal conditions (F<sub>1,23</sub> = 1.34, p = .25). There were no significant interactions.</p>
<a id="article1.body1.sec3.p5" name="article1.body1.sec3.p5"></a><p>OSIQ-spatial scores were significantly correlated with overall accuracy in both rotated (r = .51, p = .01) and unrotated (r = .48, p = .02) conditions. As <a href="#pone-0000890-g005">Figure 5</a> shows, OSIQ-spatial scores were also significantly correlated with cross-modal accuracy in both rotated (r = .58, p = .003) and unrotated (r = .55, p = .005) conditions, but not with within-modal accuracy (rotated: r = .37, p = .08; unrotated: r = .28, p = .19). OSIQ-object scores were uncorrelated with accuracy, as predicted.</p>
<div class="figure" id="pone-0000890-g005"><div class="img"><a name="pone-0000890-g005" title="Click for larger image " href="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000890.g005&amp;representation=PNG_M" data-doi="info:doi/10.1371/journal.pone.0000890" data-uri="info:doi/10.1371/journal.pone.0000890.g005"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000890.g005&amp;representation=PNG_I" alt="thumbnail" class="thumbnail"></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000890.g005/powerpoint">
                    PPT
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000890.g005/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000890.g005/largerimage">
                    PNG
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000890.g005/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pone.0000890.g005.PNG_L"></span>)
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000890.g005/originalimage">
                    TIFF
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000890.g005/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pone.0000890.g005.TIF"></span>)
                </a></li></ul></div><p><strong>Figure 5.  <span>Scatterplots showing that OSIQ-spatial imagery scores correlate with cross-modal (A &amp; B) but not within-modal object recognition accuracy (C &amp; D).</span></strong></p><span>doi:10.1371/journal.pone.0000890.g005</span></div></div>

<div id="section4" class="section"><a id="s4" name="s4" toc="s4" title="Discussion"></a><h3>Discussion</h3><a id="article1.body1.sec4.p1" name="article1.body1.sec4.p1"></a><p>This study is the first to show that visuo-haptic cross-modal object recognition is essentially viewpoint-independent. Both visual and haptic within-modal recognition were significantly reduced by rotation of the object away from the learned view. This was not so for the two cross-modal conditions. It is well established that, as here, cross-modal recognition comes at a cost compared to within-modal recognition [for example, 11–15], but there was no significant additional cost associated with object rotation. This finding is the more robust because the task in this study was more demanding than in the study of Newell et al. <a href="#pone.0000890-Newell1">[2]</a> and yet the additional difficulty of object rotation had little effect on cross-modal recognition. For example, although we used similar objects as Newell et al. <a href="#pone.0000890-Newell1">[2]</a> did (with the exception of the removal of a texture cue) we allowed only half the time for object learning. In addition, participants had to discriminate between specific objects rather than just make a new/old judgment between learned objects and unlearned distractors.</p>
<a id="article1.body1.sec4.p2" name="article1.body1.sec4.p2"></a><p>In vision, viewpoint-independence suggests mediation by a high-level, relatively abstract representation <a href="#pone.0000890-Riesenhuber1">[16]</a>. Viewpoint-independence can occur, more trivially, when all object views are familiar <a href="#pone.0000890-Tarr1">[17]</a>, perhaps because separate, lower-level representations have been established for each viewpoint; or when the object has very distinctive parts <a href="#pone.0000890-Biederman1">[18]</a> that are easily transformed to match the new viewpoint. However, the objects in the present study were unfamiliar and lacked distinctive parts because the component blocks were identical except in their relationships to one another. Thus, viewpoint-independence could not have arisen simply from object familiarity or distinctiveness of object parts. Rather, the findings of the present study favor the idea of an abstract, high-level, modality-independent representation underlying cross-modal object recognition. Such a representation could be constructed by integrating lower-level, unisensory, viewpoint-dependent representations <a href="#pone.0000890-Riesenhuber1">[16]</a>. Functional neuroimaging studies have demonstrated convergence of visual and haptic shape processing in the intraparietal sulcus (IPS) and the lateral occipital complex (LOC) <a href="#pone.0000890-Amedi1">[19]</a>–<a href="#pone.0000890-Peltier1">[22]</a>. The nature of the representations in these areas is, however, incompletely understood, and has only been studied using visual stimuli. Activity in parts of the IPS scales with the angle of mental rotation <a href="#pone.0000890-Gauthier1">[6]</a> and also appears to be viewpoint-dependent <a href="#pone.0000890-James2">[23]</a>. There is a difference of opinion as to whether LOC activity is viewpoint-dependent <a href="#pone.0000890-GrillSpector1">[24]</a> or viewpoint-independent <a href="#pone.0000890-James2">[23]</a>. Thus, at present, the locus of the modality- and viewpoint-independent, high-level representation underlying cross-modal object recognition is unknown.</p>
<a id="article1.body1.sec4.p3" name="article1.body1.sec4.p3"></a><p>The existence of the high-level, modality-independent representation inferred here was obscured in earlier work <a href="#pone.0000890-Newell1">[2]</a> using objects that were extended along the y-axis. Here, we removed the confounding near-far exchange inherent in this earlier study, by selecting a presentation axis that made all object surfaces more equally available to touch, and demonstrated that cross-modal object recognition is consistently viewpoint-independent across all three axes of rotation. This contrasts with within-modal recognition, where viewpoint-dependence suggests mediation by lower-level, unisensory representations that might feed into the high-level viewpoint-independent representation mediating cross-modal recognition. The correlation between spatial imagery scores and cross-modal, but not within-modal, accuracy, and the lack of any correlation of object imagery scores with performance, suggests that the ability to mentally image complex spatial transformations is linked to viewpoint-independent recognition and supports the view that cross-modal performance is served by an abstract spatial representation.</p>
<a id="article1.body1.sec4.p4" name="article1.body1.sec4.p4"></a><p>Our results are also the first to suggest differences between visual and haptic viewpoint-dependence. Rotating an object can occlude a surface and transform the global shape in different ways depending on the axis of rotation <a href="#pone.0000890-Gauthier1">[6]</a>, suggesting potentially different bases for viewpoint-dependence in vision and touch. Varying the axis of rotation may not matter to touch because the hands are free to move around the object or manipulate it into different orientations relative to the hand. Thus no surface is occluded in touch and it is only necessary to deal with shape transformations. However, these manipulations are not possible visually unless one physically changes location with respect to the object <a href="#pone.0000890-Pasqualotto1">[25]</a>, so that vision has to deal with both shape transformations and surface occlusion. <a href="#pone-0000890-g004">Figure 4</a> suggests that the axis of rotation affects vision but not touch. Visual recognition was best after z-rotation – although this occluded the top surface, the shape transformation is a simple left/right mirror-image in the picture-plane. The x- and y- rotations were more complex; the x-rotation occluded the top surface and produced a mirror-image in the depth-plane. The y-rotation did not occlude a surface but involved two shape transformations, reversing the object from left to right and in the depth-plane. Although it may be counterintuitive that a rotation involving the occlusion of a surface on the main information-bearing axis is easier to process, it should be borne in mind that shape information from the two side surfaces was still available. There is evidence that such picture-plane rotations are easier than depth-plane rotations <a href="#pone.0000890-Gauthier1">[6]</a>, <a href="#pone.0000890-Logothetis1">[26]</a>, <a href="#pone.0000890-Perrett1">[27]</a>. Monkey inferotemporal neurons show faster generalization and exhibit larger generalization fields for picture-plane rotations than depth-plane rotations <a href="#pone.0000890-Logothetis1">[26]</a>. Face-selective neurons are more sensitive to depth-plane rotations (faces tilted towards/away from the viewer) than to picture-plane rotations (horizontal or inverted faces) <a href="#pone.0000890-Perrett1">[27]</a>. Picture-plane (z-axis) rotations result in faster and more accurate performance than depth-plane (x- and y-axis) rotations in both object recognition and mental rotation tasks, even though these tasks involve distinct neural networks <a href="#pone.0000890-Gauthier1">[6]</a>. Thus the picture-plane advantage may be a fairly general one. However, further work is necessary to verify that the differences between vision and touch derive from the nature of shape transformations and the presence of surface occlusion.</p>
<a id="article1.body1.sec4.p5" name="article1.body1.sec4.p5"></a><p>Our main conclusion is to clarify an important point about visuo-haptic cross-modal object recognition: that the underlying representation is viewpoint-independent even for unfamiliar objects lacking distinctive local features. Further, despite the unisensory representations each being viewpoint-dependent, there are differences between modalities with the axis of rotation being important in vision but not touch.</p>
</div>



<div class="contributions"><a id="authcontrib" name="authcontrib" toc="authcontrib" title="Author Contributions"></a><h3>Author Contributions</h3><p>Conceived and designed the experiments: SL. Performed the experiments: AP. Analyzed the data: KS SL. Contributed reagents/materials/analysis tools: SL. Wrote the paper: KS SL AP.</p></div><div><a id="references" name="references" toc="references" title="References"></a><h3>References</h3><ol class="references"><li><span class="label">1.
              </span><a name="pone.0000890-Jolicoeur1" id="pone.0000890-Jolicoeur1"></a>Jolicoeur P (1985) The time to name disoriented objects. Mem Cognition,  13: 289–303.  <ul class="find" data-citedArticleID="991766" data-doi="10.3758/bf03202498"><li><a href="http://dx.doi.org/10.3758/bf03202498" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=The+time+to+name+disoriented+objects." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22The+time+to+name+disoriented+objects.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">2.
              </span><a name="pone.0000890-Newell1" id="pone.0000890-Newell1"></a>Newell FN, Ernst MO, Tjan BS, Bulthoff HH (2001) Viewpoint dependence in visual and haptic object recognition. Psychol Sci,  12: 37–42.  <ul class="find" data-citedArticleID="991776" data-doi="10.1111/1467-9280.00307"><li><a href="http://dx.doi.org/10.1111/1467-9280.00307" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Viewpoint+dependence+in+visual+and+haptic+object+recognition." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Viewpoint+dependence+in+visual+and+haptic+object+recognition.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">3.
              </span><a name="pone.0000890-Klatzky1" id="pone.0000890-Klatzky1"></a>Klatzky RL, Lederman S, Reed C (1987) There's more to touch than meets the eye: The salience of object attributes for haptics with and without vision. J Exp Psychol: Gen,  116: 356–369.  <ul class="find" data-citedArticleID="991768"><li><a href="http://www.crossref.org/guestquery/?auth2=&amp;atitle2=There%27s+more+to+touch+than+meets+the+eye%3A+The+salience+of+object+attributes+for+haptics+with+and+without+vision.&amp;auth=&amp;atitle=There%27s+more+to+touch+than+meets+the+eye%3A+The+salience+of+object+attributes+for+haptics+with+and+without+vision." target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=There%27s+more+to+touch+than+meets+the+eye%3A+The+salience+of+object+attributes+for+haptics+with+and+without+vision." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22There%27s+more+to+touch+than+meets+the+eye%3A+The+salience+of+object+attributes+for+haptics+with+and+without+vision.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">4.
              </span><a name="pone.0000890-Reales1" id="pone.0000890-Reales1"></a>Reales JM, Ballesteros S (1999) Implicit and explicit memory for visual and haptic objects: Cross-modal priming depends on structural descriptions. J Exp Psychol: Learn,  25: 644–663.  <ul class="find" data-citedArticleID="991790" data-doi="10.1037/0278-7393.25.3.644"><li><a href="http://dx.doi.org/10.1037/0278-7393.25.3.644" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Implicit+and+explicit+memory+for+visual+and+haptic+objects%3A+Cross-modal+priming+depends+on+structural+descriptions." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Implicit+and+explicit+memory+for+visual+and+haptic+objects%3A+Cross-modal+priming+depends+on+structural+descriptions.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">5.
              </span><a name="pone.0000890-Heller1" id="pone.0000890-Heller1"></a>Heller MA, Brackett DD, Scroggs E, Steffen H, Heatherly K, et al.  (2002) Tangible pictures: Viewpoint effects and linear perspective in visually impaired people. Perception,  31: 747–769.  <ul class="find" data-citedArticleID="991760" data-doi="10.1068/p3253"><li><a href="http://dx.doi.org/10.1068/p3253" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Tangible+pictures%3A+Viewpoint+effects+and+linear+perspective+in+visually+impaired+people." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Tangible+pictures%3A+Viewpoint+effects+and+linear+perspective+in+visually+impaired+people.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">6.
              </span><a name="pone.0000890-Gauthier1" id="pone.0000890-Gauthier1"></a>Gauthier I, Hayward WG, Tarr MJ, Anderson AW, Skudlarski P, et al.  (2002) BOLD activity during mental rotation and viewpoint-dependent object recognition. Neuron,  34: 161–171.  <ul class="find" data-citedArticleID="991756" data-doi="10.1016/s0896-6273(02)00622-0"><li><a href="http://dx.doi.org/10.1016/s0896-6273(02)00622-0" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=BOLD+activity+during+mental+rotation+and+viewpoint-dependent+object+recognition." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22BOLD+activity+during+mental+rotation+and+viewpoint-dependent+object+recognition.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">7.
              </span><a name="pone.0000890-Blajenkova1" id="pone.0000890-Blajenkova1"></a>Blajenkova O, Kozhevnikov M, Motes MA (2006) Object-spatial imagery: A new self-report questionnaire. Appl Cognit Psychol,  20: 239–263.  <ul class="find" data-citedArticleID="991748" data-doi="10.1002/acp.1182"><li><a href="http://dx.doi.org/10.1002/acp.1182" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Object-spatial+imagery%3A+A+new+self-report+questionnaire." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Object-spatial+imagery%3A+A+new+self-report+questionnaire.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">8.
              </span><a name="pone.0000890-Kozhevnikov1" id="pone.0000890-Kozhevnikov1"></a>Kozhevnikov M, Kosslyn S, Shephard J (2005) Spatial versus object visualizers: A new characterization of cognitive style. Mem Cognition,  33: 710–726.  <ul class="find" data-citedArticleID="991770" data-doi="10.3758/bf03195337"><li><a href="http://dx.doi.org/10.3758/bf03195337" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Spatial+versus+object+visualizers%3A+A+new+characterization+of+cognitive+style." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Spatial+versus+object+visualizers%3A+A+new+characterization+of+cognitive+style.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">9.
              </span><a name="pone.0000890-Lacey1" id="pone.0000890-Lacey1"></a>Lacey S, Campbell C (2006) Mental representation in visual/haptic crossmodal memory: Evidence from interference effects. Q J Exp Psychol,  59: 361–376.  <ul class="find" data-citedArticleID="991772" data-doi="10.1080/17470210500173232"><li><a href="http://dx.doi.org/10.1080/17470210500173232" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Mental+representation+in+visual%2Fhaptic+crossmodal+memory%3A+Evidence+from+interference+effects." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Mental+representation+in+visual%2Fhaptic+crossmodal+memory%3A+Evidence+from+interference+effects.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">10.
              </span><a name="pone.0000890-Freides1" id="pone.0000890-Freides1"></a>Freides D (1974) Human information processing and sensory modality: Cross-modal functions, information complexity, memory, and deficit. Psychol Bull,  8: 284–310.  <ul class="find" data-citedArticleID="991754" data-doi="10.1037/h0036331"><li><a href="http://dx.doi.org/10.1037/h0036331" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Human+information+processing+and+sensory+modality%3A+Cross-modal+functions%2C+information+complexity%2C+memory%2C+and+deficit." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Human+information+processing+and+sensory+modality%3A+Cross-modal+functions%2C+information+complexity%2C+memory%2C+and+deficit.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">11.
              </span><a name="pone.0000890-Casey1" id="pone.0000890-Casey1"></a>Casey SJ, Newell FN (2005) The role of long-term and short-term familiarity in visual and haptic face recognition. Exp Brain Res,  166: 583–591.  <ul class="find" data-citedArticleID="991752" data-doi="10.1007/s00221-005-2398-3"><li><a href="http://dx.doi.org/10.1007/s00221-005-2398-3" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=The+role+of+long-term+and+short-term+familiarity+in+visual+and+haptic+face+recognition." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22The+role+of+long-term+and+short-term+familiarity+in+visual+and+haptic+face+recognition.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">12.
              </span><a name="pone.0000890-Newell2" id="pone.0000890-Newell2"></a>Newell FN, Woods AT, Mernagh M, Bulthoff HH (2005) Visual, haptic and crossmodal recognition of scenes. Exp Brain Res,  161: 233–242.  <ul class="find" data-citedArticleID="991778" data-doi="10.1007/s00221-004-2067-y"><li><a href="http://dx.doi.org/10.1007/s00221-004-2067-y" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Visual%2C+haptic+and+crossmodal+recognition+of+scenes." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Visual%2C+haptic+and+crossmodal+recognition+of+scenes.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">13.
              </span><a name="pone.0000890-Norman1" id="pone.0000890-Norman1"></a>Norman JF, Norman HF, Clayton AM, Lianekhammy J, Zielke G (2004) The visual and haptic perception of natural object shape. Percept Psychophys,  66: 342–351.  <ul class="find" data-citedArticleID="991782" data-doi="10.3758/bf03194883"><li><a href="http://dx.doi.org/10.3758/bf03194883" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=The+visual+and+haptic+perception+of+natural+object+shape." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22The+visual+and+haptic+perception+of+natural+object+shape.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">14.
              </span><a name="pone.0000890-Bushnell1" id="pone.0000890-Bushnell1"></a>Bushnell EW, Baxt C (1999) Children's haptic and cross-modal recognition with familiar and unfamiliar objects. J Exp Psychol Human,  25: 1867–1881.  <ul class="find" data-citedArticleID="991750" data-doi="10.1037/0096-1523.25.6.1867"><li><a href="http://dx.doi.org/10.1037/0096-1523.25.6.1867" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Children%27s+haptic+and+cross-modal+recognition+with+familiar+and+unfamiliar+objects." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Children%27s+haptic+and+cross-modal+recognition+with+familiar+and+unfamiliar+objects.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">15.
              </span><a name="pone.0000890-Newell3" id="pone.0000890-Newell3"></a>Newell KM, Shapiro DC, Carlton MJ (1979) Coordinating visual and kinaesthetic memory codes. Brit J Psychol,  70: 87–96.  <ul class="find" data-citedArticleID="991780" data-doi="10.1111/j.2044-8295.1979.tb02147.x"><li><a href="http://dx.doi.org/10.1111/j.2044-8295.1979.tb02147.x" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Coordinating+visual+and+kinaesthetic+memory+codes." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Coordinating+visual+and+kinaesthetic+memory+codes.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">16.
              </span><a name="pone.0000890-Riesenhuber1" id="pone.0000890-Riesenhuber1"></a>Riesenhuber M, Poggio T (1999) Hierarchical models of object recognition in cortex. Nature Neurosci,  2: 1019–1025.  <ul class="find" data-citedArticleID="991792" data-doi="10.1038/14819"><li><a href="http://dx.doi.org/10.1038/14819" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Hierarchical+models+of+object+recognition+in+cortex." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Hierarchical+models+of+object+recognition+in+cortex.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">17.
              </span><a name="pone.0000890-Tarr1" id="pone.0000890-Tarr1"></a>Tarr MJ, Bulthoff HH (1995) Is human object recognition better described by geon structural descriptions or by multiple views: Comment on Biederman and Gerhardstein (1993). J Exp Psychol Human,  21: 1494–1505.  <ul class="find" data-citedArticleID="991794" data-doi="10.1037//0096-1523.21.6.1494"><li><a href="http://dx.doi.org/10.1037//0096-1523.21.6.1494" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Is+human+object+recognition+better+described+by+geon+structural+descriptions+or+by+multiple+views%3A+Comment+on+Biederman+and+Gerhardstein+%281993%29." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Is+human+object+recognition+better+described+by+geon+structural+descriptions+or+by+multiple+views%3A+Comment+on+Biederman+and+Gerhardstein+%281993%29.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">18.
              </span><a name="pone.0000890-Biederman1" id="pone.0000890-Biederman1"></a>Biederman I (1987) Recognition-by-components: A theory of human image understanding. Psychol Rev,  94: 115–147.  <ul class="find" data-citedArticleID="991746" data-doi="10.1037//0033-295x.94.2.115"><li><a href="http://dx.doi.org/10.1037//0033-295x.94.2.115" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Recognition-by-components%3A+A+theory+of+human+image+understanding." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Recognition-by-components%3A+A+theory+of+human+image+understanding.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">19.
              </span><a name="pone.0000890-Amedi1" id="pone.0000890-Amedi1"></a>Amedi A, Malach R, Hendler T, Peled S, Zohary E (2001) Visuo-haptic object-related activation in the ventral pathway. Nature Neurosci,  4: 324–330.  <ul class="find" data-citedArticleID="991744" data-doi="10.1038/85201"><li><a href="http://dx.doi.org/10.1038/85201" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Visuo-haptic+object-related+activation+in+the+ventral+pathway." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Visuo-haptic+object-related+activation+in+the+ventral+pathway.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">20.
              </span><a name="pone.0000890-James1" id="pone.0000890-James1"></a>James TW, Humphrey GK, Gati JS, Servos P, Menon RS, et al.  (2002) Haptic study of three-dimensional objects activates extrastriate visual areas. Neuropsychologia,  40: 1706–1714.  <ul class="find" data-citedArticleID="991762" data-doi="10.1016/s0028-3932(02)00017-9"><li><a href="http://dx.doi.org/10.1016/s0028-3932(02)00017-9" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Haptic+study+of+three-dimensional+objects+activates+extrastriate+visual+areas." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Haptic+study+of+three-dimensional+objects+activates+extrastriate+visual+areas.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">21.
              </span><a name="pone.0000890-Zhang1" id="pone.0000890-Zhang1"></a>Zhang M, Weisser VD, Stilla R, Prather SC, Sathian K (2004) Multisensory cortical processing of shape and its relation to mental imagery. Cogn Affect Behav Ne,  4: 251–259.  <ul class="find" data-citedArticleID="991796" data-doi="10.3758/cabn.4.2.251"><li><a href="http://dx.doi.org/10.3758/cabn.4.2.251" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Multisensory+cortical+processing+of+shape+and+its+relation+to+mental+imagery." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Multisensory+cortical+processing+of+shape+and+its+relation+to+mental+imagery.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">22.
              </span><a name="pone.0000890-Peltier1" id="pone.0000890-Peltier1"></a>Peltier S, Stilla R, Mariola E, LaConte S, Hu X, et al.  (2007) Activity and effective connectivity of parietal and occipital cortical regions during haptic shape perception. Neuropsychologia,  45: 476–483.  <ul class="find" data-citedArticleID="991786" data-doi="10.1016/j.neuropsychologia.2006.03.003"><li><a href="http://dx.doi.org/10.1016/j.neuropsychologia.2006.03.003" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Activity+and+effective+connectivity+of+parietal+and+occipital+cortical+regions+during+haptic+shape+perception." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Activity+and+effective+connectivity+of+parietal+and+occipital+cortical+regions+during+haptic+shape+perception.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">23.
              </span><a name="pone.0000890-James2" id="pone.0000890-James2"></a>James TW, Humphrey GK, Gati JS, Menon RS, Goodale MA (2002) Differential effects of viewpoint on object-driven activation in dorsal and ventral streams. Neuron,  35: 793–801.  <ul class="find" data-citedArticleID="991764" data-doi="10.1016/s0896-6273(02)00803-6"><li><a href="http://dx.doi.org/10.1016/s0896-6273(02)00803-6" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Differential+effects+of+viewpoint+on+object-driven+activation+in+dorsal+and+ventral+streams." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Differential+effects+of+viewpoint+on+object-driven+activation+in+dorsal+and+ventral+streams.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">24.
              </span><a name="pone.0000890-GrillSpector1" id="pone.0000890-GrillSpector1"></a>Grill-Spector K, Kushnir T, Edelman S, Avidan G, Itzchak Y, et al.  (1999) Differential processing of objects under various viewing conditions in the human lateral occipital complex. Neuron,  24: 187–203.  <ul class="find" data-citedArticleID="991758" data-doi="10.1016/s0896-6273(00)80832-6"><li><a href="http://dx.doi.org/10.1016/s0896-6273(00)80832-6" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Differential+processing+of+objects+under+various+viewing+conditions+in+the+human+lateral+occipital+complex." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Differential+processing+of+objects+under+various+viewing+conditions+in+the+human+lateral+occipital+complex.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">25.
              </span><a name="pone.0000890-Pasqualotto1" id="pone.0000890-Pasqualotto1"></a>Pasqualotto A, Finucane C, Newell FN (2005) Visual and haptic representations of scenes are updated with observer movement. Exp Brain Res,  166: 481–488.  <ul class="find" data-citedArticleID="991784" data-doi="10.1007/s00221-005-2388-5"><li><a href="http://dx.doi.org/10.1007/s00221-005-2388-5" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Visual+and+haptic+representations+of+scenes+are+updated+with+observer+movement." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Visual+and+haptic+representations+of+scenes+are+updated+with+observer+movement.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">26.
              </span><a name="pone.0000890-Logothetis1" id="pone.0000890-Logothetis1"></a>Logothetis NK, Pauls J, Poggio T (1995) Shape representation in the inferior temporal cortex of monkeys. Curr Biol,  5: 552–563.  <ul class="find" data-citedArticleID="991774" data-doi="10.1016/s0960-9822(95)00108-4"><li><a href="http://dx.doi.org/10.1016/s0960-9822(95)00108-4" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Shape+representation+in+the+inferior+temporal+cortex+of+monkeys." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Shape+representation+in+the+inferior+temporal+cortex+of+monkeys.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">27.
              </span><a name="pone.0000890-Perrett1" id="pone.0000890-Perrett1"></a>Perrett DI, Smith PAJ, Potter DD, Mistlin AJ, Head AS, et al.  (1985) Visual cells in the temporal cortex sensitive to face view and gaze direction. Proc R Soc Lond B,  223: 293–317.  <ul class="find" data-citedArticleID="991788" data-doi="10.1098/rspb.1985.0003"><li><a href="http://dx.doi.org/10.1098/rspb.1985.0003" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Visual+cells+in+the+temporal+cortex+sensitive+to+face+view+and+gaze+direction." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Visual+cells+in+the+temporal+cortex+sensitive+to+face+view+and+gaze+direction.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li></ol></div>

  </div>

      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000890.XML" value="53559"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000890.PDF" value="192669"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000890.g001.PNG_L" value="2518984"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000890.g001.PNG_M" value="366538"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000890.g001.PNG_S" value="12131"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000890.g001.TIF" value="3130216"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000890.g001.PNG_I" value="104812"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000890.g002.PNG_L" value="39988"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000890.g002.PNG_M" value="40804"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000890.g002.PNG_S" value="10399"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000890.g002.TIF" value="246180"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000890.g002.PNG_I" value="14734"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000890.g003.PNG_L" value="49539"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000890.g003.PNG_M" value="45239"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000890.g003.PNG_S" value="11168"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000890.g003.TIF" value="313126"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000890.g003.PNG_I" value="16506"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000890.g004.PNG_L" value="46800"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000890.g004.PNG_M" value="28719"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000890.g004.PNG_S" value="9014"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000890.g004.TIF" value="286972"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000890.g004.PNG_I" value="9729"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000890.g005.PNG_L" value="1391534"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000890.g005.PNG_M" value="121076"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000890.g005.PNG_S" value="14684"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000890.g005.TIF" value="2109616"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000890.g005.PNG_I" value="41654"/>

</div>
<div class="sidebar">

  <div class="article-actions cf">
      <div class="download">
        <span class="btn"><a href="/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0000890&amp;representation=PDF" title="Download" target="_blank">Download PDF</a></span>
      </div>
      <div class="btn-reveal dropdown">
        <div class="dropdown-icon">
          <span class="btn">&nbsp;</span>
        </div>

        <div class="content">
          <ul class="bullet">
            <li><a href="/article/citationList.action?articleURI=info%3Adoi%2F10.1371%2Fjournal.pone.0000890" title="Download citations">Citation</a></li>
            <li><a href="/article/fetchObjectAttachment.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0000890&amp;representation=XML" title="Download article XML">XML</a></li>
          </ul>
        </div>
      </div> <!-- end btn-reveal dropdown-->


    <div class="btn-reveal flt-l">
        <span class="btn">Print</span>
        <div class="content">
            <ul class="bullet">
                <li id="print-article"><a href="#" onclick="if(typeof(_gaq) != 'undefined'){ _gaq.push(['_trackEvent','Article', 'Print', 'Click']); } window.print(); return false;" title="Print Article">Print article</a></li>
                <li>
                  <a href="https://www.odysseypress.com/onlinehost/reprint_order.php?type=A&page=0&journal=7&doi=10.1371/journal.pone.0000890&volume=&issue=&title=Cross-Modal Object Recognition Is Viewpoint-Independent&author_name=Simon%20Lacey%2C%20Andrew%20Peters%2C%20K.%20Sathian&start_page=1&end_page=6" title="Odyssey Press">EzReprint</a>
                </li>
            </ul>
        </div>
    </div>

    <div class="btn-reveal flt-r">
        <span class="btn">Share</span>
        <div class="content">
            <ul class="social">
                <li><a href="http://www.reddit.com/submit?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0000890" target="_blank" title="Submit to Reddit"><img src="/images/icon.reddit.16.png" width="16" height="16" alt="Reddit">Reddit</a></li>

                <li><a href="https://plus.google.com/share?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0000890" target="_blank" title="Share on Google+"><img src="/images/icon.gplus.16.png" width="16" height="16" alt="Google+">Google+</a></li>

                <li><a href="http://www.stumbleupon.com/submit?url=http%3A%2F%2Fwww.plosone.org%2Farticle%2Finfo%253Adoi%252F10.1371%252Fjournal.pone.0000890" target="_blank" title="Add to StumbleUpon"><img src="/images/icon.stumble.16.png" width="16" height="16" alt="StumbleUpon">StumbleUpon</a></li>

                <li><a href="http://www.facebook.com/share.php?u=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0000890&amp;t=Cross-Modal%20Object%20Recognition%20Is%20Viewpoint-Independent" target="_blank" title="Share on Facebook"><img src="/images/icon.fb.16.png" width="16" height="16" alt="Facebook">Facebook</a></li>

                <li><a href="http://www.linkedin.com/shareArticle?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0000890&title=Cross-Modal%20Object%20Recognition%20Is%20Viewpoint-Independent&summary=Checkout%20this%20article%20I%20found%20at%20PLOS" target="_blank" title="Add to LinkedIn"><img src="/images/icon.linkedin.16.png" width="16" height="16" alt="Mendeley">LinkedIn</a></li>

                <li><a href="http://www.citeulike.org/posturl?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0000890&amp;title=Cross-Modal%20Object%20Recognition%20Is%20Viewpoint-Independent" target="_blank" title="Add to CiteULike"><img src="/images/icon.cul.16.png" width="16" height="16" alt="CiteULike">CiteULike</a></li>

                <li><a href="http://www.mendeley.com/import/?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0000890" target="_blank" title="Add to Mendeley"><img src="/images/icon.mendeley.16.png" width="16" height="16" alt="Mendeley">Mendeley</a></li>

                <li><a href="https://www.pubchase.com/library?add_aid=10.1371%2Fjournal.pone.0000890&amp;source=plos" target="_blank" title="Add to PubChase"><img src="/images/icon.pc.16.png" width="16" height="16" alt="PubChase">PubChase</a></li>


                <script type="text/javascript">
                    // replace tweet with one that's pre-shortened to 140 chars
                    function truncateTweetText() {
                        var twtTitle = 'Cross-Modal Object Recognition Is Viewpoint-Independent';
                        var twtUrl = 'http://dx.plos.org/10.1371/journal.pone.0000890';
                        // all URLs posted to twitter get auto-shortened to 20 chars.
                        var maxLength = 140 - (20 + 1);
                        // truncate the title to include space for twtTag and ellipsis (here, 10 = tag length + space + ellipsis)
                        if (twtTitle.length > maxLength) { twtTitle = twtTitle.substr(0, (maxLength - 10)) + '...'; }
                        // set the href to use the shortened tweet
                        $('#twitter-share-link').prop('href', 'http://twitter.com/intent/tweet?text=' + encodeURIComponent('#PLOSONE: ' + twtTitle + ' ' + twtUrl));
                    }
                </script>
                <li><a href="http://twitter.com/intent/tweet?text=#PLOSONE%3A%20Cross-Modal%20Object%20Recognition%20Is%20Viewpoint-Independent http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0000890" onclick="truncateTweetText();" target="_blank" title="Share on Twitter" id="twitter-share-link"><img src="/images/icon.twtr.16.png" width="16" height="16" alt="Twitter">Twitter</a></li>

                <li><a href="/article/email/info%3Adoi%2F10.1371%2Fjournal.pone.0000890" title="Email this article"><img src="/images/icon.email.16.png" width="16" height="16" alt="Email">Email</a></li>
            </ul>
        </div>
    </div><!--end btn-reveal flt-r-->
</div><!-- end article-actions-->

<!-- begin Crossmark -->

<a id="open-crossmark" href="#" style="margin-top: -28px; display:block"><img style="border: 0; display: none;
 padding: 10px 0 18px 0;"  id="crossmark-icon" src="/images/logo-crossmark-bw.png" /></a>
<div id="crossmark-dialog" style="display: none;" title="">
    <!-- the external CrossMark data is loaded inside this iframe -->
    <iframe id="crossmark-dialog-frame" frameborder="0"></iframe>
</div>

<!-- end crossmark -->


<div class="block" id="subject-area-sidebar-block">
    <div class="header">
        <h3>Subject Areas</h3><div title="More information" id="subject-area-sidebar-block-help-icon"><img align="right"
                                                                                                           alt="info" src="/images/button_info.png"/><div id="subject-area-sidebar-block-help"><img align="right"
                                                                                                                                                                                                    src="/images/button_info.png"/><p>
        <b>We want your feedback.</b> Do these subject areas make sense for this article? If not, click the flag
        next to the incorrect subject area and we will review it. Thanks for your help!
    </p></div></div>
    </div>


    <ul id="subject-area-sidebar-list">

























          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Analysis+of+variance%22" title="Search for articles in the subject area:'Analysis of variance'"><div class="flagText">Analysis of variance</div></a>
              <div data-categoryid="42193" data-articleid="24618"
                   data-categoryname="Analysis of variance"
                   class="flagImage" title="Flag 'Analysis of variance' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Face+recognition%22" title="Search for articles in the subject area:'Face recognition'"><div class="flagText">Face recognition</div></a>
              <div data-categoryid="32737" data-articleid="24618"
                   data-categoryname="Face recognition"
                   class="flagImage" title="Flag 'Face recognition' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Learning%22" title="Search for articles in the subject area:'Learning'"><div class="flagText">Learning</div></a>
              <div data-categoryid="20059" data-articleid="24618"
                   data-categoryname="Learning"
                   class="flagImage" title="Flag 'Learning' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Neural+networks%22" title="Search for articles in the subject area:'Neural networks'"><div class="flagText">Neural networks</div></a>
              <div data-categoryid="1931" data-articleid="24618"
                   data-categoryname="Neural networks"
                   class="flagImage" title="Flag 'Neural networks' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Neurons%22" title="Search for articles in the subject area:'Neurons'"><div class="flagText">Neurons</div></a>
              <div data-categoryid="17179" data-articleid="24618"
                   data-categoryname="Neurons"
                   class="flagImage" title="Flag 'Neurons' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Object+recognition%22" title="Search for articles in the subject area:'Object recognition'"><div class="flagText">Object recognition</div></a>
              <div data-categoryid="20057" data-articleid="24618"
                   data-categoryname="Object recognition"
                   class="flagImage" title="Flag 'Object recognition' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Touch%22" title="Search for articles in the subject area:'Touch'"><div class="flagText">Touch</div></a>
              <div data-categoryid="21651" data-articleid="24618"
                   data-categoryname="Touch"
                   class="flagImage" title="Flag 'Touch' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Vision%22" title="Search for articles in the subject area:'Vision'"><div class="flagText">Vision</div></a>
              <div data-categoryid="17177" data-articleid="24618"
                   data-categoryname="Vision"
                   class="flagImage" title="Flag 'Vision' as inappropriate"></div>
          </li>
    </ul>
</div>

<div class="ad">
    <div class="title">Advertisement</div>






  <iframe id='a0852f54' name='a0852f54'
    src='http://ads.plos.org/www/delivery/afr.php?zoneid=381&amp;cb=6959'
    frameborder='0' scrolling='no' width='160' height='600'>
    <a href='http://ads.plos.org/www/delivery/ck.php?n=a0852f54&amp;cb=2433'
      target='_top'><img src='http://ads.plos.org/www/delivery/avw.php?zoneid=381&amp;cb=7776&amp;n=a0852f54'
      border='0' alt=''/>
    </a>
  </iframe>



</div>

<div id="twitter-alm-timeline" class="twitter-alm-timeline"></div>


</div><!-- sidebar -->
    </div>
  </div>
</div>
<script src="http://wl.figshare.com/static/p_widget.js" type="text/javascript"></script><div id="pageftr">
  <div class="ftr-cols cf">
    <div class="col col-1">
      <img src="/images/logo-plos-footer.png" alt="PLOS Logo" class="logo" />
      <p><a href="/static/releaseNotes">Ambra 2.9.16</a> Managed Colocation provided <br />by <a href="http://www.isc.org/">Internet Systems Consortium</a>.<p>
      <div class="nav nav-aux">
        <a href="/static/privacy">Privacy Policy</a> |
        <a href="/static/terms">Terms of Use</a> |
        <a href="http://www.plos.org/advertise/">Advertise</a> |
        <a href="http://www.plos.org/about/media-inquiries/">Media Inquiries</a>
      </div>
    </div>
    <div class="col col-2">
      <p><a href="http://www.plos.org/publications/journals/">Publications</a></p>
      <div class="nav">
        <ul>
          <li><a href="http://www.plosbiology.org">PLOS Biology</a></li>
          <li><a href="http://www.plosmedicine.org">PLOS Medicine</a></li>
          <li><a href="http://www.ploscompbiol.org">PLOS Computational Biology</a></li>
          <li><a href="http://currents.plos.org">PLOS Currents</a></li>
          <li><a href="http://www.plosgenetics.org">PLOS Genetics</a></li>
          <li><a href="http://www.plospathogens.org">PLOS Pathogens</a></li>
          <li><a href="http://www.plosone.org">PLOS ONE</a></li>
          <li><a href="http://www.plosntds.org">PLOS Neglected Tropical Diseases</a></li>
        </ul>
      </div>
    </div>
    <div class="col col-3">
      <div class="nav">
        <p><a href="http://www.plos.org">plos.org</a></p>
        <p><a href="http://blogs.plos.org">Blogs</a></p>
        <p><a href="http://www.ploscollections.org">Collections</a></p>
        <p><a href="/feedback/new">Send us feedback</a></p>

        <p>California (US) corporation #C2354500, based in San Francisco</p>
      </div>
    </div>
  </div>
</div><!-- pageftr -->

</div><!-- end page-wrap, this div is in header.ftl -->
<script type="text/javascript" src="/javascript/jquery-1.8.1-min.js?v=Tm7VCOzZz3lE03ghpkS6SWkHbyI"></script>
<script type="text/javascript" src="/javascript/ga-min.js?v=lNQ4gt8QcPDatjsdOFl_FGpPhLY"></script>
<script type="text/javascript" src="/javascript/jquery.hoverIntent-min.js?v=mRiGNYY9cIXxVb8u0K_MdW7hHnc"></script>
<script type="text/javascript" src="/javascript/jquery.placeholder-min.js?v=21Pn56Ur9h1N4K4VZDa0nqI3Pxo"></script>
<script type="text/javascript" src="/javascript/jquery.jsonp-2.4.0-min.js?v=lqTpzoHfSq3I5Ygo01qq5WankEo"></script>
<script type="text/javascript" src="/javascript/jquery-ui-1.9.2.custom-min.js?v=raSSlfNO0YsV5uUpAKmTB9n5VTc"></script>
<script type="text/javascript" src="/javascript/jquery.tooltip-min.js?v=cw+6Smh+mdryIA25xvqIvHMrnZM"></script>
<script type="text/javascript" src="/javascript/jquery.uniform-min.js?v=kYUAnX6W2W_2fK3RIuQ2m_YFG9U"></script>
<script type="text/javascript" src="/javascript/jquery.pjax-min.js?v=939kLBjL5_YKbx71T1RHjYaD4l8"></script>
<script type="text/javascript" src="/javascript/imagesloaded-min.js?v=XeuAp8Gc3mvQUo+wZCSF8ttPwvw"></script>
<script type="text/javascript" src="/javascript/figviewer-min.js?v=yPUa0sUQ_iHkI+IRv2i9bjyZJFo"></script>
<script type="text/javascript" src="/javascript/global-min.js?v=0Q3PwjeaWtXYDnqIsQvnL_ou0qs"></script>
<script type="text/javascript" src="/javascript/jquery.touchswipe-min.js?v=huaek_e6HqTduvCNAN91dJolTyw"></script>
<script type="text/javascript" src="/javascript/jquery.base64-min.js?v=VwV1zeVqKZj5FCAdlK0q5NRxbBg"></script>
<script type="text/javascript" src="/javascript/alm-min.js?v=Y5gm6B0b4Kx2YHNObNrgEeBgXlY"></script>
<script type="text/javascript" src="/javascript/taxonomy-browser-min.js?v=vBVMuDMYkGJCXIUxLe35GoyiJNw"></script>
<script type="text/javascript" src="/javascript/jquery.filterize-min.js?v=j0ZKVnHyk2nhFy8eIuNJkp7xaM0"></script>
<script type="text/javascript" src="/javascript/plosone-min.js?v=TK4H4arL_XBSwwJq+K1N3kqYfAI"></script>
<script type="text/javascript" src="/javascript/twitter-min.js?v=xKgcxLsQFXy+at1ao1NVke8nFlM"></script>
<script type="text/javascript" src="/javascript/crossmark.1.4-min.js?v=3FO4k0SjwTaGNnKGNSqthar1080"></script>
<script type="text/javascript">
  var _sf_async_config={uid:16579,domain:"plosone.org"};
  (function(){
    function loadChartbeat() {
      window._sf_endpt=(new Date()).getTime();
      var e = document.createElement('script');
      e.setAttribute('language', 'javascript');
      e.setAttribute('type', 'text/javascript');
      e.setAttribute('src',
          (("https:" == document.location.protocol) ? "https://a248.e.akamai.net/chartbeat.download.akamai.com/102508/" : "http://static.chartbeat.com/") +
              "js/chartbeat.js");
      document.body.appendChild(e);
    }
    var oldonload = window.onload;
    window.onload = (typeof window.onload != 'function') ?
        loadChartbeat : function() { oldonload(); loadChartbeat(); };
  })();
</script>
<!-- <script type="application/javascript" src="http://crossmark.crossref.org/javascripts/v1.3/crossmark.min.js"></script> -->

</body>
</html>
