

 



<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"
      xmlns:foaf="http://xmlns.com/foaf/0.1/"
      xmlns:dc="http://purl.org/dc/terms/"
      xmlns:doi="http://dx.doi.org/"
      xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
      xmlns:xsd="http://www.w3.org/2001/XMLSchema-datatypes#"
      lang="en" xml:lang="en"
      itemscope itemtype="http://schema.org/Article"
      class="no-js">
<head prefix="og: http://ogp.me/ns#">
  <title>PLOS ONE: Causal Inference in Multisensory Perception</title>


<link rel="stylesheet" type="text/css"  href="/css/global-min.css?v=izteQ6tu7kgsJZW_xmrYizvKiHM" />


    <!--[if lte IE 7]>
<link rel="stylesheet" type="text/css"  href="/css/lte_ie7-min.css?v=3bykQUyQmReeuobVyPozcJ9LxRc" />
    <![endif]-->


<link rel="stylesheet" type="text/css"  href="/css/jquery-ui-min.css?v=eXDHTEJM0lIAmDe5k0I0Ad4nxNo" />


<link rel="stylesheet" type="text/css"  href="/css/journal.css?v=T7ZVxJfgk9jNxLAJ2qHz1vZpgYU" />


<link rel="stylesheet" type="text/css" media="print" href="/css/print-min.css?v=T5lb0B3q6EXBsuDluc5V5w+AkRc" />


  <link rel="stylesheet" href="http://f.fontdeck.com/s/css/js/www.plosone.org/24557.css" type="text/css"/>

  <!--chartbeat -->
  <script type="text/javascript">var _sf_startpt = (new Date()).getTime()</script>
  <script>document.documentElement.className += ' js';</script>

  <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <meta http-equiv="X-UA-Compatible" content="IE=EmulateIE7; IE=EmulateIE9"/>
  <meta name="description" content="PLOS ONE: an inclusive, peer-reviewed, open-access resource from the PUBLIC LIBRARY OF SCIENCE. Reports of well-performed scientific studies from all disciplines freely available to the whole world."/>
  <meta name="keywords" content="PLOS, Public Library of Science, Open Access, Open-Access, Science, Medicine, Biology, Research, Peer-review, Inclusive, Interdisciplinary, Ante-disciplinary, Physics, Chemistry, Engineering"/>
  <meta name="almHost" content="http://alm.plos.org/api/v3/articles"/>
  <meta name="searchHost" content="http://api.plos.org/search" />
  <meta name="termsHost" content="http://api.plos.org/terms" />
  <meta name="solrApiKey" content="plos"/>
  <meta name="almAPIKey" content="3pezRBRXdyzYW6ztfwft" />
  <meta name="currentJournal" content="PLoSONE" />
  <meta name="almRequestBatchSize" content="" />

  <meta name="citation_publisher" content="Public Library of Science"/>
  <meta name="citation_doi" content="10.1371/journal.pone.0000943"/>
  <meta name="dc.identifier" content="10.1371/journal.pone.0000943" />

    <meta name="citation_title" content="Causal Inference in Multisensory Perception"/>
    <meta itemprop="name" content="Causal Inference in Multisensory Perception"/>

      <meta name="citation_author" content="Konrad P. Körding"/>
            <meta name="citation_author_institution" content="Rehabilitation Institute of Chicago, Northwestern University, Chicago, Illinois, United States of America"/>
      <meta name="citation_author" content="Ulrik Beierholm"/>
            <meta name="citation_author_institution" content="Computation and Neural Systems, California Institute of Technology, Pasadena, California, United States of America"/>
      <meta name="citation_author" content="Wei Ji Ma"/>
            <meta name="citation_author_institution" content="Department of Brain and Cognitive Sciences, University of Rochester, Rochester, New York, United States of America"/>
      <meta name="citation_author" content="Steven Quartz"/>
            <meta name="citation_author_institution" content="Computation and Neural Systems, California Institute of Technology, Pasadena, California, United States of America"/>
            <meta name="citation_author_institution" content="Division of Humanities and Social Sciences, California Institute of Technology, Pasadena, California, United States of America"/>
      <meta name="citation_author" content="Joshua B. Tenenbaum"/>
            <meta name="citation_author_institution" content="Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology, Cambridge, Massachusetts, United States of America"/>
      <meta name="citation_author" content="Ladan Shams"/>
            <meta name="citation_author_institution" content="Department of Psychology, University of California at Los Angeles, Los Angeles, California, United States of America"/>

    <meta name="citation_date" content="2007/9/26"/>

  <meta name="citation_pdf_url" content="http://dx.plos.org/10.1371/journal.pone.0000943.pdf" />

      <meta name="citation_journal_title" content="PLOS ONE" />
    <meta name="citation_firstpage" content="e943"/>
    <meta name="citation_issue" content="9"/>
    <meta name="citation_volume" content="2"/>
    <meta name="citation_issn" content="1932-6203"/>

    <meta name="citation_journal_abbrev" content="PLoS ONE" />

      <meta name="citation_reference" content="citation_title=The natural and the normative theories of spatial perception from Kant to Helmholtz.; citation_author=GC Hatfield; citation_number=1; citation_date=1990; citation_publisher=MIT Press; " />
      <meta name="citation_reference" content="citation_title=Certain determinants of the “ventriloquism effect”.; citation_author=WR Thurlow; citation_author=CE Jack; citation_journal_title=Percept Mot Skills; citation_volume=36; citation_number=2; citation_pages=1171-1184; citation_date=1973; " />
      <meta name="citation_reference" content="citation_title=The role of visual-auditory “compellingness” in the ventriloquism effect: implications for transitivity among the spatial senses.; citation_author=DH Warren; citation_author=RB Welch; citation_author=TJ McCarthy; citation_journal_title=Percept Psychophys; citation_volume=30; citation_number=3; citation_pages=557-564; citation_date=1981; " />
      <meta name="citation_reference" content="citation_title=Optimal integration of texture and motion cues to depth.; citation_author=RA Jacobs; citation_journal_title=Vision Res; citation_volume=39; citation_number=4; citation_pages=3621-3629; citation_date=1999; " />
      <meta name="citation_reference" content="citation_title=Integration of proprioceptive and visual position-information: An experimentally supported model.; citation_author=RJ van Beers; citation_author=AC Sittig; citation_author=JJ Gon; citation_journal_title=J Neurophysiol; citation_volume=81; citation_number=5; citation_pages=1355-1364; citation_date=1999; " />
      <meta name="citation_reference" content="citation_title=Humans integrate visual and haptic information in a statistically optimal fashion.; citation_author=MO Ernst; citation_author=MS Banks; citation_journal_title=Nature; citation_volume=415; citation_number=6; citation_pages=429-433; citation_date=2002; " />
      <meta name="citation_reference" content="citation_title=The ventriloquist effect results from near-optimal bimodal integration.; citation_author=D Alais; citation_author=D Burr; citation_journal_title=Curr Biol; citation_volume=14; citation_number=7; citation_pages=257-262; citation_date=2004; " />
      <meta name="citation_reference" content="citation_title=Temporal and spatial dependency of the ventriloquism effect.; citation_author=DA Slutsky; citation_author=GH Recanzone; citation_journal_title=Neuroreport; citation_volume=12; citation_number=8; citation_pages=7-10; citation_date=2001; " />
      <meta name="citation_reference" content="citation_title=Temporal constraints on the McGurk effect.; citation_author=KG Munhall; citation_author=P Gribble; citation_author=L Sacco; citation_author=M Ward; citation_journal_title=Percept Psychophys; citation_volume=58; citation_number=9; citation_pages=351-362; citation_date=1996; " />
      <meta name="citation_reference" content="citation_title=Effects of degree of visual association and angle of displacement on the “ventriloquism” effect.; citation_author=CE Jack; citation_author=WR Thurlow; citation_journal_title=Percept Mot Skills; citation_volume=37; citation_number=10; citation_pages=967-979; citation_date=1973; " />
      <meta name="citation_reference" content="citation_title=Visual-proprioceptive interaction under large amounts of conflict.; citation_author=DH Warren; citation_author=WT Cleaves; citation_journal_title=J Exp Psychol; citation_volume=90; citation_number=11; citation_pages=206-214; citation_date=1971; " />
      <meta name="citation_reference" content="citation_title=The combination of vision and touch depends on spatial proximity.; citation_author=S Gepshtein; citation_author=J Burge; citation_author=MO Ernst; citation_author=MS Banks; citation_journal_title=J Vis; citation_volume=5; citation_number=12; citation_pages=1013-1023; citation_date=2005; " />
      <meta name="citation_reference" content="citation_title=Sound-induced flash illusion as an optimal percept.; citation_author=L Shams; citation_author=WJ Ma; citation_author=U Beierholm; citation_journal_title=Neuroreport; citation_volume=16; citation_number=13; citation_pages=1923-1927; citation_date=2005; " />
      <meta name="citation_reference" content="citation_title=Vision and touch are automatically integrated for the perception of sequences of events.; citation_author=JP Bresciani; citation_author=F Dammeier; citation_author=MO Ernst; citation_journal_title=J Vis; citation_volume=6; citation_number=14; citation_pages=554-564; citation_date=2006; " />
      <meta name="citation_reference" content="citation_title=Unifying multisensory signals across time and space.; citation_author=MT Wallace; citation_author=GE Roberson; citation_author=WD Hairston; citation_author=BE Stein; citation_author=JW Vaughan; citation_journal_title=Exp Brain Res; citation_volume=158; citation_number=15; citation_pages=252-258; citation_date=2004; " />
      <meta name="citation_reference" content="citation_title=Visual localization ability influences cross-modal bias.; citation_author=WD Hairston; citation_author=MT Wallace; citation_author=JW Vaughan; citation_author=BE Stein; citation_author=JL Norris; citation_journal_title=J Cogn Neurosci; citation_volume=15; citation_number=16; citation_pages=20-29; citation_date=2003; " />
      <meta name="citation_reference" content="citation_title=A Bayesian model unifies multisensory spatial localization with the physiological properties of the superior colliculus.; citation_author=B Rowland; citation_author=T Stanford; citation_author=BE Stein; citation_number=17; citation_date=2007; " />
      <meta name="citation_reference" content="citation_title=Resolving multisensory conflict: a strategy for balancing the costs and benefits of audio-visual integration.; citation_author=NW Roach; citation_author=J Heron; citation_author=PV McGraw; citation_journal_title=Proc Biol Sci; citation_volume=273; citation_number=18; citation_pages=2159-2168; citation_date=2006; " />
      <meta name="citation_reference" content="citation_title=Demonstration by Simulation: the Philosophical Significance of Experiment in Helmholtz's Theory of Perception.; citation_author=P McDonald; citation_journal_title=Perspectives on Science; citation_volume=11; citation_number=19; citation_pages=170-207; citation_date=2003; " />
      <meta name="citation_reference" content="citation_title=On the sensations of tone as a physiological basis for the theory of music.; citation_author=HFv Helmholtz; citation_number=20; citation_date=1863 (1954); citation_publisher=Dover Publics; " />
      <meta name="citation_reference" content="citation_title=Learning to integrate arbitrary signals from vision and touch.; citation_author=MO Ernst; citation_number=21; citation_date=2007; " />
      <meta name="citation_reference" content="citation_title=The binding problem.; citation_author=A Treisman; citation_journal_title=Curr Opin Neurobiol; citation_volume=6; citation_number=22; citation_pages=171-178; citation_date=1996; " />
      <meta name="citation_reference" content="citation_title=The role of neural mechanisms of attention in solving the binding problem.; citation_author=JH Reynolds; citation_author=R Desimone; citation_journal_title=Neuron; citation_volume=24; citation_number=23; citation_pages=19-29; citation_date=1999; " />
      <meta name="citation_reference" content="citation_title=Mixture models and the probabilistic structure of depth cues.; citation_author=DC Knill; citation_journal_title=Vision Res; citation_volume=43; citation_number=24; citation_pages=831-854; citation_date=2003; " />
      <meta name="citation_reference" content="citation_title=Auditory influences on visual temporal rate perception.; citation_author=GH Recanzone; citation_journal_title=J Neurophysiol; citation_volume=89; citation_number=25; citation_pages=1078-1093; citation_date=2003; " />
      <meta name="citation_reference" content="citation_title=The “ventriloquist effect”: visual dominance or response bias.; citation_author=CS Choe; citation_author=RB Welch; citation_author=RM Gilford; citation_author=JF Juola; citation_journal_title=Perception and Psychophysics; citation_volume=18; citation_number=26; citation_pages=55-60; citation_date=1975; " />
      <meta name="citation_reference" content="citation_title=Ventriloquism in patients with unilateral visual neglect.; citation_author=P Bertelson; citation_author=F Pavani; citation_author=E Ladavas; citation_author=J Vroomen; citation_author=B de Gelder; citation_journal_title=Neuropsychologia; citation_volume=38; citation_number=27; citation_pages=1634-1642; citation_date=2000; " />
      <meta name="citation_reference" content="citation_title=Model Selection and Multimodel Inference: A Practical-Theoretic Approach.; citation_author=KP Burnham; citation_author=DR Anderson; citation_number=28; citation_date=2002; citation_publisher=Springer; " />
      <meta name="citation_reference" content="citation_title=Experience can change the ‘light-from-above’ prior.; citation_author=WJ Adams; citation_author=EW Graf; citation_author=MO Ernst; citation_journal_title=Nat Neurosci; citation_volume=7; citation_number=29; citation_pages=1057-1058; citation_date=2004; " />
      <meta name="citation_reference" content="citation_title=The binding problem.; citation_author=AL Roskies; citation_journal_title=Neuron; citation_volume=24; citation_number=30; citation_pages=7-9; citation_date=1999; " />
      <meta name="citation_reference" content="citation_title=The Spandrels of San Marco and the Panglossian Paradigm: A Critique of the Adaptionist Programme.; citation_author=SJ Gould; citation_author=R Lewontin; citation_journal_title=Proceedings of the Royal Society B; citation_volume=205; citation_number=31; citation_pages=581-598; citation_date=1979; " />
      <meta name="citation_reference" content="citation_title=The adaptive toolbox: toward a darwinian rationality.; citation_author=G Gigerenzer; citation_journal_title=Nebr Symp Motiv; citation_volume=47; citation_number=32; citation_pages=113-143; citation_date=2001; " />
      <meta name="citation_reference" content="citation_title=Probabilistic interpretation of population codes.; citation_author=RS Zemel; citation_author=P Dayan; citation_author=A Pouget; citation_journal_title=Neural Comput; citation_volume=10; citation_number=33; citation_pages=403-430; citation_date=1998; " />
      <meta name="citation_reference" content="citation_title=Inference and computation with population codes.; citation_author=A Pouget; citation_author=P Dayan; citation_author=RS Zemel; citation_journal_title=Annu Rev Neurosci; citation_volume=26; citation_number=34; citation_pages=381-410; citation_date=2003; " />
      <meta name="citation_reference" content="citation_title=Bayesian inference with probabilistic population codes.; citation_author=WJ Ma; citation_author=JM Beck; citation_author=PE Latham; citation_author=A Pouget; citation_journal_title=Nat Neurosci; citation_volume=9; citation_number=35; citation_pages=1432-1438; citation_date=2006; " />
      <meta name="citation_reference" content="citation_title=From covariation to causation: a test of the assumption of causal power.; citation_author=MJ Buehner; citation_author=PW Cheng; citation_author=D Clifford; citation_journal_title=J Exp Psychol Learn Mem Cogn; citation_volume=29; citation_number=36; citation_pages=1119-1140; citation_date=2003; " />
      <meta name="citation_reference" content="citation_title=A theory of causal learning in children: causal maps and Bayes nets.; citation_author=A Gopnik; citation_author=C Glymour; citation_author=DM Sobel; citation_author=LE Schulz; citation_author=T Kushnir; citation_journal_title=Psychol Rev; citation_volume=111; citation_number=37; citation_pages=3-32; citation_date=2004; " />
      <meta name="citation_reference" content="citation_title=Secret agents: inferences about hidden causes by 10- and 12-month-old infants.; citation_author=R Saxe; citation_author=JB Tenenbaum; citation_author=S Carey; citation_journal_title=Psychol Sci; citation_volume=16; citation_number=38; citation_pages=995-1001; citation_date=2005; " />
      <meta name="citation_reference" content="citation_title=Structure and strength in causal induction.; citation_author=TL Griffiths; citation_author=JB Tenenbaum; citation_journal_title=Cognit Psychol; citation_volume=51; citation_number=39; citation_pages=334-384; citation_date=2005; " />
      <meta name="citation_reference" content="citation_title=Competition among causes but not effects in predictive and diagnostic learning.; citation_author=MR Waldmann; citation_journal_title=J Exp Psychol Learn Mem Cogn; citation_volume=26; citation_number=40; citation_pages=53-76; citation_date=2000; " />
      <meta name="citation_reference" content="citation_title=Theory-based Bayesian models of inductive learning and reasoning.; citation_author=JB Tenenbaum; citation_author=TL Griffiths; citation_author=C Kemp; citation_journal_title=Trends Cogn Sci; citation_volume=10; citation_number=41; citation_pages=309-318; citation_date=2006; " />
      <meta name="citation_reference" content="citation_title=Children's causal inferences from indirect evidence: Backwards blocking and Bayesian reasoning in preschoolers.; citation_author=D Sobel; citation_author=JB Tenenbaum; citation_author=A Gopnik; citation_journal_title=Cognitive Science; citation_volume=28; citation_number=42; citation_pages=303-333; citation_date=2004; " />
      <meta name="citation_reference" content="citation_title=The loss function of sensorimotor learning.; citation_author=KP Kording; citation_author=DM Wolpert; citation_journal_title=Proc Natl Acad Sci U S A; citation_volume=101; citation_number=43; citation_pages=9839-9842; citation_date=2004; " />
      <meta name="citation_reference" content="citation_title=Computational and psychophysics of sensorimotor integration.; citation_author=Z Ghahramani; citation_number=44; citation_date=1995; citation_publisher=Massachusetts Institute of Technology; " />
      <meta name="citation_reference" content="citation_title=Memory deficits associated with senescence: a neurophysiological and behavioral study in the rat.; citation_author=CA Barnes; citation_journal_title=J Comp Physiol Psychol; citation_volume=93; citation_number=45; citation_pages=74-104; citation_date=1979; " />

  <link rel="canonical" href="http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0000943" />

    <meta name="twitter:card" content="summary"/>
    <meta name="twitter:site" content="@plosone"/>
    <meta name="twitter:title" content="Causal Inference in Multisensory Perception"/>
    <meta name="twitter:description" content="Perceptual events derive their significance to an animal from their meaning about the world, that is from the information they carry about their causes. The brain should thus be able to efficiently infer the causes underlying our sensory events. Here we use multisensory cue combination to study causal inference in perception. We formulate an ideal-observer model that infers whether two sensory cues originate from the same location and that also estimates their location(s). This model accurately predicts the nonlinear integration of cues by human subjects in two auditory-visual localization tasks. The results show that indeed humans can efficiently infer the causal structure as well as the location of causes. By combining insights from the study of causal inference with the ideal-observer approach to sensory cue combination, we show that the capacity to infer causal structure is not limited to conscious, high-level cognition; it is also performed continually and effortlessly in perception."/>
      <meta name="twitter:image" content="http://dx.plos.org/10.1371/journal.pone.0000943.g003"/>

  <meta property="og:title" content="Causal Inference in Multisensory Perception" />
  <meta property="og:type" content="article" />
  <meta property="og:url" content="http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0000943" />

 <!--end articleInfoX-->

  <link rel="pingback" href="http://www.plosone.org/pingback" />


  <link rel="shortcut icon" href="/images/favicon.ico" type="image/x-icon"/>
  <link rel="home" title="home" href="/"/>
  <link rel="alternate" type="application/rss+xml"
        title="PLOS ONE: New Articles"
        href="http://www.plosone.org/article/feed"/>
</head>
<body>

  <div id="page-wrap">
    <div id="topbanner" class="cf">

<!-- Div for the ad at the top of journal home page-->
<div class="center">
  <div class="title">Advertisement</div>
  <iframe id='a3ac9da4' name='a3ac9da4'
    src='http://ads.plos.org/www/delivery/afr.php?zoneid=345&amp;cb=3269'
    frameborder='0' scrolling='no' width='730' height='90'>
    <a href='http://ads.plos.org/www/delivery/ck.php?n=a3ac9da4&amp;cb=8935'
      target='_top'><img src='http://ads.plos.org/www/delivery/avw.php?zoneid=345&amp;cb=2473&amp;n=a3ac9da4'
      border='0' alt=''/>
    </a>
  </iframe>
</div>    </div>

    <div id="pagehdr-wrap">
      <div id="pagehdr">
        <div id="user" class="nav">
          <ul>
            <li><a href="http://www.plos.org">plos.org</a></li>
            <li><a href="https://register.plos.org/ambra-registration/register.action">create account</a></li>
            <li class="btn-style"><a
              href="/user/secure/secureRedirect.action?goTo=%2Farticle%2FfetchArticle.action%3FarticleURI%3Dinfo%253Adoi%252F10.1371%252Fjournal.pone.0000943">sign in</a>
            </li>
          </ul>
        </div>
        <div class="logo">
          <a href="/"><img src="/images/logo.png" alt="PLOS ONE"></a>
        </div>

<div id="nav-main" class="nav">
  <ul>
        <li id="mn-01"><a href="/taxonomy" class="areas-link">Subject Areas</a></li>
    <li id="mn-02"><a href="javascript:void(0);">For Authors</a>
      <div class="submenu" style="width: 540px; margin-left: -300px;">
        <div class="block">
          <div class="submit-script">
            <h3>Submit your Manuscript</h3>
            <ul>
              <li>Fair, rigorous peer review</li>
              <li>Broad scope and wide reach</li>
            </ul>
            <a href="/static/submissionInstructions" class="btn">get started</a>
          </div>
        </div>
        <div class="menu">
          <ul>
            <li><a href="/static/publish">Why Publish with PLOS ONE</a></li>
            <li><a href="/static/publication">Publication Criteria</a></li>
            <li><a href="/static/editorial">Editorial Policies</a></li>
            <li><a href="/static/guidelines">Preparing A Manuscript</a></li>
            <li><a href="/static/figureGuidelines">Figure and Table Guidelines</a></li>
          <li><a href="/static/supportingInformation">Supporting Information Guidelines</a></li>
            <li><a href="/static/submissionInstructions">Submitting a Manuscript</a></li>
          </ul>
        </div>
      </div>
    </li>

    <li id="mn-03"><a href="javascript:void(0);">About Us</a>
      <div class="submenu" style="width:248px; margin-left:-30px;">
        <div class="menu">
          <ul>
            <li><a href="/static/information">Journal Information</a></li>
            <li><a href="/static/edboard">Editorial Board</a></li>
            <li><a href="/static/reviewerGuidelines">Reviewer Guidelines</a></li>
            <li><a href="/static/almInfo">Article-Level Metrics</a></li>
            <li><a href="/static/license">Open-Access License</a></li>
            <li><a href="/static/downloads">Media Downloads</a></li>
            <li><a href="/static/commentGuidelines">Guidelines for Comments</a></li>
            <li><a href="/static/corrections">Corrections</a></li>
            <li><a href="/static/help">Help Using this Site</a></li>
            <li><a href="/static/contact">Contact Us</a></li>
          </ul>
        </div>
      </div>
    </li>
  </ul>
<div id="db">
  <form name="searchForm" action="/search/simple?noSearchFlag=true&amp;query=&amp;articleURI=info%3Adoi%2F10.1371%2Fjournal.pone.0000943" method="get" >
<input type="hidden" name="from" value="globalSimpleSearch" id="from"/><input type="hidden" name="filterJournals" value="PLoSONE" id="filterJournals"/>    <fieldset>
      <legend>Search</legend>
      <label for="search">Search</label>
      <div class="wrap">
        <input id="search" type="text" name="query" placeholder="Search">
        <input type="image" alt="SEARCH" src="/images/icon.search.gif">
      </div>
    </fieldset>
  </form>
    <a id="advSearch" href="/search/advanced?noSearchFlag=true&amp;query=&amp;articleURI=info%3Adoi%2F10.1371%2Fjournal.pone.0000943&filterJournals=PLoSONE">advanced search</a>
</div></div>

      </div>
      <!-- pagehdr-->
    </div>
    <!-- pagehdr-wrap -->

  <!--body and html tags gets closed in global_footer.ftl-->
<script type="text/javascript" src="/javascript/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<div id="pagebdy-wrap">
  <div id="pagebdy">

    <div id="article-block" class="cf">

<div class="article-meta cf">
  <ul id="almSignPost" style="display: none;"></ul>
  <div class="article-type">
    <span class="type oa">Open Access</span>
      <span class="type pr">Peer-Reviewed</span>
  </div>
</div>

<div class="header" id="hdr-article">

<div class="article-kicker">
      <span id="article-type-heading">
        Research Article
      </span>
</div>  <h1 property="dc:title" datatype="" rel="dc:type" href="http://purl.org/dc/dcmitype/Text">
    Causal Inference in Multisensory Perception
  </h1>

  <ul class="authors">
      <li>


        <span rel="dc:creator" class="author">
          <span class="person" property="foaf:name" typeof="foaf:Person">
            Konrad P. Körding
              <span class="equal-contrib"
                    title="These authors contributed equally to this work">equal contributor</span>, 
          </span>
        </span>

          <div class="author_meta">
            <div class="author_inner">

                <p><span class="equal-contrib" title="These authors contributed equally to this work">equal contributor</span>
                Contributed equally to this work with: Konrad P. Körding, Ulrik Beierholm, Wei Ji Ma</p>

              
              

                <p>Affiliation:
                  Rehabilitation Institute of Chicago, Northwestern University, Chicago, Illinois, United States of America
                </p>


              <span class="close">X</span>

            </div>
          </div>
      </li>
      <li>


        <span rel="dc:creator" class="author">
          <span class="person" property="foaf:name" typeof="foaf:Person">
            Ulrik Beierholm
              <span class="equal-contrib"
                    title="These authors contributed equally to this work">equal contributor</span>
              <span class="corresponding">mail</span>, 
          </span>
        </span>

          <div class="author_meta">
            <div class="author_inner">

                <p><span class="equal-contrib" title="These authors contributed equally to this work">equal contributor</span>
                Contributed equally to this work with: Konrad P. Körding, Ulrik Beierholm, Wei Ji Ma</p>

              
              <p><span class="email">*</span>To whom correspondence should be addressed. E-mail: <a href="mailto:beierh@caltech.edu">beierh@caltech.edu</a></p>

                <p>Affiliation:
                  Computation and Neural Systems, California Institute of Technology, Pasadena, California, United States of America
                </p>


              <span class="close">X</span>

            </div>
          </div>
      </li>
      <li>


        <span rel="dc:creator" class="author">
          <span class="person" property="foaf:name" typeof="foaf:Person">
            Wei Ji Ma
              <span class="equal-contrib"
                    title="These authors contributed equally to this work">equal contributor</span>, 
          </span>
        </span>

          <div class="author_meta">
            <div class="author_inner">

                <p><span class="equal-contrib" title="These authors contributed equally to this work">equal contributor</span>
                Contributed equally to this work with: Konrad P. Körding, Ulrik Beierholm, Wei Ji Ma</p>

              
              

                <p>Affiliation:
                  Department of Brain and Cognitive Sciences, University of Rochester, Rochester, New York, United States of America
                </p>


              <span class="close">X</span>

            </div>
          </div>
      </li>
      <li>


        <span rel="dc:creator" class="author">
          <span class="person" property="foaf:name" typeof="foaf:Person">
            Steven Quartz, 
          </span>
        </span>

          <div class="author_meta">
            <div class="author_inner">


              
              

                <p>Affiliations:
                  Computation and Neural Systems, California Institute of Technology, Pasadena, California, United States of America, 
                  Division of Humanities and Social Sciences, California Institute of Technology, Pasadena, California, United States of America
                </p>


              <span class="close">X</span>

            </div>
          </div>
      </li>
      <li>


        <span rel="dc:creator" class="author">
          <span class="person" property="foaf:name" typeof="foaf:Person">
            Joshua B. Tenenbaum, 
          </span>
        </span>

          <div class="author_meta">
            <div class="author_inner">


              
              

                <p>Affiliation:
                  Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology, Cambridge, Massachusetts, United States of America
                </p>


              <span class="close">X</span>

            </div>
          </div>
      </li>
      <li>


        <span rel="dc:creator" class="author">
          <span class="person" property="foaf:name" typeof="foaf:Person">
            Ladan Shams
          </span>
        </span>

          <div class="author_meta">
            <div class="author_inner">


              
              

                <p>Affiliation:
                  Department of Psychology, University of California at Los Angeles, Los Angeles, California, United States of America
                </p>


              <span class="close">X</span>

            </div>
          </div>
      </li>
  </ul>
  <ul class="date-doi-line">
    <li>Published: September 26, 2007</li>
    <li>DOI: 10.1371/journal.pone.0000943</li>
  </ul>


</div><!--end header-->
<div class="main cf" id="pjax-container">
  

<div class="nav items-5" id="nav-article">
  <ul>
  <li>
        <span class="active" name="article">Article</span>
  </li>
  <li>
      <a href="/article/authors/info%3Adoi%2F10.1371%2Fjournal.pone.0000943" name="authors">About the Authors</a>
  </li>
  <li>
      <a href="/article/metrics/info%3Adoi%2F10.1371%2Fjournal.pone.0000943" name="metrics">Metrics</a>
  </li>
  <li>
      <a href="/article/comments/info%3Adoi%2F10.1371%2Fjournal.pone.0000943" name="comments">Comments</a>
  </li>
  <li>
      <a href="/article/related/info%3Adoi%2F10.1371%2Fjournal.pone.0000943" name="related">Related Content</a>
  </li>
  </ul>
</div>

<script type="text/javascript">
  var selected_tab = "article";
</script>
  <div id="figure-thmbs" class="carousel cf">
    <div class="wrapper">
      <div class="slider">
              <div class="item">
                <a href="#pone-0000943-g001" data-doi="info:doi/10.1371/journal.pone.0000943" data-uri="info:doi/10.1371/journal.pone.0000943.g001" title="Figure 1">
                  <span class="thmb-wrap">
                    <img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000943.g001&representation=PNG_I" alt="">
                  </span>
                </a>
              </div>
              <div class="item">
                <a href="#pone-0000943-g002" data-doi="info:doi/10.1371/journal.pone.0000943" data-uri="info:doi/10.1371/journal.pone.0000943.g002" title="Figure 2">
                  <span class="thmb-wrap">
                    <img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000943.g002&representation=PNG_I" alt="">
                  </span>
                </a>
              </div>
              <div class="item">
                <a href="#pone-0000943-t001" data-doi="info:doi/10.1371/journal.pone.0000943" data-uri="info:doi/10.1371/journal.pone.0000943.t001" title="Table 1">
                  <span class="thmb-wrap">
                    <img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000943.t001&representation=PNG_I" alt="">
                  </span>
                </a>
              </div>
              <div class="item">
                <a href="#pone-0000943-g003" data-doi="info:doi/10.1371/journal.pone.0000943" data-uri="info:doi/10.1371/journal.pone.0000943.g003" title="Figure 3">
                  <span class="thmb-wrap">
                    <img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000943.g003&representation=PNG_I" alt="">
                  </span>
                </a>
              </div>
      </div>
    </div>
  </div>

  <div class="nav-col">
    <div class="nav" id="nav-article-page">
      <ul>
        <li class="nav-col-comments"><a href="/article/comments/info%3Adoi%2F10.1371%2Fjournal.pone.0000943">Reader Comments (4)</a></li>
          <li id="nav-figures"><a data-doi="info:doi/10.1371/journal.pone.0000943" >Figures</a></li>
      </ul>
    </div>
  </div>

  <div class="article">







<div class="abstract"><a id="abstract0" name="abstract0" toc="abstract0" title="Abstract"></a><h2>Abstract</h2><a id="article1.front1.article-meta1.abstract1.p1" name="article1.front1.article-meta1.abstract1.p1"></a><p>Perceptual events derive their significance to an animal from their meaning about the world, that is from the information they carry about their causes. The brain should thus be able to efficiently infer the causes underlying our sensory events. Here we use multisensory cue combination to study causal inference in perception. We formulate an ideal-observer model that infers whether two sensory cues originate from the same location and that also estimates their location(s). This model accurately predicts the nonlinear integration of cues by human subjects in two auditory-visual localization tasks. The results show that indeed humans can efficiently infer the causal structure as well as the location of causes. By combining insights from the study of causal inference with the ideal-observer approach to sensory cue combination, we show that the capacity to infer causal structure is not limited to conscious, high-level cognition; it is also performed continually and effortlessly in perception.</p>
</div>


<div class="articleinfo"><p><strong>Citation: </strong>Körding KP, Beierholm U, Ma WJ, Quartz S, Tenenbaum JB, et al.  (2007) Causal Inference in Multisensory Perception. PLoS ONE 2(9):
          e943.
            doi:10.1371/journal.pone.0000943</p><p><strong>Academic Editor: </strong>Olaf Sporns, Indiana University, United States of America</p><p><strong>Received:</strong> June 15, 2007; <strong>Accepted:</strong> September 3, 2007; <strong>Published:</strong> September 26, 2007</p><p><strong>Copyright:</strong> © 2007 Körding et al. This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p><p><strong>Funding: </strong>KPK was supported by a DFG Heisenberg Stipend. UB and SQ were supported by the David and Lucille Packard Foundation as well as by the Moore Foundation. JBT was supported by the P. E. Newton Career Development Chair. LS was supported by UCLA Academic senate and Career development grants.</p><p><strong>Competing interests:</strong> The authors have declared that no competing interests exist.</p></div>





<div id="section1" class="section"><a id="s1" name="s1" toc="s1" title="Introduction"></a><h3>Introduction</h3><a id="article1.body1.sec1.p1" name="article1.body1.sec1.p1"></a><p>Imagine you are walking in the forest and you see a sudden movement in the bushes. You may infer that this movement was caused by a hidden animal, but you may also consider a gust of wind as an alternative and possibly more probable cause. If you are a hungry predator–or a life-loving prey–this estimation may be critical to your survival. However, you may also hear an animal vocalization coming from a similar direction. Combining both pieces of sensory information, you will be better at judging if there is an animal in the bushes and if so, where exactly it is hiding. Importantly, the way how you will combine pieces of information must depend on the causal relationships you inferred. This example illustrates that perceptual cues are seldom ecologically relevant by themselves, but rather acquire their significance through their meaning about their causes. It also illustrates how cues from multiple sensory modalities can be used to infer underlying causes. The nervous system is constantly engaged in combining uncertain information from different sensory modalities into an integrated understanding of the causes of sensory stimulation.</p>
<a id="article1.body1.sec1.p2" name="article1.body1.sec1.p2"></a><p>The study of multisensory integration has a long and fruitful history in experimental psychology, neurophysiology, and psychophysics. Von Helmholtz, in the late 19<sup>th</sup> century started considering cue combination, formalizing perception as unconscious probabilistic inference of a best guess of the state of the world <a href="#pone.0000943-Hatfield1">[1]</a>. Since then, numerous studies have analyzed the way people use and combine cues for perception <a href="#pone.0000943-Thurlow1">[e.g. 2]</a>, <a href="#pone.0000943-Warren1">[3]</a>, highlighting the rich set of effects that occur in multimodal perception.</p>
<a id="article1.body1.sec1.p3" name="article1.body1.sec1.p3"></a><p>Over the last decade, many scientists have gone back to a probabilistic interpretation of cue combination as had been proposed by von Helmholtz. These probabilistic models formalize the problem of cue combination in an elegant way. It is assumed that there is a single variable in the outside world (e.g., the position of an animal) that is causing the cues (auditory and visual information). Each of the cues is assumed to be a noisy observation of this underlying variable. Due to noise in sensation, there is some uncertainty about the information conveyed by each cue and Bayesian statistics is the systematic way of predicting how subjects could optimally infer the underlying variables from the cues. Several recent studies have demonstrated impressive fits to psychophysical data, starting from the assumption that human performance is close to the ideal defined by probabilistic models <a href="#pone.0000943-Jacobs1">[4]</a>–<a href="#pone.0000943-Alais1">[7]</a>. In these experiments, cues tend to be close to each other in time, space, and structure, providing strong evidence for there only being a single cause for both cues. In situations where there is only a single underlying cause, these models formalize the central idea of probabilistic inference of a hidden cause.</p>
<a id="article1.body1.sec1.p4" name="article1.body1.sec1.p4"></a><p>A range of experiments have shown effects that are hard to reconcile with the single-cause (i.e., forced-fusion) idea. Auditory-visual integration breaks down when the difference between the presentation of the visual and the auditory stimulus is large <a href="#pone.0000943-Slutsky1">[8]</a>–<a href="#pone.0000943-Jack1">[10]</a>. Such a distance or inconsistency is called disparity. Increasing disparity, for example by moving an auditory stimulus farther away from the position of a visual stimulus, reduces the influence each stimulus has on the perception of the other <a href="#pone.0000943-Warren2">[11]</a>–<a href="#pone.0000943-Bresciani1">[14]</a>. Throughout this paper we only consider spatial disparity along the azimuthal axis. When subjects are asked to report their percepts in both modalities <em>on the same trial</em>, one can measure the influence that the two senses have on each other <a href="#pone.0000943-Shams1">[13]</a>. The data from such a dual-report paradigm show that, although at small disparities there is a tendency to integrate, greater disparity makes it more likely that a subject responds differently in both modalities. Moreover, when people are simply asked whether they perceive a single cue or several cues they give answers that intuitively make a lot of sense: if two events are close to each other in space, time, and structure, subjects tend to perceive a single underlying cause, while if they are far away from one another subjects tend to infer two independent causes <a href="#pone.0000943-Wallace1">[15]</a>, <a href="#pone.0000943-Hairston1">[16]</a>. If cues are close to one another, they interact and influence the perception of each other, whereas they are processed independently when the discrepancy is large.</p>
<a id="article1.body1.sec1.p5" name="article1.body1.sec1.p5"></a><p>New modeling efforts have made significant progress at formalizing the interactions between two cues. These models assume that there exist <em>two</em> relevant variables in the world, for example the position of a visual and the position of an auditory stimulus. The visual and auditory cues that reach the nervous system are noisy versions of the underlying visual and auditory variables. The models further assume an “interaction prior”, a joint prior distribution that defines how likely each combination of visual and auditory stimuli is in the absence of any evidence. This prior formalizes that the probability of both positions being the same (related to a common cause) is high in comparison to the positions being different from one another. This prior in effect determines the way in which two modalities influence each other. Very good fits to human performance have been shown for the combination of two cues <a href="#pone.0000943-Shams1">[13]</a>, <a href="#pone.0000943-Bresciani1">[14]</a>, <a href="#pone.0000943-Rowland1">[17]</a>, <a href="#pone.0000943-Roach1">[18]</a>. These studies assume an interaction between processing in each modality and derive predictions of human performance from this idea.</p>
<a id="article1.body1.sec1.p6" name="article1.body1.sec1.p6"></a><p>Von Helmholtz did not only stress the issue of probabilistic inference but also that multiple objects may be the causes of our sensations <a href="#pone.0000943-McDonald1">[19]</a>, <a href="#pone.0000943-Helmholtz1">[20]</a>. In other words, any two sensory signals may either have a common cause, in which case they should be integrated, or have different, independent causes, in which case they should be processed separately. Further evidence for this idea comes from a study that showed that by providing evidence that two signals are related it is possible to incite subjects to more strongly combine two cues <a href="#pone.0000943-Ernst2">[21]</a>. The within-modality binding problem is another example where causal inference is necessary and the nervous system has to determine which set of stimuli correspond to the same object and should be bound together <a href="#pone.0000943-Treisman1">[22]</a>–<a href="#pone.0000943-Knill1">[24]</a>. We are usually surrounded by many sights, sounds, odors, and tactile stimuli, and our nervous system constantly needs to estimate which signals have a common cause. The nervous system frequently needs to solve problems where it needs to interpret sensory signals in terms of potential causes.</p>
<a id="article1.body1.sec1.p7" name="article1.body1.sec1.p7"></a><p>In this paper we formalize the problem of causal inference as well as integration versus segregation in multisensory perception as an optimal Bayesian observer that not only infers source location from two sensory signals (visual, s<em><sub>V</sub></em>, and auditory, s<em><sub>A</sub></em>) but also whether the signals have a common cause (<em>C</em>). This inference is complicated by the fact that the nervous system does not have access to the source locations of the signals but only to noisy measurements thereof (visual, <em>x<sub>V</sub></em>, and auditory, <em>x<sub>A</sub></em>). From these noisy observations it needs to infer the best estimates of the source locations (<em>Ŝ<sub>V</sub></em> and <em>Ŝ<sub>A</sub></em>). All this needs to happen in the presence of uncertainnty about the presence of a common cause (<em>C</em>). To take into account multiple possible causal structures, we need a so-called mixture model <a href="#pone.0000943-Knill1">[e.g. 24]</a>, but one of a very specific form.</p>
<a id="article1.body1.sec1.p8" name="article1.body1.sec1.p8"></a><p>The model assumes that the underlying variables (azimuthal stimulus positions) cause the sensory inputs.&nbsp;The model considers two hypotheses, either that there is a common cause or that there are independent causes. The optimal observer model defines how the cues might actually be combined (i.e., in a statistically optimal manner).&nbsp;In the model, cues are fused if the cues have one common cause and segregated if they have independent causes. The model typically has uncertainty about the causal interpretation, in which case it will adjust its cue combination continuously depending on the degree of belief about the causal structure.</p>
<a id="article1.body1.sec1.p9" name="article1.body1.sec1.p9"></a><p>This model makes three important predictions: (1) It predicts the circumstances under which subjects should perceive a common cause or independent causes. (2) It predicts if the individual cues should be fused or if they should be processed separately. (3) It predicts how the cues are combined if they are combined. Here we test the predictions of the model and analyze how well it predicts human behavior.</p>
</div>

<div id="section2" class="section"><a id="s2" name="s2" toc="s2" title="Results"></a><h3>Results</h3>
<h4>Causal Bayesian inference</h4>
<a id="article1.body1.sec2.sec1.p1" name="article1.body1.sec2.sec1.p1"></a><p>We model situations in which observers are presented with simultaneous auditory and visual stimuli, and are asked to report their location(s). If the visual and the auditory stimuli have a common cause (<a href="#pone-0000943-g001">Fig. 1</a>, left), subjects could use the visual cue to improve the auditory estimate, and vice versa. However, in the real world we are usually surrounded by multiple sources of sensory stimulation and hence multiple sights and sounds. Therefore the nervous system cannot simply combine all signals into a joint estimate; it must infer which signals have a common cause and only integrate those. Specifically, for any pair of visual and auditory stimuli, it should also consider the alternative possibility that they are unrelated and co-occurred randomly (<a href="#pone-0000943-g001">Fig. 1</a>, right).</p>
<div class="figure" id="pone-0000943-g001"><div class="img"><a name="pone-0000943-g001" title="Click for larger image " href="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000943.g001&amp;representation=PNG_M" data-doi="info:doi/10.1371/journal.pone.0000943" data-uri="info:doi/10.1371/journal.pone.0000943.g001"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000943.g001&amp;representation=PNG_I" alt="thumbnail" class="thumbnail"></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000943.g001/powerpoint">
                    PPT
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000943.g001/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000943.g001/largerimage">
                    PNG
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000943.g001/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pone.0000943.g001.PNG_L"></span>)
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000943.g001/originalimage">
                    TIFF
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000943.g001/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pone.0000943.g001.TIF"></span>)
                </a></li></ul></div><p><strong>Figure 1.  <span>The causal inference model.</span></strong></p><a id="article1.body1.sec2.sec1.fig1.caption1.p1" name="article1.body1.sec2.sec1.fig1.caption1.p1"></a><p>Left: One cause can be responsible for both cues. In this case the visually perceived position <em>x<sub>V</sub></em> will be the common position <em>s</em> perturbed by visual noise with width <em>σ<sub>V</sub></em> and the auditory perceived position will be the common position perturbed by auditory noise with width <em>σ<sub>A</sub></em>. Right: Alternatively, two distinct causes may be relevant, decoupling the problem into two independent estimation problems. The causal inference model infers the probability of a causal structure with a common cause (left, <em>C</em> = 1) versus the causal structure with two independent causes (right, <em>C</em> = 2) and then derives optimal predictions from this. We introduce a single variable <em>C</em> which determines which sub-model generates the data.</p>
<span>doi:10.1371/journal.pone.0000943.g001</span></div><a id="article1.body1.sec2.sec1.p2" name="article1.body1.sec2.sec1.p2"></a><p>Here we developed an ideal observer that estimates the positions of cues and also whether they have a common cause. This <em>causal inference model</em> uses two pieces of information. One piece is the likelihood: the sensed visual and auditory positions, which are corrupted by noise. Because perception is corrupted by noise, any sensory stimulus does not reveal the true visual position, but rather induces a distribution of where the stimulus could be, given the stimulus. The other piece of information is the prior: from experience we may know how likely two co-occurring signals are to have a common cause versus two independent causes. The causal inference model combines those pieces of information to estimate if there is a common cause and to estimate the positions of cues (see the <a href="#s4">Methods</a> section and Supporting Information for details, <a href="#pone.0000943.s001">Text S1</a>).</p>
<a id="article1.body1.sec2.sec1.p3" name="article1.body1.sec2.sec1.p3"></a><p>The causal inference model depends on four parameters characterizing the knowledge about the environment and the observer's sensory systems: the uncertainty of vision (<em>σ<sub>V</sub></em>) and audition (<em>σ<sub>A</sub></em>); knowledge the observer has about the spatial layout of objects, in particular how much the observer expects that objects are more likely to be located centrally (<em>σ<sub>P</sub></em>, introduced to formalize that subjects have a bias to perceive stimuli straight ahead); and the prior probability that there is a single cause versus two causes (<em>p</em><sub>common</sub>). These four parameters are fit to human behavior in psychophysical experiments (see <a href="#s4">Methods</a> for details).</p>


<h4>Experiment 1: Auditory-visual spatial localization</h4>
<a id="article1.body1.sec2.sec2.p1" name="article1.body1.sec2.sec2.p1"></a><p>Experienced ventriloquists move a puppet's mouth in synchrony with their speech patterns, creating a powerful illusion of a talking puppet. This effect is a classical demonstration of auditory-visual integration, where subjects infer that there is only a single cause (the puppet's talking) for both visual (puppet's facial movements) and auditory (speech) stimuli. Numerous experimental studies have analyzed this kind of auditory-visual cue integration and found situations in which the cues are combined and situations in which they are processed separately <a href="#pone.0000943-Thurlow1">[2]</a>, <a href="#pone.0000943-Warren1">[3]</a>, <a href="#pone.0000943-Alais1">[7]</a>, <a href="#pone.0000943-Slutsky1">[8]</a>, <a href="#pone.0000943-Jack1">[10]</a>, <a href="#pone.0000943-Wallace1">[15]</a>, <a href="#pone.0000943-Recanzone1">[25]</a>–<a href="#pone.0000943-Bertelson1">[27]</a>. To test the causal inference model, we use a laboratory version of the ventriloquist illusion, in which brief auditory and visual stimuli are presented simultaneously with varying amounts of spatial disparity. We use the dual-report paradigm which was introduced recently to study auditory-visual numerosity judgment <a href="#pone.0000943-Shams1">[13]</a>, because this provides information about the joint auditory-visual percepts of subjects.</p>
<a id="article1.body1.sec2.sec2.p2" name="article1.body1.sec2.sec2.p2"></a><p>Nineteen subjects participated in the experiment. On each trial, subjects were presented with either a brief visual stimulus in one of five locations along azimuth, or a brief tone at one of the same five locations, or both simultaneously (see <a href="#s4">Methods</a> for details). The task was to report the location of the visual stimulus as well as the location of the sound in each trial using two button presses (<a href="#pone-0000943-g002">Fig. 2a</a>).</p>
<div class="figure" id="pone-0000943-g002"><div class="img"><a name="pone-0000943-g002" title="Click for larger image " href="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000943.g002&amp;representation=PNG_M" data-doi="info:doi/10.1371/journal.pone.0000943" data-uri="info:doi/10.1371/journal.pone.0000943.g002"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000943.g002&amp;representation=PNG_I" alt="thumbnail" class="thumbnail"></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000943.g002/powerpoint">
                    PPT
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000943.g002/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000943.g002/largerimage">
                    PNG
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000943.g002/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pone.0000943.g002.PNG_L"></span>)
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000943.g002/originalimage">
                    TIFF
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000943.g002/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pone.0000943.g002.TIF"></span>)
                </a></li></ul></div><p><strong>Figure 2.  <span>Combination of visual and auditory cues.</span></strong></p><a id="article1.body1.sec2.sec2.fig1.caption1.p1" name="article1.body1.sec2.sec2.fig1.caption1.p1"></a><p>a) The experimental paradigm is shown. In each trial a visual and an auditory stimulus is presented simultaneously and subjects report both the position of the perceived visual and the position of the perceived auditory stimuli by button presses. b) The influence of vision on the perceived position of an auditory stimulus in the center is shown. Different colors correspond to the visual stimulus at different locations (sketched in warm to cold colors from the left to the right). The unimodal auditory case is shown in gray. c) The averaged responses of the subjects (solid lines) are shown along with the predictions of the ideal observer (broken lines) for each of the 35 stimulus conditions. These plot show how often on average which button was pressed in each of the conditions. d) The model responses from c) are plotted with the human responses from c). e) The average auditory bias <span class="inline-formula"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000943.e001&amp;representation=PNG" class="inline-graphic"></span>, i.e. the influence of deviations of the visual position on the perceived auditory position is shown as a function of the spatial disparity (solid line) along with the model prediction (dashed line).</p>
<span>doi:10.1371/journal.pone.0000943.g002</span></div><a id="article1.body1.sec2.sec2.p3" name="article1.body1.sec2.sec2.p3"></a><p>We found that subjects show large variability across trials, shown in <a href="#pone-0000943-g002">Fig 2b</a> for an auditory stimulus at the central location. If subjects would not be affected by noise then we would expect a plot that has 100% of the trials as a press of the button corresponding to the central location. Instead we see a wide distribution, highlighting the presence of uncertainty in auditory perception. All modern theories of cue combination predict that two cues, presented simultaneously, will influence one another and lead to a bimodal precision that is better than the unimodal precision, as the other cue can be used to reduce uncertainty. Indeed, we found that the visual stimulus influences the estimate of the auditory stimulus when the auditory stimulus is held at a fixed location (<a href="#pone-0000943-g002">Fig. 2b</a>, yellow versus gray). Moreover, we find that vision does have an influence on the perception of the auditory stimulus and a visual stimulus to the left biases perception to the left (<a href="#pone-0000943-g002">Fig 2b</a>, red versus yellow). Subjects thus base their estimate of the auditory position on both visual and auditory cues. Moreover, subjects' estimates of the auditory position often differ from their estimates of the visual position.</p>
<a id="article1.body1.sec2.sec2.p4" name="article1.body1.sec2.sec2.p4"></a><p>To examine whether the causal inference model can account for the full range of cue combination observed in human multisensory perception, we make use of both the auditory and the visual response frequencies we measured in our experiment (<a href="#pone-0000943-g002">Fig. 2c</a>). Four parameters were used to fit 250 data points (25 bisensory conditions, 2 modalities, 5 buttons per modality). The causal inference model accounts for the data very well (<em>R</em><sup>2</sup> = 0.97; <em>R</em><sup>2 </sup>is calculated as the explained variance divided by the total variance) (<a href="#pone-0000943-g002">Fig. 2c,d</a>). One interesting finding is that the response distribution generally only has one peak (in <a href="#pone-0000943-g002">Fig 2c</a>), but its position and skewness is affected by the position of the other stimulus. The model shows this effect because it does not simply decide if there is a common cause or individual causes but considers both possibilities on each trial.</p>
<a id="article1.body1.sec2.sec2.p5" name="article1.body1.sec2.sec2.p5"></a><p>To facilitate quantitative comparison of the causal inference model with other models, we fitted the parameters individually to each subject's data using a maximum-likelihood procedure: we maximized the probability of the data under the model. For each subject, the best fit from 6 different sets of initial parameter values was used, to reduce the effect of these initial values. We did this for several different models that use previously proposed interaction priors as well as the prior derived from causal inference. We first considered two special cases of the causal inference model: pure integration (causal inference with <em>p</em><sub>common</sub> = 1) and pure segregation (causal inference with <em>p</em><sub>common</sub> = 0). We then considered two two-dimensional ad hoc priors that have been proposed in other papers. Roach et al. <a href="#pone.0000943-Roach1">[18]</a> proposed a two-dimensional (auditory-visual) prior that is defined as the sum of a Gaussian ridge along the diagonal, and a constant. This prior is somewhat similar to the causal inference prior as the constant relates to events that are independent and the Gaussian relates to sensory events that are related and thus have a common cause. Bresciani et al <a href="#pone.0000943-Bresciani1">[14]</a> used a special case of the Roach et al. prior ( The Shams et al. model (Shams et al, 2005) was not considered as it involves a prior specified by a large number of parameters (25)). where no constant is added to the Gaussian. According to the Bresciani prior, visual and auditory positions that are very far away from each other are extremely unlikely. According to the Roach prior, such two positions have a fixed, non-vanishing probability.</p>
<a id="article1.body1.sec2.sec2.p6" name="article1.body1.sec2.sec2.p6"></a><p>In the comparison, we obtain the predicted response distribution by integrating out the internal variables instead of equating it to the posterior distribution. This is the correct way of solving this Bayesian problem and differs from the approach taken in previous papers <a href="#pone.0000943-Shams1">[13]</a>, <a href="#pone.0000943-Bresciani1">[14]</a>, <a href="#pone.0000943-Roach1">[18]</a> (although it only affects predictions in the Roach et al. model). We measure the goodness of fit obtained from these priors relative to that obtained from the causal inference prior, using the log likelihood over the entire data set. The resulting log likelihood ratios are shown in <a href="#pone-0000943-t001">Table 1</a>. The causal inference model fits the data better than the other models. We also compare with an alternative model that instead of minimizing the mean squared error maximizes the probability of being correct and can exclude this model based on the presented evidence.</p>
<div class="figure" id="pone-0000943-t001"><div class="img"><a name="pone-0000943-t001" title="Click for larger image " href="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000943.t001&amp;representation=PNG_M" data-doi="info:doi/10.1371/journal.pone.0000943" data-uri="info:doi/10.1371/journal.pone.0000943.t001"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000943.t001&amp;representation=PNG_I" alt="thumbnail" class="thumbnail"></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000943.t001/powerpoint">
                    PPT
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000943.t001/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000943.t001/largerimage">
                    PNG
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000943.t001/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pone.0000943.t001.PNG_L"></span>)
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000943.t001/originalimage">
                    TIFF
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000943.t001/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pone.0000943.t001.TIF"></span>)
                </a></li></ul></div><p><strong>Table 1.  <span>Maximal log likelihood ratios (base <em>e</em>) across subjects of models relative to causal inference model (mean±s.e.m., see <a href="#s4">methods</a> for details).</span></strong></p><span>doi:10.1371/journal.pone.0000943.t001</span></div><a id="article1.body1.sec2.sec2.p7" name="article1.body1.sec2.sec2.p7"></a><p>The parameters found in the likelihood optimization of the causal inference model are as follows. We found the visual system to be relatively precise (<em>σ<sub>V</sub></em> = 2.14±0.22°) and the auditory system to be much less precise (<em>σ<sub>A</sub></em> = 9.2±1.1°). We found that people have a modest prior estimating stimuli to be more likely to be central (<em>σ<sub>P</sub></em> = 12.3±1.1°). Subjects have the tendency of indicating a direction that is straight ahead and the prior allows the model to show such behavior as well. The average probability of perceiving a common cause for visual and auditory stimuli is relatively low (<em>p</em><sub>common</sub> = 0.28±0.05°). This explained that the observed biases are small in comparison to the values predicted if subjects were certain that there is a common cause (<a href="#pone-0000943-g002">Fig. 2e</a>). In summary, the causal inference model provides precise predictions of the way people combine cues in an auditory-visual spatial localization task, and it does so better than earlier models.</p>
<a id="article1.body1.sec2.sec2.p8" name="article1.body1.sec2.sec2.p8"></a><p>In the cue combination literature, bias is commonly used as an index of crossmodal interactions. In our experiment, auditory localization bias is a measure of the influence of vision on audition and can be plotted as a function of the spatial disparity <a href="#pone.0000943-Wallace1">[15]</a>, <a href="#pone.0000943-Hairston1">[16]</a>. Like other authors, we find that the bias decreases with increasing spatial disparity (<a href="#pone-0000943-g002">Fig. 2e</a>). Thus, the larger the distance between visual and auditory stimuli, the smaller is the influence of vision on audition. This result is naturally predicted by the causal inference model: larger discrepancies make the single cause model less likely as it would need to assume large noise values, which are unlikely. A model in which no combination happens at all (<em>p</em><sub>common</sub> = 0) cannot explain the observed biases (<a href="#pone-0000943-g002">Fig. 2e</a>) as it predicts a very small bias (<em>p</em>&lt;0.0001 t-test). The traditional forced-fusion model <a href="#pone.0000943-Jacobs1">[4]</a>–<a href="#pone.0000943-Alais1">[7]</a>, <a href="#pone.0000943-Adams1">[29]</a> fails to explain much of the variance in <a href="#pone-0000943-g002">Fig. 2c</a> (<em>R</em><sup>2</sup> = 0.56). Moreover, this model would predict a very high bias—as vision is much more precise than audition in our experiment—and is ruled out by the bias data (<a href="#pone-0000943-g002">Fig. 2e</a>) (<em>p</em>&lt;0.0001 t-test). Neither the traditional nor the no-interaction model can explain the data, whereas the causal inference model can explain the observed patterns of partial combination well (see Supporting Information, <a href="#pone.0000943.s001">Text S1</a> and <a href="#pone.0000943.s003">Fig. S2</a> for a comparison with some other recent models of cue combination).</p>


<h4>Experiment 2: Auditory-visual spatial localization with measured perception of causality</h4>
<a id="article1.body1.sec2.sec3.p1" name="article1.body1.sec2.sec3.p1"></a><p>While the causal inference model accounts for the cue combination data described above, it makes a prediction that goes beyond the estimates of positions. If people infer the probability of common cause then it should be possible to ask them if they perceive a common cause versus two causes. A recent experiment asked this question <a href="#pone.0000943-Wallace1">[15]</a>. We compare the predictions of the causal inference model with the reported data from this experiment. These experiments differed in a number of important respects from our experiment. Subjects were asked to report their perception of unity (i.e., whether the two stimuli have a common cause or two independent causes) on each trial. Only the location of the auditory stimulus was probed. Subjects pointed towards the location of the auditory stimulus instead of choosing a button to indicate the position (see <a href="#s4">Methods</a>, data analysis for <a href="#pone-0000943-g003">figure 3</a>, for details).</p>
<div class="figure" id="pone-0000943-g003"><div class="img"><a name="pone-0000943-g003" title="Click for larger image " href="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000943.g003&amp;representation=PNG_M" data-doi="info:doi/10.1371/journal.pone.0000943" data-uri="info:doi/10.1371/journal.pone.0000943.g003"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000943.g003&amp;representation=PNG_I" alt="thumbnail" class="thumbnail"></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000943.g003/powerpoint">
                    PPT
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000943.g003/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000943.g003/largerimage">
                    PNG
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000943.g003/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pone.0000943.g003.PNG_L"></span>)
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0000943.g003/originalimage">
                    TIFF
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0000943.g003/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pone.0000943.g003.TIF"></span>)
                </a></li></ul></div><p><strong>Figure 3.  <span>Reports of causal inference.</span></strong></p><a id="article1.body1.sec2.sec3.fig1.caption1.p1" name="article1.body1.sec2.sec3.fig1.caption1.p1"></a><p>a) The relative frequency of subjects reporting one cause (black) is shown (reprinted with permission from <a href="#pone.0000943-Wallace1">[15]</a>) with the prediction of the causal inference model (red). b) The bias, i.e. the influence of vision on the perceived auditory position is shown (gray and black). The predictions of the model are shown in red. c) A schematic illustration explaining the finding of negative biases. Blue and black dots represent the perceived visual and auditory stimuli, respectively. In the pink area people perceive a common cause.</p>
<span>doi:10.1371/journal.pone.0000943.g003</span></div><a id="article1.body1.sec2.sec3.p2" name="article1.body1.sec2.sec3.p2"></a><p>The results of these experiments <a href="#pone.0000943-Wallace1">[15]</a> indicate that the closer the visual stimulus is to the auditory stimulus, the more often do people perceive them as having a common cause (<a href="#pone-0000943-g003">Fig. 3a</a>). However, even if the two stimuli are close to one another, on some trials the noise in the perception of the auditory stimulus will sometimes lead to the perception of distinct causes. For example, a subject may hear the sound at 10° even when both stimuli really are at 0°; on such a trial, the large perceived disparity may lead the subject to report distinct causes. The model also shows the trend that with increasing disparity the probability of the perception of a common cause decreases. It explains 72% of the variance in human performance (<a href="#pone-0000943-g003">Fig. 3a</a>) and thus well models the human perception of causality.</p>
<a id="article1.body1.sec2.sec3.p3" name="article1.body1.sec2.sec3.p3"></a><p>We next examined how the perception of a common versus distinct causes affects the estimation of the position of auditory stimuli. The results indicate that when people perceive a common cause they point to a position that is on average very close to the position of the visual stimulus, and therefore the bias is high (<a href="#pone-0000943-g003">Fig. 3b</a>). If, on the other hand, subjects perceive distinct causes, they seem to not only rely on the auditory stimulus but seem to be pushed away from the visual stimulus and exhibit <em>negative</em> bias. This is a counterintuitive finding, as previous models <a href="#pone.0000943-Jacobs1">[4]</a>–<a href="#pone.0000943-Alais1">[7]</a>, <a href="#pone.0000943-Adams1">[29]</a> predict only positive bias. Causal inference shows very similar behavior as it also exhibits negative biases, and explains 87% of the variance of the bias. The causal inference model thus accounts for the counterintuitive negative biases.</p>
<a id="article1.body1.sec2.sec3.p4" name="article1.body1.sec2.sec3.p4"></a><p>How can an optimal system exhibit negative bias? We argue that this is a selection bias stemming from restricting ourselves to trials in which causes were perceived as distinct. To clarify this, we consider, as an example, the case where the visual stimulus is 5° to the right of the center and the auditory stimulus is in the center. On some trials, the lack of precision of the auditory system will lead to the internal representation of the auditory signal being close to the visual signal and then the system infers a common cause. On those trials, the auditory stimulus will be inferred to be very close to the visual one, and the bias will be high (<a href="#pone-0000943-g003">Fig. 3c</a> red). On other trials, the internal representation of the auditory signal will, by chance, be far away from the visual signal, resulting in the model inferring distinct causes. The distribution of perceived auditory positions on these trials would be a truncated Gaussian distribution, with the right part of the distribution (corresponding to the perception of a common cause) cut away. This truncated distribution has a negative mean and thus leads to a negative bias. Due to this selection process, when a common cause is inferred, the perceived auditory location must be close to the visual stimulus, resulting in high bias. In contrast, when distinct causes are inferred, the perceived auditory location must be far away from the visual stimulus, and the bias thus becomes negative.</p>

</div>

<div id="section3" class="section"><a id="s3" name="s3" toc="s3" title="Discussion"></a><h3>Discussion</h3><a id="article1.body1.sec3.p1" name="article1.body1.sec3.p1"></a><p>The causal inference model formalizes and addresses the problem that had been phrased by von Helmholtz as “probabilistically inferring the causes of cues”. This refers to the general problem that subjects are faced with: deciding which cues correspond to the same source and which are unrelated to one another. The causal inference model can predict both subjects' unity judgments and their stimulus estimates. Moreover, its interaction prior is similar to the ones that have been proposed in earlier models that did not model causal inference but only the interaction between cues <a href="#pone.0000943-Shams1">[13]</a>, <a href="#pone.0000943-Bresciani1">[14]</a>, <a href="#pone.0000943-Rowland1">[17]</a>, <a href="#pone.0000943-Roach1">[18]</a>. It thus provides an explanation for why models utilizing an interaction prior have been successful at modeling human performance.</p>
<a id="article1.body1.sec3.p2" name="article1.body1.sec3.p2"></a><p>In addition to providing a formalism that derives from a strong normative idea, the causal inference model leads to better fits of human performance in the auditory-visual localization task presented here. Moreover, the causal inference model can make direct predictions about the causal structure of sensory input that would have been impossible with the previous models. Inference about causal structure is an important element of the perceptual binding of multisensory stimuli: when and how do sights and sounds get paired into a unified conscious percept <a href="#pone.0000943-Treisman1">[22]</a>, <a href="#pone.0000943-Roskies1">[30]</a>? The causal inference model presents a partial answer to this question.</p>
<a id="article1.body1.sec3.p3" name="article1.body1.sec3.p3"></a><p>As the causal inference model uses a single inference rule to account for the entire spectrum of sensory cue combination, many previous models are special cases of the model presented here, including those showing statistical optimality of complete integration (<em>p</em><sub>common</sub> = 1) when the discrepancy between the signals is small. In that case, the probability of a common cause given the sensory cues will be close to 1.</p>
<a id="article1.body1.sec3.p4" name="article1.body1.sec3.p4"></a><p>It is necessary to discuss in which way we may expect subjects to behave in an optimal way. We have shown that the assumption that subjects optimally solve causal inference problems can well predict many facets of their decision process. However, as has often been argued <a href="#pone.0000943-Gould1">[31]</a>, <a href="#pone.0000943-Gigerenzer1">[32]</a>, there is no reason why the nervous system should be optimal under all circumstances. For problems of high importance to everyday life, however, we should expect evolution to have found a very good solution. For this range of problems we should expect ideal observer models to make good predictions of human behavior.</p>
<a id="article1.body1.sec3.p5" name="article1.body1.sec3.p5"></a><p>This leaves the question of what neural processes underlie the causal inference computations that emerge at the behavioral level as close to Bayes-optimal. Recent work has shed some light on this issue for the case of complete integration (<em>p</em><sub>common</sub> = 1). It is well-known that neural populations naturally encode probability distributions over stimuli through Bayes' rule, a type of coding known as probabilistic population coding <a href="#pone.0000943-Zemel1">[33]</a>, <a href="#pone.0000943-Pouget1">[34]</a>. Under the assumption of a common cause, optimal cue combination can be implemented in a biologically realistic network using this type of coding. Unexpectedly, this only requires simple linear operations on neural activity <a href="#pone.0000943-Ma1">[35]</a>. This implementation makes essential use of the structure of neural variability and leads to physiological predictions for activity in areas that combine multisensory input, such as the superior colliculus. Since complete integration is a special case of causal inference, computational mechanisms for the latter are expected to have a neural substrate that generalizes these linear operations on population activities. A neural implementation of optimal causal inference will be an important step towards a complete neural theory of multisensory perception.</p>
<a id="article1.body1.sec3.p6" name="article1.body1.sec3.p6"></a><p>In the study of higher-level cognition, many experiments have shown that people, starting from infancy, interpret events in terms of the actions of hidden causes <a href="#pone.0000943-Buehner1">[36]</a>–<a href="#pone.0000943-Waldmann1">[40]</a>. If we see a window shatter, something or someone must have broken it; if a ball flies up into the air, something launched it. It is particularly hard to resist positing invisible common causes to explain surprising conjunctions of events, such as the sudden occurrence of several cases of the same rare cancer in a small town. These causal inferences in higher-level cognition may seem quite different than the causal inferences in sensory integration we have studied here: more deliberate, consciously accessible, and knowledge-dependent, rather than automatic, instantaneous, and universal. Yet an intriguing link is suggested by our finding. The optimal statistical principles that can explain causal inference in sensory integration are very similar to those that have recently been shown to explain more conventional hidden-cause inferences in higher-level cognition <a href="#pone.0000943-Griffiths1">[39]</a>, <a href="#pone.0000943-Tenenbaum1">[41]</a>, <a href="#pone.0000943-Sobel1">[42]</a>. Problems of inferring common causes from observed conjunctions arise everywhere across perception and cognition, and the brain may have evolved similar or even common mechanisms for performing these inferences optimally, in order to build veridical models of the environment's structure.</p>
</div>

<div id="section4" class="section"><a id="s4" name="s4" toc="s4" title="Materials and Methods"></a><h3>Materials and Methods</h3>
<h4>Paradigm for Experiment 1</h4>
<a id="article1.body1.sec4.sec1.p1" name="article1.body1.sec4.sec1.p1"></a><p>Twenty naive subjects (undergraduate students at the California Institute of Technology, ten male) participated in the experiment. All subjects were informed of the purposes of the study and gave written informed consent as approved by the local ethics committee (Caltech Committee for the Protection of Human Subjects). Subjects were seated at a viewing distance of 54 cm from a 21-inch monitor. In each trial, subjects were asked to report both the perceived visual and auditory positions using keyboard keys 1 through 5, with 1 being the leftmost location and 5 the rightmost. No feedback about the correctness of the response was given. Visual and auditory stimuli were presented independently at one of five positions. The five locations extended from 10° to the left of the fixation point to 10° to the right of the fixation point at 5° intervals, along a horizontal line 5° below the fixation point, Visual stimuli were 35 ms presentations of Gabor wavelets of high contrast extending 2° on a background of visual noise. Auditory stimuli, synchronized with the visual stimuli in the auditory-visual conditions, were presented through a pair of headphones and consisted of 35 ms white noise. Unisensory stimuli were also presented, for which there was no presentation of stimulus for the other modality. However, for the purposes of the modeling in <a href="#pone-0000943-g002">Fig. 2c</a> we disregarded the unimodal data because of potential attentional differences between bimodal and unimodal data. The sound stimuli were filtered through a Head-Related Transfer Function (HRTF), measured individually for each subject, using methods similar to those described by <a href="http://sound.media.mit.edu/KEMAR.html">http://sound.media.mit.edu/KEMAR.html</a>. The HRTFs were created to simulate sounds originating from the five spatial locations in the frontoparallel plane where the visual stimuli were presented. The data from one subject had to be discarded, as the fitted auditory variance of that subject was 98° and the subject was therefore effectively deaf with respect to the objective of our study. Apart from this subject, the HRTF function yielded to good auditory precision in the range of 10°.</p>


<h4>Generative model</h4>
<a id="article1.body1.sec4.sec2.p1" name="article1.body1.sec4.sec2.p1"></a><p>A Bayesian approach based on a generative model requires one to fully specify how the variables of interest are interrelated statistically. This is done as follows. Determine if there is one cause (<em>C</em> = 1) versus two causes (<em>C</em> = 2) by drawing from a binomial distribution with <em>p</em>(<em>C</em> = 1) = <em>p</em><sub>common</sub>. Outside of the experiment this will not be constant but depend on temporal delays, visual experience, context, and many other factors. In the experiments we consider all these factors are held constant so we can use a fixed <em>p</em><sub>common</sub>. If there is one cause (<em>C</em> = 1), draw a position <em>s</em> from a normal prior distribution <em>N</em>(0,<em>σ<sub>P</sub></em>), where <em>N</em>(<em>μ</em>,<em>σ<sub>P</sub></em>) stands for a normal distribution with mean <em>μ</em> and standard deviation <em>σ</em>. It is thus more likely that a stimulus is centrally located than far to the side. We then set <em>s<sub>V</sub> = s</em> and <em>s<sub>A</sub></em> = <em>s</em>. If there are two causes (<em>C</em> = 2), draw positions <em>s<sub>V</sub></em> and <em>s<sub>A</sub></em> each independently from <em>N</em>(0,<em>σ<sub>P</sub></em>). We assume that the visual and the auditory signal are corrupted by unbiased Gaussian noise of standard deviations <em>σ<sub>V</sub></em> and <em>σ<sub>A,</sub></em> respectively and draw <em>x<sub>V</sub></em> from <em>N</em>(<em>s<sub>V</sub></em>,<em>σ<sub>V</sub></em>) and <em>x<sub>A</sub></em> from <em>N</em>(<em>s<sub>A</sub></em>,<em>σ<sub>A</sub></em>). The noise is thus assumed to be independent across modalities.</p>


<h4>Estimating the probability of a common cause</h4>
<a id="article1.body1.sec4.sec3.p1" name="article1.body1.sec4.sec3.p1"></a><p>An ideal observer is faced with the problem of inferring the causal structure, i.e., whether there is one cause or there are two causes. This inference is performed optimally using Bayes' rule: <a name="" id=""></a><span class="equation"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000943.e002&amp;representation=PNG" class="inline-graphic"><span class="note">(1)</span></span><br>Here <em>p</em>(<em>x<sub>A</sub></em>, <em>x<sub>V</sub></em>) must be chosen such that <em>p</em>(<em>C</em> = 1|<em>x<sub>V</sub></em>, <em>x<sub>A</sub></em>) and <em>p</em>(<em>C</em> = 2|<em>x<sub>V</sub></em>, <em>x<sub>A</sub></em>) add to 1, as we are dealing with probabilities. We thus obtain: <a name="" id=""></a><span class="equation"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000943.e003&amp;representation=PNG" class="inline-graphic"><span class="note">(2)</span></span><br>For <em>p</em>(<em>x<sub>V</sub></em>, <em>x<sub>A</sub></em>|<em>C</em> = 1) we obtain<a name="" id=""></a><span class="equation"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000943.e004&amp;representation=PNG" class="inline-graphic"><span class="note">(3)</span></span><br>All three factors in this integral are Gaussians, allowing for an analytic solution: <a name="" id=""></a><span class="equation"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000943.e005&amp;representation=PNG" class="inline-graphic"><span class="note">(4)</span></span><br>where <em>μ<sub>P</sub></em> = 0is the mean of the prior.</p>
<a id="article1.body1.sec4.sec3.p2" name="article1.body1.sec4.sec3.p2"></a><p>For <em>p</em>(<em>x<sub>V</sub></em>, <em>x<sub>A</sub></em>|<em>C</em> = 2) we note that <em>x</em><sub>V</sub> and <em>x<sub>A</sub></em> are independent of each other and we thus obtain a product of two factors: <a name="" id=""></a><span class="equation"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000943.e006&amp;representation=PNG" class="inline-graphic"><span class="note">(5)</span></span><br>Again, as all these distributions are assumed to be Gaussian, we can write down an analytic solution, <a name="" id=""></a><span class="equation"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000943.e007&amp;representation=PNG" class="inline-graphic"><span class="note">(6)</span></span><br>For comparison with experimental data <a href="#pone.0000943-Wallace1">[15]</a>, we assume that the model reports a common cause when <span class="inline-formula"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000943.e008&amp;representation=PNG" class="inline-graphic"></span>. The estimation of a common cause amounts to a Bayesian model selection problem. It is mathematically similar to a mixture model for depth <a href="#pone.0000943-Knill1">[24]</a>.</p>


<h4>Optimally estimating the position</h4>
<a id="article1.body1.sec4.sec4.p1" name="article1.body1.sec4.sec4.p1"></a><p>When estimating the position of the visual target, we assume that making an error about the position of <em>s</em> in the case of a common cause is just as bad as making an error in the estimate of <em>s<sub>V</sub></em>, and likewise for the position of the auditory target. We assume that the cost in this estimation process is the mean squared error <a href="#pone.0000943-Kording1">[43]</a>: <a name="" id=""></a><span class="equation"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000943.e009&amp;representation=PNG" class="inline-graphic"><span class="note">(7)</span></span><br>An optimal estimate for the subject is the estimate that leads to the lowest expected cost under the subject's posterior belief: <a name="" id=""></a><span class="equation"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000943.e010&amp;representation=PNG" class="inline-graphic"><span class="note">(8)</span></span><br>where, <em>Ŝ<sub>V</sub></em> is the possible estimate. In general, when the cost function is quadratic, the optimal estimation problem reduces to the problem of finding the mean of the posterior distribution. The estimate that minimizes the mean expected squared error in our case is therefore: <a name="" id=""></a><span class="equation"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000943.e011&amp;representation=PNG" class="inline-graphic"><span class="note">(9)</span></span><br>and<a name="" id=""></a><span class="equation"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000943.e012&amp;representation=PNG" class="inline-graphic"><span class="note">(10)</span></span><br>where <em>Ŝ<sub>C</sub></em><sub> = 1</sub> and <em>Ŝ<sub>C</sub></em><sub> = 2</sub> are the best estimates we would obtain if we were certain about there being one or two causes, respectively. These <em>conditionalized</em> solutions are obtained by linearly weighing the different cues proportional to their inverse variances <a href="#pone.0000943-Ghahramani1">[44]</a>, as follows from the fact that the posterior is a product of Gaussians and thus a Gaussian itself. Therefore, if we know if there is a common or distinct causes we are able to analytically solve for the solution minimizing the mean squared error, which here coincides with the maximum-a-posteriori solution. The optimal visual estimate when visual and auditory sources are different is the same as when there would be only a visual signal, and likewise for the auditory estimate: <a name="" id=""></a><span class="equation"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000943.e013&amp;representation=PNG" class="inline-graphic"><span class="note">(11)</span></span><br>When the sensory signals have a common cause, the optimal solution is: <a name="" id=""></a><span class="equation"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000943.e014&amp;representation=PNG" class="inline-graphic"><span class="note">(12)</span></span><br>While these are both linear combinations of <em>x<sub>V</sub></em> and <em>x<sub>A</sub></em>, the prefactors <em>p</em>(<em>C</em>|<em>x<sub>V</sub></em>, <em>x<sub>A</sub></em>) appearing in from Eqs. (9) and (10) are nonlinear in these two variables. Therefore, the overall optimal estimates <em>Ŝ<sub>V</sub></em> and <em>Ŝ<sub>A</sub></em>, when both common and independent causes are possible, are no longer linear combinations. The traditional way of studying the system by only measuring linear weights is not sufficient to fully describe a system that performs causal inference.</p>
<a id="article1.body1.sec4.sec4.p2" name="article1.body1.sec4.sec4.p2"></a><p>The predicted distribution of visual positions is obtained through marginalization: <a name="" id=""></a><span class="equation"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000943.e015&amp;representation=PNG" class="inline-graphic"><span class="note">(13)</span></span><br>These are the distributions which we can compare with experimental data, the response distributions were obtained through simulation (for more details see the Supporting Information, <a href="#pone.0000943.s001">Text S1</a> and <a href="#pone.0000943.s002">Fig. S1</a>).</p>
<a id="article1.body1.sec4.sec4.p3" name="article1.body1.sec4.sec4.p3"></a><p>In the simulation, we ran each condition (<em>s<sub>V</sub></em>, <em>s<sub>A</sub></em>) 10,000 times for each subject. For each trial, we obtained estimates (<em>Ŝ<sub>V</sub></em>, <em>Ŝ<sub>A</sub></em>) in the way described above. To link these estimates with our data in which we have only five possible responses, we assume that people press the button which is associated with the position closest to <em>Ŝ</em>. This is equivalent to choosing the button that leads to the lowest expected mean squared error between the position indicated by the button and the position of the stimulus. This amounts to allowing only estimated values <em>Ŝ</em> that are in the set {−10°, −5°, 0°, 5°, 10°}. In this way, we obtained a histogram of responses for each subject and condition. For the model comparison in <a href="#pone-0000943-t001">Table 1</a>, we also considered a maximum-a-posteriori estimator, which does not use a cost function but instead selects the location from the 5-element response set that has the highest probability of being correct. To link the causal inference model with the data on causal inference <a href="#pone.0000943-Wallace1">[15]</a> of <a href="#pone-0000943-g003">figure 3</a> in the main text, we assume additional motor noise with width <em>σ</em><sub>motor</sub>, that perturbs the estimated position <em>Ŝ</em>.</p>
<a id="article1.body1.sec4.sec4.p4" name="article1.body1.sec4.sec4.p4"></a><p>A potential problem with any kind of optimization procedure is the danger of over-fitting. To test for this with out model we split the data set in two groups, each with half the data from each subject. When optimizing on the first group, we find an excellent performance of the model (<em>R</em><sup>2</sup> = 0.98) and when we transfer the optimized parameters to the second set of data, we still find an excellent performance (<em>R</em><sup>2</sup> = 0.96). Overfitting is therefore not a problem with the causal inference model on this data set.</p>


<h4>Data Analysis for <a href="#pone-0000943-g002">Figure 2</a></h4>
<a id="article1.body1.sec4.sec5.p1" name="article1.body1.sec4.sec5.p1"></a><p>We choose <em>p</em><sub>common</sub>, <em>σ<sub>P</sub></em>, <em>σ<sub>V</sub></em>, <em>σ<sub>A</sub></em> to maximize the probability of the experimental data given the model. For the group analysis we obtain these 4 parameters from a dataset obtained by pooling together the data from all subjects. To obtain the histograms plotted in <a href="#pone-0000943-g002">Figure 2C</a> we simulate the system for each combination of cues for 10,000 times. It is a common mistake <a href="#pone.0000943-Shams1">[e.g. 13]</a>, <a href="#pone.0000943-Roach1">[18]</a> to directly compare a distribution like <em>p</em>(<em>s<sub>A</sub></em> | <em>x<sub>V</sub> ,x<sub>A</sub></em>) to the data. This is a mistake because <em>x<sub>V</sub></em> and <em>x<sub>A</sub></em> are internal representations that differ from trial to trial and are not accessible to the experimenter. An inferred distribution like <em>p</em>(<em>s<sub>A</sub></em> | <em>x<sub>V</sub> ,x<sub>A</sub></em>) will give rise to a single estimate <em>Ŝ<sub>V</sub></em> or <em>Ŝ<sub>A</sub></em> as given by Equation 2. Only a histogram of estimates over many simulated trials can be compared with behavioral responses. This histogram will in many cases be very different in shape from <em>p</em>(<em>s<sub>A</sub></em> | <em>x<sub>V </sub>, x<sub>A</sub></em>). To fit the model to the data we maximize the probability of the data given the model using the multinomial likelihood.</p>


<h4>Calculating the multinomial likelihood</h4>
<a id="article1.body1.sec4.sec6.p1" name="article1.body1.sec4.sec6.p1"></a><p>We need to calculate the likelihood of the data given a model and its parameters. Since the data take the form of counts of discrete outcomes (1 to 5), we use the multinomial distribution. On each trial, a response is drawn from a discrete probability distribution. Each model produces numerical approximations to the probabilities of each response in a given condition (<em>s<sub>V</sub>, s<sub>A</sub></em> ). We will denote these <em>p<sub>i</sub></em>, with <em>i</em> = 1, 2, 3, 4, 5. Suppose the observed response counts in this condition are <em>n<sub>i</sub></em>, with <em>i</em> = 1, 2, 3, 4, 5. Then the probability of observing counts {<em>n<sub>i</sub></em>} under this model is: <a name="" id=""></a><span class="equation"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000943.e016&amp;representation=PNG" class="inline-graphic"><span class="note">(14)</span></span><br>where <em>n</em> is the total number of trials in this condition, <span class="inline-formula"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000943.e017&amp;representation=PNG" class="inline-graphic"></span>, and the prefactor <span class="inline-formula"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000943.e018&amp;representation=PNG" class="inline-graphic"></span> is the number of ways in which <em>n</em> responses can be split up into five counts {<em>n<sub>i</sub></em>}. This gives the likelihood of the model given the data: <a name="" id=""></a><span class="equation"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000943.e019&amp;representation=PNG" class="inline-graphic"><span class="note">(15)</span></span><br>Maximizing this likelihood is equivalent to maximizing its logarithm, which is given by. <a name="" id=""></a><span class="equation"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0000943.e020&amp;representation=PNG" class="inline-graphic"><span class="note">(16)</span></span><br></p>
<a id="article1.body1.sec4.sec6.p2" name="article1.body1.sec4.sec6.p2"></a><p>So far we have considered a single condition. Since conditions are independent of each other, the overall log likelihood is the sum of the log likelihoods in all conditions. These overall log likelihoods are compared for different models in <a href="#pone-0000943-t001">Table 1</a> in the main text and are used for fitting the parameters to the causal inference model and all other models we are comparing to.</p>


<h4>Data analysis for <a href="#pone-0000943-t001">Table 1</a></h4>
<a id="article1.body1.sec4.sec7.p1" name="article1.body1.sec4.sec7.p1"></a><p>We compare how good the causal inference model is relative to other models. The way this is usually done within Bayesian statistics is by calculating the Bayes factor, the ratio of the likelihood of the data under one model to the likelihood of the data under the other model, <em>p</em>(data|model<sub>1</sub>)/<em>p</em>(data|model<sub>2</sub>). As the necessary integration of all nuisance parameters is infeasible we instead analyze the ratios of maximal probabilities which can be corrected for the different number of free variables using the BIC <a href="#pone.0000943-Barnes1">[45]</a>. This ratio formalizes how much evidence the data provides for one model relative to another model and is thus a very general and mathematically precise way of quantifying the quality of a model. According to the BIC the causal inference model is preferable over the models that have fewer parameters. <a href="#pone-0000943-t001">Table 1</a> reports the log maximal probability ratios for several models relative to the causal inference model.</p>


<h4>Data analysis for <a href="#pone-0000943-g003">Figure 3</a></h4>
<a id="article1.body1.sec4.sec8.p1" name="article1.body1.sec4.sec8.p1"></a><p>In these experiments, subjects report the perceived position by pointing with a laser pointer. In such cases there may be additional noise due to a misalignment of the cursor relative to the intended position of the cursor. To model this, we introduce one additional variable, motor noise. We assume that motor noise corrupts all reports, is additive and drawn from a Gaussian with width <em>σ</em><sub>motor</sub>. We estimate the relevant uncertainties as follows. In both auditory and visual trials the noise will have two sources, motor noise and sensory noise. We assume that visual only trials are dominated by motor noise, stemming from motor errors and memory errors. From data presented in <a href="#pone-0000943-g002">figure 2</a> of other experiments <a href="#pone.0000943-Hairston1">[16]</a> where pointing responses are made in unimodal trials, we obtain <em>σ</em><sub>motor</sub> = 2.5°, and from the same graph we obtain <em>σ<sub>A</sub></em> = 7.6° (because variances are added linearly). These parameters were not tuned. The other two parameters, <em>p</em><sub>common</sub> and <em>σ<sub>P</sub></em>, were obtained by minimizing the squared deviation of the model predictions from the data. The choice of <em>p</em><sub>common</sub> affects the judgment of unity and thus strongly affects the bias graph as well as the commonality judgments. Large values of <em>p</em><sub>common</sub> lead to high bias and small values of <em>p</em><sub>common</sub> lead to bias values that are high only very close to zero disparity. The same parameter values are used for both graphs in <a href="#pone-0000943-g003">figure 3</a>.</p>

</div>

<div id="section5" class="section"><a id="s5" name="s5" toc="s5" title="Supporting Information"></a><h3>Supporting Information</h3><div class="figshare_widget" doi="10.1371/journal.pone.0000943"></div><a name="pone.0000943.s001" id="pone.0000943.s001"></a><p class="siTitle"><strong><a href="/article/fetchSingleRepresentation.action?uri=info:doi/10.1371/journal.pone.0000943.s001">Text S1. </a></strong></p><a id="article1.body1.sec5.supplementary-material1.caption1.p1" name="article1.body1.sec5.supplementary-material1.caption1.p1"></a><p class="preSiDOI">Supporting Information for “Causal inference in multisensory perception”</p>
<p class="siDoi">doi:10.1371/journal.pone.0000943.s001</p><a id="article1.body1.sec5.supplementary-material1.caption1.p2" name="article1.body1.sec5.supplementary-material1.caption1.p2"></a><p class="postSiDOI">(0.11 MB DOC)</p>
<a name="pone.0000943.s002" id="pone.0000943.s002"></a><p class="siTitle"><strong><a href="/article/fetchSingleRepresentation.action?uri=info:doi/10.1371/journal.pone.0000943.s002">Figure S1. </a></strong></p><a id="article1.body1.sec5.supplementary-material2.caption1.p1" name="article1.body1.sec5.supplementary-material2.caption1.p1"></a><p class="preSiDOI">The interaction priors when fit to our dataset are shown for the causal inference model, the Roach et al. <a href="#pone.0000943-Hatfield1">[1]</a> and the Bresciani et al. priors<a href="#pone.0000943-Warren1">[3]</a>.</p>
<p class="siDoi">doi:10.1371/journal.pone.0000943.s002</p><a id="article1.body1.sec5.supplementary-material2.caption1.p2" name="article1.body1.sec5.supplementary-material2.caption1.p2"></a><p class="postSiDOI">(1.15 MB EPS)</p>
<a name="pone.0000943.s003" id="pone.0000943.s003"></a><p class="siTitle"><strong><a href="/article/fetchSingleRepresentation.action?uri=info:doi/10.1371/journal.pone.0000943.s003">Figure S2. </a></strong></p><a id="article1.body1.sec5.supplementary-material3.caption1.p1" name="article1.body1.sec5.supplementary-material3.caption1.p1"></a><p class="preSiDOI">The average auditory bias, i.e. the relative influence of the visual position on the perceived auditory position, is shown as a function of the absolute spatial disparity (solid line, as in <a href="#pone-0000943-g002">Fig. 2</a> main text) along with the model predictions (dashed lines). Red: causal inference model. Green: behavior derived from using the Roach et al prior. Purple: behaviour derived from using the Bresciani et al prior.</p>
<p class="siDoi">doi:10.1371/journal.pone.0000943.s003</p><a id="article1.body1.sec5.supplementary-material3.caption1.p2" name="article1.body1.sec5.supplementary-material3.caption1.p2"></a><p class="postSiDOI">(0.94 MB EPS)</p>
</div>





<div><a id="ack" name="ack" toc="ack" title="Acknowledgments"></a><h3>Acknowledgments</h3>
<a id="article1.back1.ack1.p1" name="article1.back1.ack1.p1"></a><p>We would like to thank David Knill and Philip Sabes for inspiring discussions and Jose L. Pena for help with the setup of the experiment in figure 2. K.P.K., U.B., and W.J.M. contributed equally to this paper. After completion of this work, we became aware of a paper by Yoshiyuki Sato, Taro Toyoizumi and Kazuyuki Aihara, who independently developed a similar model (Neural Computation, in press).</p>
</div><div class="contributions"><a id="authcontrib" name="authcontrib" toc="authcontrib" title="Author Contributions"></a><h3>Author Contributions</h3><p>Conceived and designed the experiments: LS SQ JT. Performed the experiments: UB. Analyzed the data: UB KK WM. Wrote the paper: UB KK LS WM.</p></div><div><a id="references" name="references" toc="references" title="References"></a><h3>References</h3><ol class="references"><li><span class="label">1.
              </span><a name="pone.0000943-Hatfield1" id="pone.0000943-Hatfield1"></a>Hatfield GC (1990) The natural and the normative theories of spatial perception from Kant to Helmholtz. Cambridge, Mass: MIT Press.   <ul class="find-nolinks"></ul></li><li><span class="label">2.
              </span><a name="pone.0000943-Thurlow1" id="pone.0000943-Thurlow1"></a>Thurlow WR, Jack CE (1973) Certain determinants of the “ventriloquism effect”. Percept Mot Skills  36: 1171–1184.  <ul class="find" data-citedArticleID="996650" data-doi="10.2466/pms.1973.36.3c.1171"><li><a href="http://dx.doi.org/10.2466/pms.1973.36.3c.1171" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Certain+determinants+of+the+%E2%80%9Cventriloquism+effect%E2%80%9D." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Certain+determinants+of+the+%E2%80%9Cventriloquism+effect%E2%80%9D.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">3.
              </span><a name="pone.0000943-Warren1" id="pone.0000943-Warren1"></a>Warren DH, Welch RB, McCarthy TJ (1981) The role of visual-auditory “compellingness” in the ventriloquism effect: implications for transitivity among the spatial senses. Percept Psychophys  30: 557–564.  <ul class="find" data-citedArticleID="996658" data-doi="10.3758/bf03202010"><li><a href="http://dx.doi.org/10.3758/bf03202010" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=The+role+of+visual-auditory+%E2%80%9Ccompellingness%E2%80%9D+in+the+ventriloquism+effect%3A+implications+for+transitivity+among+the+spatial+senses." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22The+role+of+visual-auditory+%E2%80%9Ccompellingness%E2%80%9D+in+the+ventriloquism+effect%3A+implications+for+transitivity+among+the+spatial+senses.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">4.
              </span><a name="pone.0000943-Jacobs1" id="pone.0000943-Jacobs1"></a>Jacobs RA (1999) Optimal integration of texture and motion cues to depth. Vision Res  39: 3621–3629.  <ul class="find" data-citedArticleID="996616" data-doi="10.1016/s0042-6989(99)00088-7"><li><a href="http://dx.doi.org/10.1016/s0042-6989(99)00088-7" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Optimal+integration+of+texture+and+motion+cues+to+depth." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Optimal+integration+of+texture+and+motion+cues+to+depth.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">5.
              </span><a name="pone.0000943-vanBeers1" id="pone.0000943-vanBeers1"></a>van Beers RJ, Sittig AC, Gon JJ (1999) Integration of proprioceptive and visual position-information: An experimentally supported model. J Neurophysiol  81: 1355–1364.  <ul class="find" data-citedArticleID="996664"><li><a href="http://www.crossref.org/guestquery/?auth2=&amp;atitle2=Integration+of+proprioceptive+and+visual+position-information%3A+An+experimentally+supported+model.&amp;auth=&amp;atitle=Integration+of+proprioceptive+and+visual+position-information%3A+An+experimentally+supported+model." target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Integration+of+proprioceptive+and+visual+position-information%3A+An+experimentally+supported+model." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Integration+of+proprioceptive+and+visual+position-information%3A+An+experimentally+supported+model.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">6.
              </span><a name="pone.0000943-Ernst1" id="pone.0000943-Ernst1"></a>Ernst MO, Banks MS (2002) Humans integrate visual and haptic information in a statistically optimal fashion. Nature  415: 429–433.  <ul class="find" data-citedArticleID="996592" data-doi="10.1038/415429a"><li><a href="http://dx.doi.org/10.1038/415429a" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Humans+integrate+visual+and+haptic+information+in+a+statistically+optimal+fashion." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Humans+integrate+visual+and+haptic+information+in+a+statistically+optimal+fashion.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">7.
              </span><a name="pone.0000943-Alais1" id="pone.0000943-Alais1"></a>Alais D, Burr D (2004) The ventriloquist effect results from near-optimal bimodal integration. Curr Biol  14: 257–262.  <ul class="find" data-citedArticleID="996578" data-doi="10.1016/j.cub.2004.01.029"><li><a href="http://dx.doi.org/10.1016/j.cub.2004.01.029" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=The+ventriloquist+effect+results+from+near-optimal+bimodal+integration." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22The+ventriloquist+effect+results+from+near-optimal+bimodal+integration.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">8.
              </span><a name="pone.0000943-Slutsky1" id="pone.0000943-Slutsky1"></a>Slutsky DA, Recanzone GH (2001) Temporal and spatial dependency of the ventriloquism effect. Neuroreport  12: 7–10.  <ul class="find" data-citedArticleID="996644" data-doi="10.1097/00001756-200101220-00009"><li><a href="http://dx.doi.org/10.1097/00001756-200101220-00009" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Temporal+and+spatial+dependency+of+the+ventriloquism+effect." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Temporal+and+spatial+dependency+of+the+ventriloquism+effect.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">9.
              </span><a name="pone.0000943-Munhall1" id="pone.0000943-Munhall1"></a>Munhall KG, Gribble P, Sacco L, Ward M (1996) Temporal constraints on the McGurk effect. Percept Psychophys  58: 351–362.  <ul class="find" data-citedArticleID="996626" data-doi="10.3758/bf03206811"><li><a href="http://dx.doi.org/10.3758/bf03206811" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Temporal+constraints+on+the+McGurk+effect." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Temporal+constraints+on+the+McGurk+effect.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">10.
              </span><a name="pone.0000943-Jack1" id="pone.0000943-Jack1"></a>Jack CE, Thurlow WR (1973) Effects of degree of visual association and angle of displacement on the “ventriloquism” effect. Percept Mot Skills  37: 967–979.  <ul class="find" data-citedArticleID="996614" data-doi="10.2466/pms.1973.37.3.967"><li><a href="http://dx.doi.org/10.2466/pms.1973.37.3.967" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Effects+of+degree+of+visual+association+and+angle+of+displacement+on+the+%E2%80%9Cventriloquism%E2%80%9D+effect." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Effects+of+degree+of+visual+association+and+angle+of+displacement+on+the+%E2%80%9Cventriloquism%E2%80%9D+effect.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">11.
              </span><a name="pone.0000943-Warren2" id="pone.0000943-Warren2"></a>Warren DH, Cleaves WT (1971) Visual-proprioceptive interaction under large amounts of conflict. J Exp Psychol  90: 206–214.  <ul class="find" data-citedArticleID="996660" data-doi="10.1037/h0031545"><li><a href="http://dx.doi.org/10.1037/h0031545" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Visual-proprioceptive+interaction+under+large+amounts+of+conflict." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Visual-proprioceptive+interaction+under+large+amounts+of+conflict.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">12.
              </span><a name="pone.0000943-Gepshtein1" id="pone.0000943-Gepshtein1"></a>Gepshtein S, Burge J, Ernst MO, Banks MS (2005) The combination of vision and touch depends on spatial proximity. J Vis  5: 1013–1023.  <ul class="find" data-citedArticleID="996596" data-doi="10.1167/5.11.7"><li><a href="http://dx.doi.org/10.1167/5.11.7" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=The+combination+of+vision+and+touch+depends+on+spatial+proximity." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22The+combination+of+vision+and+touch+depends+on+spatial+proximity.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">13.
              </span><a name="pone.0000943-Shams1" id="pone.0000943-Shams1"></a>Shams L, Ma WJ, Beierholm U (2005) Sound-induced flash illusion as an optimal percept. Neuroreport  16: 1923–1927.  <ul class="find" data-citedArticleID="996642" data-doi="10.1097/01.wnr.0000187634.68504.bb"><li><a href="http://dx.doi.org/10.1097/01.wnr.0000187634.68504.bb" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Sound-induced+flash+illusion+as+an+optimal+percept." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Sound-induced+flash+illusion+as+an+optimal+percept.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">14.
              </span><a name="pone.0000943-Bresciani1" id="pone.0000943-Bresciani1"></a>Bresciani JP, Dammeier F, Ernst MO (2006) Vision and touch are automatically integrated for the perception of sequences of events. J Vis  6: 554–564.  <ul class="find" data-citedArticleID="996584" data-doi="10.1167/6.5.2"><li><a href="http://dx.doi.org/10.1167/6.5.2" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Vision+and+touch+are+automatically+integrated+for+the+perception+of+sequences+of+events." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Vision+and+touch+are+automatically+integrated+for+the+perception+of+sequences+of+events.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">15.
              </span><a name="pone.0000943-Wallace1" id="pone.0000943-Wallace1"></a>Wallace MT, Roberson GE, Hairston WD, Stein BE, Vaughan JW, et al.  (2004) Unifying multisensory signals across time and space. Exp Brain Res  158: 252–258.  <ul class="find" data-citedArticleID="996656" data-doi="10.1007/s00221-004-1899-9"><li><a href="http://dx.doi.org/10.1007/s00221-004-1899-9" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Unifying+multisensory+signals+across+time+and+space." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Unifying+multisensory+signals+across+time+and+space.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">16.
              </span><a name="pone.0000943-Hairston1" id="pone.0000943-Hairston1"></a>Hairston WD, Wallace MT, Vaughan JW, Stein BE, Norris JL, et al.  (2003) Visual localization ability influences cross-modal bias. J Cogn Neurosci  15: 20–29.  <ul class="find" data-citedArticleID="996608" data-doi="10.1162/089892903321107792"><li><a href="http://dx.doi.org/10.1162/089892903321107792" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Visual+localization+ability+influences+cross-modal+bias." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Visual+localization+ability+influences+cross-modal+bias.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">17.
              </span><a name="pone.0000943-Rowland1" id="pone.0000943-Rowland1"></a>Rowland B, Stanford T, Stein BE (2007) A Bayesian model unifies multisensory spatial localization with the physiological properties of the superior colliculus.   Exp Brain Res DOI 10.1007/s00221-006-0847-2.  <ul class="find-nolinks"></ul></li><li><span class="label">18.
              </span><a name="pone.0000943-Roach1" id="pone.0000943-Roach1"></a>Roach NW, Heron J, McGraw PV (2006) Resolving multisensory conflict: a strategy for balancing the costs and benefits of audio-visual integration. Proc Biol Sci  273: 2159–2168.  <ul class="find" data-citedArticleID="996634" data-doi="10.1098/rspb.2006.3578"><li><a href="http://dx.doi.org/10.1098/rspb.2006.3578" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Resolving+multisensory+conflict%3A+a+strategy+for+balancing+the+costs+and+benefits+of+audio-visual+integration." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Resolving+multisensory+conflict%3A+a+strategy+for+balancing+the+costs+and+benefits+of+audio-visual+integration.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">19.
              </span><a name="pone.0000943-McDonald1" id="pone.0000943-McDonald1"></a>McDonald P (2003) Demonstration by Simulation: the Philosophical Significance of Experiment in Helmholtz's Theory of Perception. Perspectives on Science  11: 170–207.  <ul class="find" data-citedArticleID="996624" data-doi="10.1162/106361403322495885"><li><a href="http://dx.doi.org/10.1162/106361403322495885" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Demonstration+by+Simulation%3A+the+Philosophical+Significance+of+Experiment+in+Helmholtz%27s+Theory+of+Perception." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Demonstration+by+Simulation%3A+the+Philosophical+Significance+of+Experiment+in+Helmholtz%27s+Theory+of+Perception.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">20.
              </span><a name="pone.0000943-Helmholtz1" id="pone.0000943-Helmholtz1"></a>Helmholtz HFv (1863 (1954)) On the sensations of tone as a physiological basis for the theory of music. New York: Dover Publics.   <ul class="find-nolinks"></ul></li><li><span class="label">21.
              </span><a name="pone.0000943-Ernst2" id="pone.0000943-Ernst2"></a>Ernst MO (2007) Learning to integrate arbitrary signals from vision and touch. Journal of Vision in press.   <ul class="find-nolinks"></ul></li><li><span class="label">22.
              </span><a name="pone.0000943-Treisman1" id="pone.0000943-Treisman1"></a>Treisman A (1996) The binding problem. Curr Opin Neurobiol  6: 171–178.  <ul class="find" data-citedArticleID="996652" data-doi="10.1016/s0959-4388(96)80070-5"><li><a href="http://dx.doi.org/10.1016/s0959-4388(96)80070-5" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=The+binding+problem." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22The+binding+problem.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">23.
              </span><a name="pone.0000943-Reynolds1" id="pone.0000943-Reynolds1"></a>Reynolds JH, Desimone R (1999) The role of neural mechanisms of attention in solving the binding problem. Neuron  24: 19–29.  <ul class="find" data-citedArticleID="996632" data-doi="10.1016/s0896-6273(00)80819-3"><li><a href="http://dx.doi.org/10.1016/s0896-6273(00)80819-3" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=The+role+of+neural+mechanisms+of+attention+in+solving+the+binding+problem." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22The+role+of+neural+mechanisms+of+attention+in+solving+the+binding+problem.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">24.
              </span><a name="pone.0000943-Knill1" id="pone.0000943-Knill1"></a>Knill DC (2003) Mixture models and the probabilistic structure of depth cues. Vision Res  43: 831–854.  <ul class="find" data-citedArticleID="996618" data-doi="10.1016/s0042-6989(03)00003-8"><li><a href="http://dx.doi.org/10.1016/s0042-6989(03)00003-8" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Mixture+models+and+the+probabilistic+structure+of+depth+cues." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Mixture+models+and+the+probabilistic+structure+of+depth+cues.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">25.
              </span><a name="pone.0000943-Recanzone1" id="pone.0000943-Recanzone1"></a>Recanzone GH (2003) Auditory influences on visual temporal rate perception. J Neurophysiol  89: 1078–1093.  <ul class="find" data-citedArticleID="996630" data-doi="10.1152/jn.00706.2002"><li><a href="http://dx.doi.org/10.1152/jn.00706.2002" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Auditory+influences+on+visual+temporal+rate+perception." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Auditory+influences+on+visual+temporal+rate+perception.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">26.
              </span><a name="pone.0000943-Choe1" id="pone.0000943-Choe1"></a>Choe CS, Welch RB, Gilford RM, Juola JF (1975) The “ventriloquist effect”: visual dominance or response bias. Perception and Psychophysics  18: 55–60.  <ul class="find" data-citedArticleID="996590" data-doi="10.3758/bf03199367"><li><a href="http://dx.doi.org/10.3758/bf03199367" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=The+%E2%80%9Cventriloquist+effect%E2%80%9D%3A+visual+dominance+or+response+bias." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22The+%E2%80%9Cventriloquist+effect%E2%80%9D%3A+visual+dominance+or+response+bias.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">27.
              </span><a name="pone.0000943-Bertelson1" id="pone.0000943-Bertelson1"></a>Bertelson P, Pavani F, Ladavas E, Vroomen J, de Gelder B (2000) Ventriloquism in patients with unilateral visual neglect. Neuropsychologia  38: 1634–1642.  <ul class="find" data-citedArticleID="996582" data-doi="10.1016/s0028-3932(00)00067-1"><li><a href="http://dx.doi.org/10.1016/s0028-3932(00)00067-1" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Ventriloquism+in+patients+with+unilateral+visual+neglect." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Ventriloquism+in+patients+with+unilateral+visual+neglect.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">28.
              </span><a name="pone.0000943-Burnham1" id="pone.0000943-Burnham1"></a>Burnham KP, Anderson DR (2002) Model Selection and Multimodel Inference: A Practical-Theoretic Approach. New York: Springer.   <ul class="find-nolinks"></ul></li><li><span class="label">29.
              </span><a name="pone.0000943-Adams1" id="pone.0000943-Adams1"></a>Adams WJ, Graf EW, Ernst MO (2004) Experience can change the ‘light-from-above’ prior. Nat Neurosci  7: 1057–1058.  <ul class="find" data-citedArticleID="996576" data-doi="10.1038/nn1312"><li><a href="http://dx.doi.org/10.1038/nn1312" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Experience+can+change+the+%E2%80%98light-from-above%E2%80%99+prior." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Experience+can+change+the+%E2%80%98light-from-above%E2%80%99+prior.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">30.
              </span><a name="pone.0000943-Roskies1" id="pone.0000943-Roskies1"></a>Roskies AL (1999) The binding problem. Neuron  24: 7–9.  <ul class="find" data-citedArticleID="996636" data-doi="10.1016/s0896-6273(00)80817-x"><li><a href="http://dx.doi.org/10.1016/s0896-6273(00)80817-x" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=The+binding+problem." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22The+binding+problem.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">31.
              </span><a name="pone.0000943-Gould1" id="pone.0000943-Gould1"></a>Gould SJ, Lewontin R (1979) The Spandrels of San Marco and the Panglossian Paradigm: A Critique of the Adaptionist Programme. Proceedings of the Royal Society B  205: 581–598.  <ul class="find" data-citedArticleID="996604" data-doi="10.1098/rspb.1979.0086"><li><a href="http://dx.doi.org/10.1098/rspb.1979.0086" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=The+Spandrels+of+San+Marco+and+the+Panglossian+Paradigm%3A+A+Critique+of+the+Adaptionist+Programme." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22The+Spandrels+of+San+Marco+and+the+Panglossian+Paradigm%3A+A+Critique+of+the+Adaptionist+Programme.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">32.
              </span><a name="pone.0000943-Gigerenzer1" id="pone.0000943-Gigerenzer1"></a>Gigerenzer G (2001) The adaptive toolbox: toward a darwinian rationality. Nebr Symp Motiv  47: 113–143.  <ul class="find" data-citedArticleID="996600"><li><a href="http://www.crossref.org/guestquery/?auth2=&amp;atitle2=The+adaptive+toolbox%3A+toward+a+darwinian+rationality.&amp;auth=&amp;atitle=The+adaptive+toolbox%3A+toward+a+darwinian+rationality." target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=The+adaptive+toolbox%3A+toward+a+darwinian+rationality." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22The+adaptive+toolbox%3A+toward+a+darwinian+rationality.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">33.
              </span><a name="pone.0000943-Zemel1" id="pone.0000943-Zemel1"></a>Zemel RS, Dayan P, Pouget A (1998) Probabilistic interpretation of population codes. Neural Comput  10: 403–430.  <ul class="find" data-citedArticleID="996662" data-doi="10.1162/089976698300017818"><li><a href="http://dx.doi.org/10.1162/089976698300017818" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Probabilistic+interpretation+of+population+codes." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Probabilistic+interpretation+of+population+codes.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">34.
              </span><a name="pone.0000943-Pouget1" id="pone.0000943-Pouget1"></a>Pouget A, Dayan P, Zemel RS (2003) Inference and computation with population codes. Annu Rev Neurosci  26: 381–410.  <ul class="find" data-citedArticleID="996628" data-doi="10.1146/annurev.neuro.26.041002.131112"><li><a href="http://dx.doi.org/10.1146/annurev.neuro.26.041002.131112" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Inference+and+computation+with+population+codes." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Inference+and+computation+with+population+codes.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">35.
              </span><a name="pone.0000943-Ma1" id="pone.0000943-Ma1"></a>Ma WJ, Beck JM, Latham PE, Pouget A (2006) Bayesian inference with probabilistic population codes. Nat Neurosci  9: 1432–1438.  <ul class="find" data-citedArticleID="996622" data-doi="10.1038/nn1790"><li><a href="http://dx.doi.org/10.1038/nn1790" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Bayesian+inference+with+probabilistic+population+codes." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Bayesian+inference+with+probabilistic+population+codes.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">36.
              </span><a name="pone.0000943-Buehner1" id="pone.0000943-Buehner1"></a>Buehner MJ, Cheng PW, Clifford D (2003) From covariation to causation: a test of the assumption of causal power. J Exp Psychol Learn Mem Cogn  29: 1119–1140.  <ul class="find" data-citedArticleID="996586" data-doi="10.1037/0278-7393.29.6.1119"><li><a href="http://dx.doi.org/10.1037/0278-7393.29.6.1119" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=From+covariation+to+causation%3A+a+test+of+the+assumption+of+causal+power." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22From+covariation+to+causation%3A+a+test+of+the+assumption+of+causal+power.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">37.
              </span><a name="pone.0000943-Gopnik1" id="pone.0000943-Gopnik1"></a>Gopnik A, Glymour C, Sobel DM, Schulz LE, Kushnir T, et al.  (2004) A theory of causal learning in children: causal maps and Bayes nets. Psychol Rev  111: 3–32.  <ul class="find" data-citedArticleID="996602" data-doi="10.1037/0033-295x.111.1.3"><li><a href="http://dx.doi.org/10.1037/0033-295x.111.1.3" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=A+theory+of+causal+learning+in+children%3A+causal+maps+and+Bayes+nets." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22A+theory+of+causal+learning+in+children%3A+causal+maps+and+Bayes+nets.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">38.
              </span><a name="pone.0000943-Saxe1" id="pone.0000943-Saxe1"></a>Saxe R, Tenenbaum JB, Carey S (2005) Secret agents: inferences about hidden causes by 10- and 12-month-old infants. Psychol Sci  16: 995–1001.  <ul class="find" data-citedArticleID="996640" data-doi="10.1111/j.1467-9280.2005.01649.x"><li><a href="http://dx.doi.org/10.1111/j.1467-9280.2005.01649.x" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Secret+agents%3A+inferences+about+hidden+causes+by+10-+and+12-month-old+infants." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Secret+agents%3A+inferences+about+hidden+causes+by+10-+and+12-month-old+infants.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">39.
              </span><a name="pone.0000943-Griffiths1" id="pone.0000943-Griffiths1"></a>Griffiths TL, Tenenbaum JB (2005) Structure and strength in causal induction. Cognit Psychol  51: 334–384.  <ul class="find" data-citedArticleID="996606" data-doi="10.1016/j.cogpsych.2005.05.004"><li><a href="http://dx.doi.org/10.1016/j.cogpsych.2005.05.004" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Structure+and+strength+in+causal+induction." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Structure+and+strength+in+causal+induction.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">40.
              </span><a name="pone.0000943-Waldmann1" id="pone.0000943-Waldmann1"></a>Waldmann MR (2000) Competition among causes but not effects in predictive and diagnostic learning. J Exp Psychol Learn Mem Cogn  26: 53–76.  <ul class="find" data-citedArticleID="996654" data-doi="10.1037/0278-7393.26.1.53"><li><a href="http://dx.doi.org/10.1037/0278-7393.26.1.53" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Competition+among+causes+but+not+effects+in+predictive+and+diagnostic+learning." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Competition+among+causes+but+not+effects+in+predictive+and+diagnostic+learning.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">41.
              </span><a name="pone.0000943-Tenenbaum1" id="pone.0000943-Tenenbaum1"></a>Tenenbaum JB, Griffiths TL, Kemp C (2006) Theory-based Bayesian models of inductive learning and reasoning. Trends Cogn Sci  10: 309–318.  <ul class="find" data-citedArticleID="996648" data-doi="10.1016/j.tics.2006.05.009"><li><a href="http://dx.doi.org/10.1016/j.tics.2006.05.009" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Theory-based+Bayesian+models+of+inductive+learning+and+reasoning." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Theory-based+Bayesian+models+of+inductive+learning+and+reasoning.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">42.
              </span><a name="pone.0000943-Sobel1" id="pone.0000943-Sobel1"></a>Sobel D, Tenenbaum JB, Gopnik A (2004) Children's causal inferences from indirect evidence: Backwards blocking and Bayesian reasoning in preschoolers. Cognitive Science  28: 303–333.  <ul class="find" data-citedArticleID="996646" data-doi="10.1207/s15516709cog2803_1"><li><a href="http://dx.doi.org/10.1207/s15516709cog2803_1" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Children%27s+causal+inferences+from+indirect+evidence%3A+Backwards+blocking+and+Bayesian+reasoning+in+preschoolers." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Children%27s+causal+inferences+from+indirect+evidence%3A+Backwards+blocking+and+Bayesian+reasoning+in+preschoolers.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">43.
              </span><a name="pone.0000943-Kording1" id="pone.0000943-Kording1"></a>Kording KP, Wolpert DM (2004) The loss function of sensorimotor learning. Proc Natl Acad Sci U S A  101: 9839–9842.  <ul class="find" data-citedArticleID="996620" data-doi="10.1073/pnas.0308394101"><li><a href="http://dx.doi.org/10.1073/pnas.0308394101" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=The+loss+function+of+sensorimotor+learning." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22The+loss+function+of+sensorimotor+learning.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">44.
              </span><a name="pone.0000943-Ghahramani1" id="pone.0000943-Ghahramani1"></a>Ghahramani Z (1995) Computational and psychophysics of sensorimotor integration. Cambridge: Massachusetts Institute of Technology.   <ul class="find-nolinks"></ul></li><li><span class="label">45.
              </span><a name="pone.0000943-Barnes1" id="pone.0000943-Barnes1"></a>Barnes CA (1979) Memory deficits associated with senescence: a neurophysiological and behavioral study in the rat. J Comp Physiol Psychol  93: 74–104.  <ul class="find" data-citedArticleID="996580" data-doi="10.1037/h0077579"><li><a href="http://dx.doi.org/10.1037/h0077579" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Memory+deficits+associated+with+senescence%3A+a+neurophysiological+and+behavioral+study+in+the+rat." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Memory+deficits+associated+with+senescence%3A+a+neurophysiological+and+behavioral+study+in+the+rat.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li></ol></div>

  </div>

      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.XML" value="102604"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.PDF" value="284634"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.g001.PNG_L" value="405254"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.g001.PNG_M" value="69784"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.g001.PNG_S" value="8409"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.g001.TIF" value="685416"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.g001.PNG_I" value="25499"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.e001.PNG" value="6341"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.e001.TIF" value="22308"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.g002.PNG_L" value="744446"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.g002.PNG_M" value="119438"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.g002.PNG_S" value="17459"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.g002.TIF" value="1446230"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.g002.PNG_I" value="101813"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.t001.PNG_L" value="116910"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.t001.PNG_M" value="159797"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.t001.PNG_S" value="18141"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.t001.TIF" value="483704"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.t001.PNG_I" value="56294"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.g003.PNG_L" value="142577"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.g003.PNG_M" value="65670"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.g003.PNG_S" value="6832"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.g003.TIF" value="316632"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.g003.PNG_I" value="24182"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.e002.PNG" value="11186"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.e002.TIF" value="26808"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.e003.PNG" value="14986"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.e003.TIF" value="31120"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.e004.PNG" value="13583"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.e004.TIF" value="30220"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.e005.PNG" value="20397"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.e005.TIF" value="36416"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.e006.PNG" value="18097"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.e006.TIF" value="35100"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.e007.PNG" value="19556"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.e007.TIF" value="35252"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.e008.PNG" value="7681"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.e008.TIF" value="23668"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.e009.PNG" value="8564"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.e009.TIF" value="24492"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.e010.PNG" value="16810"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.e010.TIF" value="33332"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.e011.PNG" value="9759"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.e011.TIF" value="26076"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.e012.PNG" value="9724"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.e012.TIF" value="26068"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.e013.PNG" value="14594"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.e013.TIF" value="30112"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.e014.PNG" value="13037"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.e014.TIF" value="28324"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.e015.PNG" value="11425"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.e015.TIF" value="28156"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.e016.PNG" value="10523"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.e016.TIF" value="26504"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.e017.PNG" value="6393"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.e017.TIF" value="22352"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.e018.PNG" value="6240"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.e018.TIF" value="22236"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.e019.PNG" value="9097"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.e019.TIF" value="25252"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.e020.PNG" value="10519"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.e020.TIF" value="26728"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.s001.DOC" value="111104"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.s002.EPS" value="1148423"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0000943.s003.EPS" value="938159"/>

</div>
<div class="sidebar">

  <div class="article-actions cf">
      <div class="download">
        <span class="btn"><a href="/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0000943&amp;representation=PDF" title="Download" target="_blank">Download PDF</a></span>
      </div>
      <div class="btn-reveal dropdown">
        <div class="dropdown-icon">
          <span class="btn">&nbsp;</span>
        </div>

        <div class="content">
          <ul class="bullet">
            <li><a href="/article/citationList.action?articleURI=info%3Adoi%2F10.1371%2Fjournal.pone.0000943" title="Download citations">Citation</a></li>
            <li><a href="/article/fetchObjectAttachment.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0000943&amp;representation=XML" title="Download article XML">XML</a></li>
          </ul>
        </div>
      </div> <!-- end btn-reveal dropdown-->


    <div class="btn-reveal flt-l">
        <span class="btn">Print</span>
        <div class="content">
            <ul class="bullet">
                <li id="print-article"><a href="#" onclick="if(typeof(_gaq) != 'undefined'){ _gaq.push(['_trackEvent','Article', 'Print', 'Click']); } window.print(); return false;" title="Print Article">Print article</a></li>
                <li>
                  <a href="https://www.odysseypress.com/onlinehost/reprint_order.php?type=A&page=0&journal=7&doi=10.1371/journal.pone.0000943&volume=&issue=&title=Causal Inference in Multisensory Perception&author_name=Konrad%20P.%20K%C3%B6rding%2C%20Ulrik%20Beierholm%2C%20Wei%20Ji%20Ma%2C%20Steven%20Quartz%2C%20Joshua%20B.%20Tenenbaum%2C%20Ladan%20Shams&start_page=1&end_page=10" title="Odyssey Press">EzReprint</a>
                </li>
            </ul>
        </div>
    </div>

    <div class="btn-reveal flt-r">
        <span class="btn">Share</span>
        <div class="content">
            <ul class="social">
                <li><a href="http://www.reddit.com/submit?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0000943" target="_blank" title="Submit to Reddit"><img src="/images/icon.reddit.16.png" width="16" height="16" alt="Reddit">Reddit</a></li>

                <li><a href="https://plus.google.com/share?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0000943" target="_blank" title="Share on Google+"><img src="/images/icon.gplus.16.png" width="16" height="16" alt="Google+">Google+</a></li>

                <li><a href="http://www.stumbleupon.com/submit?url=http%3A%2F%2Fwww.plosone.org%2Farticle%2Finfo%253Adoi%252F10.1371%252Fjournal.pone.0000943" target="_blank" title="Add to StumbleUpon"><img src="/images/icon.stumble.16.png" width="16" height="16" alt="StumbleUpon">StumbleUpon</a></li>

                <li><a href="http://www.facebook.com/share.php?u=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0000943&amp;t=Causal%20Inference%20in%20Multisensory%20Perception" target="_blank" title="Share on Facebook"><img src="/images/icon.fb.16.png" width="16" height="16" alt="Facebook">Facebook</a></li>

                <li><a href="http://www.linkedin.com/shareArticle?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0000943&title=Causal%20Inference%20in%20Multisensory%20Perception&summary=Checkout%20this%20article%20I%20found%20at%20PLOS" target="_blank" title="Add to LinkedIn"><img src="/images/icon.linkedin.16.png" width="16" height="16" alt="Mendeley">LinkedIn</a></li>

                <li><a href="http://www.citeulike.org/posturl?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0000943&amp;title=Causal%20Inference%20in%20Multisensory%20Perception" target="_blank" title="Add to CiteULike"><img src="/images/icon.cul.16.png" width="16" height="16" alt="CiteULike">CiteULike</a></li>

                <li><a href="http://www.mendeley.com/import/?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0000943" target="_blank" title="Add to Mendeley"><img src="/images/icon.mendeley.16.png" width="16" height="16" alt="Mendeley">Mendeley</a></li>

                <li><a href="https://www.pubchase.com/library?add_aid=10.1371%2Fjournal.pone.0000943&amp;source=plos" target="_blank" title="Add to PubChase"><img src="/images/icon.pc.16.png" width="16" height="16" alt="PubChase">PubChase</a></li>


                <script type="text/javascript">
                    // replace tweet with one that's pre-shortened to 140 chars
                    function truncateTweetText() {
                        var twtTitle = 'Causal Inference in Multisensory Perception';
                        var twtUrl = 'http://dx.plos.org/10.1371/journal.pone.0000943';
                        // all URLs posted to twitter get auto-shortened to 20 chars.
                        var maxLength = 140 - (20 + 1);
                        // truncate the title to include space for twtTag and ellipsis (here, 10 = tag length + space + ellipsis)
                        if (twtTitle.length > maxLength) { twtTitle = twtTitle.substr(0, (maxLength - 10)) + '...'; }
                        // set the href to use the shortened tweet
                        $('#twitter-share-link').prop('href', 'http://twitter.com/intent/tweet?text=' + encodeURIComponent('#PLOSONE: ' + twtTitle + ' ' + twtUrl));
                    }
                </script>
                <li><a href="http://twitter.com/intent/tweet?text=#PLOSONE%3A%20Causal%20Inference%20in%20Multisensory%20Perception http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0000943" onclick="truncateTweetText();" target="_blank" title="Share on Twitter" id="twitter-share-link"><img src="/images/icon.twtr.16.png" width="16" height="16" alt="Twitter">Twitter</a></li>

                <li><a href="/article/email/info%3Adoi%2F10.1371%2Fjournal.pone.0000943" title="Email this article"><img src="/images/icon.email.16.png" width="16" height="16" alt="Email">Email</a></li>
            </ul>
        </div>
    </div><!--end btn-reveal flt-r-->
</div><!-- end article-actions-->

<!-- begin Crossmark -->

<a id="open-crossmark" href="#" style="margin-top: -28px; display:block"><img style="border: 0; display: none;
 padding: 10px 0 18px 0;"  id="crossmark-icon" src="/images/logo-crossmark-bw.png" /></a>
<div id="crossmark-dialog" style="display: none;" title="">
    <!-- the external CrossMark data is loaded inside this iframe -->
    <iframe id="crossmark-dialog-frame" frameborder="0"></iframe>
</div>

<!-- end crossmark -->


<div class="block" id="subject-area-sidebar-block">
    <div class="header">
        <h3>Subject Areas</h3><div title="More information" id="subject-area-sidebar-block-help-icon"><img align="right"
                                                                                                           alt="info" src="/images/button_info.png"/><div id="subject-area-sidebar-block-help"><img align="right"
                                                                                                                                                                                                    src="/images/button_info.png"/><p>
        <b>We want your feedback.</b> Do these subject areas make sense for this article? If not, click the flag
        next to the incorrect subject area and we will review it. Thanks for your help!
    </p></div></div>
    </div>


    <ul id="subject-area-sidebar-list">



















          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Bayes+theorem%22" title="Search for articles in the subject area:'Bayes theorem'"><div class="flagText">Bayes theorem</div></a>
              <div data-categoryid="32251" data-articleid="24724"
                   data-categoryname="Bayes theorem"
                   class="flagImage" title="Flag 'Bayes theorem' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Behavior%22" title="Search for articles in the subject area:'Behavior'"><div class="flagText">Behavior</div></a>
              <div data-categoryid="32855" data-articleid="24724"
                   data-categoryname="Behavior"
                   class="flagImage" title="Flag 'Behavior' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Human+performance%22" title="Search for articles in the subject area:'Human performance'"><div class="flagText">Human performance</div></a>
              <div data-categoryid="18145" data-articleid="24724"
                   data-categoryname="Human performance"
                   class="flagImage" title="Flag 'Human performance' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Nervous+system%22" title="Search for articles in the subject area:'Nervous system'"><div class="flagText">Nervous system</div></a>
              <div data-categoryid="48377" data-articleid="24724"
                   data-categoryname="Nervous system"
                   class="flagImage" title="Flag 'Nervous system' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Sensory+cues%22" title="Search for articles in the subject area:'Sensory cues'"><div class="flagText">Sensory cues</div></a>
              <div data-categoryid="46249" data-articleid="24724"
                   data-categoryname="Sensory cues"
                   class="flagImage" title="Flag 'Sensory cues' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Sensory+perception%22" title="Search for articles in the subject area:'Sensory perception'"><div class="flagText">Sensory perception</div></a>
              <div data-categoryid="46099" data-articleid="24724"
                   data-categoryname="Sensory perception"
                   class="flagImage" title="Flag 'Sensory perception' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Vision%22" title="Search for articles in the subject area:'Vision'"><div class="flagText">Vision</div></a>
              <div data-categoryid="32965" data-articleid="24724"
                   data-categoryname="Vision"
                   class="flagImage" title="Flag 'Vision' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Visual+signals%22" title="Search for articles in the subject area:'Visual signals'"><div class="flagText">Visual signals</div></a>
              <div data-categoryid="35641" data-articleid="24724"
                   data-categoryname="Visual signals"
                   class="flagImage" title="Flag 'Visual signals' as inappropriate"></div>
          </li>
    </ul>
</div>

<div class="ad">
    <div class="title">Advertisement</div>






  <iframe id='a0852f54' name='a0852f54'
    src='http://ads.plos.org/www/delivery/afr.php?zoneid=381&amp;cb=3379'
    frameborder='0' scrolling='no' width='160' height='600'>
    <a href='http://ads.plos.org/www/delivery/ck.php?n=a0852f54&amp;cb=9175'
      target='_top'><img src='http://ads.plos.org/www/delivery/avw.php?zoneid=381&amp;cb=2754&amp;n=a0852f54'
      border='0' alt=''/>
    </a>
  </iframe>



</div>

<div id="twitter-alm-timeline" class="twitter-alm-timeline"></div>

<div class="block sidebar-comments">
    <div class="header">
        <h3>Comments</h3>
    </div>
      <p><a href="/annotation/listThread.action?root=7453">Used paper in class</a><br>Posted by bencip</p>
      <p><a href="/annotation/listThread.action?root=5741">full citation</a><br>Posted by koerding</p>
      <p><a href="/annotation/listThread.action?root=15029">strong fusion</a><br>Posted by koerding</p>
</div>

</div><!-- sidebar -->
    </div>
  </div>
</div>
<script src="http://wl.figshare.com/static/p_widget.js" type="text/javascript"></script><div id="pageftr">
  <div class="ftr-cols cf">
    <div class="col col-1">
      <img src="/images/logo-plos-footer.png" alt="PLOS Logo" class="logo" />
      <p><a href="/static/releaseNotes">Ambra 2.9.16</a> Managed Colocation provided <br />by <a href="http://www.isc.org/">Internet Systems Consortium</a>.<p>
      <div class="nav nav-aux">
        <a href="/static/privacy">Privacy Policy</a> |
        <a href="/static/terms">Terms of Use</a> |
        <a href="http://www.plos.org/advertise/">Advertise</a> |
        <a href="http://www.plos.org/about/media-inquiries/">Media Inquiries</a>
      </div>
    </div>
    <div class="col col-2">
      <p><a href="http://www.plos.org/publications/journals/">Publications</a></p>
      <div class="nav">
        <ul>
          <li><a href="http://www.plosbiology.org">PLOS Biology</a></li>
          <li><a href="http://www.plosmedicine.org">PLOS Medicine</a></li>
          <li><a href="http://www.ploscompbiol.org">PLOS Computational Biology</a></li>
          <li><a href="http://currents.plos.org">PLOS Currents</a></li>
          <li><a href="http://www.plosgenetics.org">PLOS Genetics</a></li>
          <li><a href="http://www.plospathogens.org">PLOS Pathogens</a></li>
          <li><a href="http://www.plosone.org">PLOS ONE</a></li>
          <li><a href="http://www.plosntds.org">PLOS Neglected Tropical Diseases</a></li>
        </ul>
      </div>
    </div>
    <div class="col col-3">
      <div class="nav">
        <p><a href="http://www.plos.org">plos.org</a></p>
        <p><a href="http://blogs.plos.org">Blogs</a></p>
        <p><a href="http://www.ploscollections.org">Collections</a></p>
        <p><a href="/feedback/new">Send us feedback</a></p>

        <p>California (US) corporation #C2354500, based in San Francisco</p>
      </div>
    </div>
  </div>
</div><!-- pageftr -->

</div><!-- end page-wrap, this div is in header.ftl -->
<script type="text/javascript" src="/javascript/jquery-1.8.1-min.js?v=Tm7VCOzZz3lE03ghpkS6SWkHbyI"></script>
<script type="text/javascript" src="/javascript/ga-min.js?v=lNQ4gt8QcPDatjsdOFl_FGpPhLY"></script>
<script type="text/javascript" src="/javascript/jquery.hoverIntent-min.js?v=mRiGNYY9cIXxVb8u0K_MdW7hHnc"></script>
<script type="text/javascript" src="/javascript/jquery.placeholder-min.js?v=21Pn56Ur9h1N4K4VZDa0nqI3Pxo"></script>
<script type="text/javascript" src="/javascript/jquery.jsonp-2.4.0-min.js?v=lqTpzoHfSq3I5Ygo01qq5WankEo"></script>
<script type="text/javascript" src="/javascript/jquery-ui-1.9.2.custom-min.js?v=raSSlfNO0YsV5uUpAKmTB9n5VTc"></script>
<script type="text/javascript" src="/javascript/jquery.tooltip-min.js?v=cw+6Smh+mdryIA25xvqIvHMrnZM"></script>
<script type="text/javascript" src="/javascript/jquery.uniform-min.js?v=kYUAnX6W2W_2fK3RIuQ2m_YFG9U"></script>
<script type="text/javascript" src="/javascript/jquery.pjax-min.js?v=939kLBjL5_YKbx71T1RHjYaD4l8"></script>
<script type="text/javascript" src="/javascript/imagesloaded-min.js?v=XeuAp8Gc3mvQUo+wZCSF8ttPwvw"></script>
<script type="text/javascript" src="/javascript/figviewer-min.js?v=yPUa0sUQ_iHkI+IRv2i9bjyZJFo"></script>
<script type="text/javascript" src="/javascript/global-min.js?v=0Q3PwjeaWtXYDnqIsQvnL_ou0qs"></script>
<script type="text/javascript" src="/javascript/jquery.touchswipe-min.js?v=huaek_e6HqTduvCNAN91dJolTyw"></script>
<script type="text/javascript" src="/javascript/jquery.base64-min.js?v=VwV1zeVqKZj5FCAdlK0q5NRxbBg"></script>
<script type="text/javascript" src="/javascript/alm-min.js?v=Y5gm6B0b4Kx2YHNObNrgEeBgXlY"></script>
<script type="text/javascript" src="/javascript/taxonomy-browser-min.js?v=vBVMuDMYkGJCXIUxLe35GoyiJNw"></script>
<script type="text/javascript" src="/javascript/jquery.filterize-min.js?v=j0ZKVnHyk2nhFy8eIuNJkp7xaM0"></script>
<script type="text/javascript" src="/javascript/plosone-min.js?v=TK4H4arL_XBSwwJq+K1N3kqYfAI"></script>
<script type="text/javascript" src="/javascript/twitter-min.js?v=xKgcxLsQFXy+at1ao1NVke8nFlM"></script>
<script type="text/javascript" src="/javascript/crossmark.1.4-min.js?v=3FO4k0SjwTaGNnKGNSqthar1080"></script>
<script type="text/javascript">
  var _sf_async_config={uid:16579,domain:"plosone.org"};
  (function(){
    function loadChartbeat() {
      window._sf_endpt=(new Date()).getTime();
      var e = document.createElement('script');
      e.setAttribute('language', 'javascript');
      e.setAttribute('type', 'text/javascript');
      e.setAttribute('src',
          (("https:" == document.location.protocol) ? "https://a248.e.akamai.net/chartbeat.download.akamai.com/102508/" : "http://static.chartbeat.com/") +
              "js/chartbeat.js");
      document.body.appendChild(e);
    }
    var oldonload = window.onload;
    window.onload = (typeof window.onload != 'function') ?
        loadChartbeat : function() { oldonload(); loadChartbeat(); };
  })();
</script>
<!-- <script type="application/javascript" src="http://crossmark.crossref.org/javascripts/v1.3/crossmark.min.js"></script> -->

</body>
</html>
