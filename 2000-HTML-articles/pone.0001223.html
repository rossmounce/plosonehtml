

 



<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"
      xmlns:foaf="http://xmlns.com/foaf/0.1/"
      xmlns:dc="http://purl.org/dc/terms/"
      xmlns:doi="http://dx.doi.org/"
      xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
      xmlns:xsd="http://www.w3.org/2001/XMLSchema-datatypes#"
      lang="en" xml:lang="en"
      itemscope itemtype="http://schema.org/Article"
      class="no-js">
<head prefix="og: http://ogp.me/ns#">
  <title>PLOS ONE: The Effect of Real-World Personal Familiarity on the Speed of Face Information Processing</title>


<link rel="stylesheet" type="text/css"  href="/css/global-min.css?v=izteQ6tu7kgsJZW_xmrYizvKiHM" />


    <!--[if lte IE 7]>
<link rel="stylesheet" type="text/css"  href="/css/lte_ie7-min.css?v=3bykQUyQmReeuobVyPozcJ9LxRc" />
    <![endif]-->


<link rel="stylesheet" type="text/css"  href="/css/jquery-ui-min.css?v=eXDHTEJM0lIAmDe5k0I0Ad4nxNo" />


<link rel="stylesheet" type="text/css"  href="/css/journal.css?v=T7ZVxJfgk9jNxLAJ2qHz1vZpgYU" />


<link rel="stylesheet" type="text/css" media="print" href="/css/print-min.css?v=T5lb0B3q6EXBsuDluc5V5w+AkRc" />


  <link rel="stylesheet" href="http://f.fontdeck.com/s/css/js/www.plosone.org/24557.css" type="text/css"/>

  <!--chartbeat -->
  <script type="text/javascript">var _sf_startpt = (new Date()).getTime()</script>
  <script>document.documentElement.className += ' js';</script>

  <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <meta http-equiv="X-UA-Compatible" content="IE=EmulateIE7; IE=EmulateIE9"/>
  <meta name="description" content="PLOS ONE: an inclusive, peer-reviewed, open-access resource from the PUBLIC LIBRARY OF SCIENCE. Reports of well-performed scientific studies from all disciplines freely available to the whole world."/>
  <meta name="keywords" content="PLOS, Public Library of Science, Open Access, Open-Access, Science, Medicine, Biology, Research, Peer-review, Inclusive, Interdisciplinary, Ante-disciplinary, Physics, Chemistry, Engineering"/>
  <meta name="almHost" content="http://alm.plos.org/api/v3/articles"/>
  <meta name="searchHost" content="http://api.plos.org/search" />
  <meta name="termsHost" content="http://api.plos.org/terms" />
  <meta name="solrApiKey" content="plos"/>
  <meta name="almAPIKey" content="3pezRBRXdyzYW6ztfwft" />
  <meta name="currentJournal" content="PLoSONE" />
  <meta name="almRequestBatchSize" content="" />

  <meta name="citation_publisher" content="Public Library of Science"/>
  <meta name="citation_doi" content="10.1371/journal.pone.0001223"/>
  <meta name="dc.identifier" content="10.1371/journal.pone.0001223" />

    <meta name="citation_title" content="The Effect of Real-World Personal Familiarity on the Speed of Face Information Processing"/>
    <meta itemprop="name" content="The Effect of Real-World Personal Familiarity on the Speed of Face Information Processing"/>

      <meta name="citation_author" content="Benjamin Balas"/>
            <meta name="citation_author_institution" content="Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology, Cambridge, Massachusetts, United States of America"/>
      <meta name="citation_author" content="David Cox"/>
            <meta name="citation_author_institution" content="Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology, Cambridge, Massachusetts, United States of America"/>
      <meta name="citation_author" content="Erin Conwell"/>
            <meta name="citation_author_institution" content="Department of Cognitive and Linguistic Sciences, Brown University, Providence, Rhode Island, United States of America"/>

    <meta name="citation_date" content="2007/11/21"/>

  <meta name="citation_pdf_url" content="http://dx.plos.org/10.1371/journal.pone.0001223.pdf" />

      <meta name="citation_journal_title" content="PLOS ONE" />
    <meta name="citation_firstpage" content="e1223"/>
    <meta name="citation_issue" content="11"/>
    <meta name="citation_volume" content="2"/>
    <meta name="citation_issn" content="1932-6203"/>

    <meta name="citation_journal_abbrev" content="PLoS ONE" />

      <meta name="citation_reference" content="citation_title=The lasting impression of chairman Mao: hyperfidelity of familiar-face memory.; citation_author=L Ge; citation_author=J Luo; citation_author=M Nishimuira; citation_author=K Lee; citation_journal_title=Perception; citation_volume=32(5); citation_number=1; citation_pages=601-614; citation_date=2003; " />
      <meta name="citation_reference" content="citation_title=Familiarisation with faces selectively enhances sensitivity to changes made to the eyes.; citation_author=C O'Donnell; citation_author=V Bruce; citation_journal_title=Perception; citation_volume=30; citation_number=2; citation_pages=755-764; citation_date=2001; " />
      <meta name="citation_reference" content="citation_title=Matching identities of familiar and unfamiliar faces caught on CCTV images.; citation_author=V Bruce; citation_author=Z Henderson; citation_author=C Newman; citation_author=AM Burton; citation_journal_title=Journal of Experimental Psychology (Applied); citation_volume=7(3); citation_number=3; citation_pages=207-218; citation_date=2001; " />
      <meta name="citation_reference" content="citation_title=Recognition of unfamiliar faces.; citation_author=PJB Hancock; citation_author=V Bruce; citation_author=M Burton; citation_journal_title=Trends in Cognitive Science; citation_volume=4(9); citation_number=4; citation_pages=330-337; citation_date=2000; " />
      <meta name="citation_reference" content="citation_title=Understanding face recognition.; citation_author=V Bruce; citation_author=A Young; citation_journal_title=British Journal of Psychology; citation_volume=77; citation_number=5; citation_pages=305-327; citation_date=1986; " />
      <meta name="citation_reference" content="citation_title=Influences of familiarity on the processing of faces.; citation_author=V Bruce; citation_journal_title=Perception; citation_volume=15; citation_number=6; citation_pages=387-397; citation_date=1986; " />
      <meta name="citation_reference" content="citation_title=Matching familiar and unfamilar faces on identity and expression.; citation_author=AW Young; citation_author=KH McWeeny; citation_author=DC Hay; citation_author=AW Ellis; citation_journal_title=Psychological Research; citation_volume=48; citation_number=7; citation_pages=63-68; citation_date=1986; " />
      <meta name="citation_reference" content="citation_title=Is sex categorisation from faces really parallel to face recognition?; citation_author=B Rossion; citation_journal_title=Visual Cognition; citation_volume=9; citation_number=8; citation_pages=1003-1020; citation_date=2002; " />
      <meta name="citation_reference" content="citation_title=Ethnic categorisation of faces is not independent of face identity.; citation_author=R Bruyer; citation_author=S Leclere; citation_author=P Quinet; citation_journal_title=Perception; citation_volume=33(2); citation_number=9; citation_pages=169-179; citation_date=2004; " />
      <meta name="citation_reference" content="citation_title=Positive facial affect facilitates the identification of famous faces.; citation_author=DR Gallegos; citation_author=D Tranel; citation_journal_title=Brain Lang.; citation_volume=93(3); citation_number=10; citation_pages=338-348; citation_date=2005; " />
      <meta name="citation_reference" content="citation_title=Expression influences the recognition of familiar faces.; citation_author=JM Kaufmann; citation_author=SR Schweinberger; citation_journal_title=Perception; citation_volume=33(4); citation_number=11; citation_pages=399-408; citation_date=2004; " />
      <meta name="citation_reference" content="citation_title=Facial identity and facial speech processing: familiar faces and voices in the McGurk effect.; citation_author=S Walker; citation_author=V Bruce; citation_author=C O'Malley; citation_journal_title=Perceptual Psychophysics; citation_volume=57(8); citation_number=12; citation_pages=1124-33; citation_date=1995; " />
      <meta name="citation_reference" content="citation_title=Exploring levels of face familiarity by using an indirect face-matching measure.; citation_author=R Clutterbuck; citation_author=RA Johnston; citation_journal_title=Perception; citation_volume=31; citation_number=13; citation_pages=985-994; citation_date=2002; " />
      <meta name="citation_reference" content="citation_title=Recognising faces.; citation_author=V Bruce; citation_journal_title=Philosophical Transactions of the Royal Society of London, Series B: Biological Sciences; citation_volume=302; citation_number=14; citation_pages=423-436; citation_date=1983; " />
      <meta name="citation_reference" content="citation_title=The psychophysics toolbox.; citation_author=DH Brainard; citation_journal_title=Spatial Vision; citation_volume=10; citation_number=15; citation_pages=433-436; citation_date=1997; " />
      <meta name="citation_reference" content="citation_title=The VideoToolbox software for visual psychophysics: transforming numbers into movies.; citation_author=DG Pelli; citation_journal_title=Spatial Vision; citation_volume=10; citation_number=16; citation_pages=437-442; citation_date=1997; " />
      <meta name="citation_reference" content="citation_title=Matching familiar and unfamiliar faces on internal and external features.; citation_author=AW Young; citation_author=DC Hay; citation_author=KH McWeeny; citation_author=BM Flude; citation_author=AW Ellis; citation_journal_title=Perception; citation_volume=14; citation_number=17; citation_pages=737-746; citation_date=1985; " />
      <meta name="citation_reference" content="citation_title=Bubbles: a technique to reveal the use of information in recognition tasks.; citation_author=F Gosselin; citation_author=PG Schyns; citation_journal_title=Vision Research; citation_volume=41(17); citation_number=18; citation_pages=2261-2271; citation_date=2001; " />
      <meta name="citation_reference" content="citation_title=Show me the features! Understanding recognition from the use of visual information.; citation_author=PG Schyns; citation_author=L Bonnar; citation_author=F Gosselin; citation_journal_title=Psychological Science; citation_volume=13; citation_number=19; citation_pages=402-409; citation_date=2002; " />
      <meta name="citation_reference" content="citation_title=Effects of lighting on the perception of facial surfaces.; citation_author=H Hill; citation_author=V Bruce; citation_journal_title=Journal of Experimental Psychology: Human Perception and Performance; citation_volume=22(4); citation_number=20; citation_pages=986-1004; citation_date=1996; " />
      <meta name="citation_reference" content="citation_title=Face recognition under varying poses: the role of texture and shape.; citation_author=NF Troje; citation_author=HH Bulthoff; citation_journal_title=Vision Research; citation_volume=36(12); citation_number=21; citation_pages=1761-1771; citation_date=1996; " />
      <meta name="citation_reference" content="citation_title=Viewpoint-dependent recognition of familiar faces.; citation_author=NF Troje; citation_author=D Kersten; citation_journal_title=Perception; citation_volume=28; citation_number=22; citation_pages=483-487; citation_date=1999; " />
      <meta name="citation_reference" content="citation_title=A unified account of the effects of distinctiveness, inversion, and race in face recognition.; citation_author=T Valentine; citation_journal_title=Quarterly Journal of Experimental Psychology A; citation_volume=43A; citation_number=23; citation_pages=89-94; citation_date=1991; " />
      <meta name="citation_reference" content="citation_title=Prototype-referenced shape encoding revealed by high-level after-effects.; citation_author=DA Leopold; citation_author=AJ O'Toole; citation_author=T Vetter; citation_author=V Blanz; citation_journal_title=Nature Neuroscience; citation_volume=4; citation_number=24; citation_pages=84-89; citation_date=2001; " />
      <meta name="citation_reference" content="citation_title=Recognizing Faces of Other Ethnic Groups: An Integration of Theories.; citation_author=SL Sporer; citation_journal_title=Psychology, Public Policy, and Law; citation_volume=7(1); citation_number=25; citation_pages=36-97; citation_date=2001; " />
      <meta name="citation_reference" content="citation_title=Becoming a “Greeble” expert: Exploring the face recognition mechanism.; citation_author=I Gauthier; citation_author=MJ Tarr; citation_journal_title=Vision Research; citation_volume=37(12); citation_number=26; citation_pages=1673-1682; citation_date=1997; " />
      <meta name="citation_reference" content="citation_title=Effects of temporal association on recognition memory.; citation_author=G Wallis; citation_author=HH Bulthoff; citation_journal_title=Proceedings of the National Academy of Sciences; citation_volume=98(8); citation_number=27; citation_pages=4800-4804; citation_date=2001; " />
      <meta name="citation_reference" content="citation_title=‘Breaking’ position-invariant object recognition.; citation_author=DD Cox; citation_author=P Meier; citation_author=N Oertelt; citation_author=JJ DiCarlo; citation_journal_title=Nature Neuroscience; citation_volume=8(9); citation_number=28; citation_pages=1145-1147; citation_date=2005; " />

  <link rel="canonical" href="http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0001223" />

    <meta name="twitter:card" content="summary"/>
    <meta name="twitter:site" content="@plosone"/>
    <meta name="twitter:title" content="The Effect of Real-World Personal Familiarity on the Speed of Face Information Processing"/>
    <meta name="twitter:description" content="BackgroundPrevious studies have explored the effects of familiarity on various kinds of visual face judgments, yet the role of familiarity in face processing is not fully understood. Across different face judgments and stimulus sets, the data is equivocal as to whether or not familiarity impacts recognition processes.Methodology/Principal FindingsHere, we examine the effect of real-world personal familiarity in three simple delayed-match-to-sample tasks in which subjects were required to match faces on the basis of orientation (upright v. inverted), gender and identity. We find that subjects had a significant speed advantage with familiar faces in all three tasks, with large effects for the gender and identity matching tasks.Conclusion/SignificanceOur data indicates that real-world experience with a face exerts a powerful influence on face processing in tasks where identity information is irrelevant, even in tasks that could in principle be solved via low-level cues. These results underscore the importance of experience in shaping visual recognition processes."/>
      <meta name="twitter:image" content="http://dx.plos.org/10.1371/journal.pone.0001223.g003"/>

  <meta property="og:title" content="The Effect of Real-World Personal Familiarity on the Speed of Face Information Processing" />
  <meta property="og:type" content="article" />
  <meta property="og:url" content="http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0001223" />

 <!--end articleInfoX-->

  <link rel="pingback" href="http://www.plosone.org/pingback" />


  <link rel="shortcut icon" href="/images/favicon.ico" type="image/x-icon"/>
  <link rel="home" title="home" href="/"/>
  <link rel="alternate" type="application/rss+xml"
        title="PLOS ONE: New Articles"
        href="http://www.plosone.org/article/feed"/>
</head>
<body>

  <div id="page-wrap">
    <div id="topbanner" class="cf">

<!-- Div for the ad at the top of journal home page-->
<div class="center">
  <div class="title">Advertisement</div>
  <iframe id='a3ac9da4' name='a3ac9da4'
    src='http://ads.plos.org/www/delivery/afr.php?zoneid=345&amp;cb=1933'
    frameborder='0' scrolling='no' width='730' height='90'>
    <a href='http://ads.plos.org/www/delivery/ck.php?n=a3ac9da4&amp;cb=6818'
      target='_top'><img src='http://ads.plos.org/www/delivery/avw.php?zoneid=345&amp;cb=1278&amp;n=a3ac9da4'
      border='0' alt=''/>
    </a>
  </iframe>
</div>    </div>

    <div id="pagehdr-wrap">
      <div id="pagehdr">
        <div id="user" class="nav">
          <ul>
            <li><a href="http://www.plos.org">plos.org</a></li>
            <li><a href="https://register.plos.org/ambra-registration/register.action">create account</a></li>
            <li class="btn-style"><a
              href="/user/secure/secureRedirect.action?goTo=%2Farticle%2FfetchArticle.action%3FarticleURI%3Dinfo%253Adoi%252F10.1371%252Fjournal.pone.0001223">sign in</a>
            </li>
          </ul>
        </div>
        <div class="logo">
          <a href="/"><img src="/images/logo.png" alt="PLOS ONE"></a>
        </div>

<div id="nav-main" class="nav">
  <ul>
        <li id="mn-01"><a href="/taxonomy" class="areas-link">Subject Areas</a></li>
    <li id="mn-02"><a href="javascript:void(0);">For Authors</a>
      <div class="submenu" style="width: 540px; margin-left: -300px;">
        <div class="block">
          <div class="submit-script">
            <h3>Submit your Manuscript</h3>
            <ul>
              <li>Fair, rigorous peer review</li>
              <li>Broad scope and wide reach</li>
            </ul>
            <a href="/static/submissionInstructions" class="btn">get started</a>
          </div>
        </div>
        <div class="menu">
          <ul>
            <li><a href="/static/publish">Why Publish with PLOS ONE</a></li>
            <li><a href="/static/publication">Publication Criteria</a></li>
            <li><a href="/static/editorial">Editorial Policies</a></li>
            <li><a href="/static/guidelines">Preparing A Manuscript</a></li>
            <li><a href="/static/figureGuidelines">Figure and Table Guidelines</a></li>
          <li><a href="/static/supportingInformation">Supporting Information Guidelines</a></li>
            <li><a href="/static/submissionInstructions">Submitting a Manuscript</a></li>
          </ul>
        </div>
      </div>
    </li>

    <li id="mn-03"><a href="javascript:void(0);">About Us</a>
      <div class="submenu" style="width:248px; margin-left:-30px;">
        <div class="menu">
          <ul>
            <li><a href="/static/information">Journal Information</a></li>
            <li><a href="/static/edboard">Editorial Board</a></li>
            <li><a href="/static/reviewerGuidelines">Reviewer Guidelines</a></li>
            <li><a href="/static/almInfo">Article-Level Metrics</a></li>
            <li><a href="/static/license">Open-Access License</a></li>
            <li><a href="/static/downloads">Media Downloads</a></li>
            <li><a href="/static/commentGuidelines">Guidelines for Comments</a></li>
            <li><a href="/static/corrections">Corrections</a></li>
            <li><a href="/static/help">Help Using this Site</a></li>
            <li><a href="/static/contact">Contact Us</a></li>
          </ul>
        </div>
      </div>
    </li>
  </ul>
<div id="db">
  <form name="searchForm" action="/search/simple?noSearchFlag=true&amp;query=&amp;articleURI=info%3Adoi%2F10.1371%2Fjournal.pone.0001223" method="get" >
<input type="hidden" name="from" value="globalSimpleSearch" id="from"/><input type="hidden" name="filterJournals" value="PLoSONE" id="filterJournals"/>    <fieldset>
      <legend>Search</legend>
      <label for="search">Search</label>
      <div class="wrap">
        <input id="search" type="text" name="query" placeholder="Search">
        <input type="image" alt="SEARCH" src="/images/icon.search.gif">
      </div>
    </fieldset>
  </form>
    <a id="advSearch" href="/search/advanced?noSearchFlag=true&amp;query=&amp;articleURI=info%3Adoi%2F10.1371%2Fjournal.pone.0001223&filterJournals=PLoSONE">advanced search</a>
</div></div>

      </div>
      <!-- pagehdr-->
    </div>
    <!-- pagehdr-wrap -->

  <!--body and html tags gets closed in global_footer.ftl-->
<script type="text/javascript" src="/javascript/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<div id="pagebdy-wrap">
  <div id="pagebdy">

    <div id="article-block" class="cf">

<div class="article-meta cf">
  <ul id="almSignPost" style="display: none;"></ul>
  <div class="article-type">
    <span class="type oa">Open Access</span>
      <span class="type pr">Peer-Reviewed</span>
  </div>
</div>

<div class="header" id="hdr-article">

<div class="article-kicker">
      <span id="article-type-heading">
        Research Article
      </span>
</div>  <h1 property="dc:title" datatype="" rel="dc:type" href="http://purl.org/dc/dcmitype/Text">
    The Effect of Real-World Personal Familiarity on the Speed of Face Information Processing
  </h1>

  <ul class="authors">
      <li>


        <span rel="dc:creator" class="author">
          <span class="person" property="foaf:name" typeof="foaf:Person">
            Benjamin Balas
              <span class="corresponding">mail</span>, 
          </span>
        </span>

          <div class="author_meta">
            <div class="author_inner">


              
              <p><span class="email">*</span>To whom correspondence should be addressed. E-mail: <a href="mailto:bjbalas@gmail.com">bjbalas@gmail.com</a></p>

                <p>Affiliation:
                  Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology, Cambridge, Massachusetts, United States of America
                </p>


              <span class="close">X</span>

            </div>
          </div>
      </li>
      <li>


        <span rel="dc:creator" class="author">
          <span class="person" property="foaf:name" typeof="foaf:Person">
            David Cox, 
          </span>
        </span>

          <div class="author_meta">
            <div class="author_inner">


              
              

                <p>Affiliation:
                  Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology, Cambridge, Massachusetts, United States of America
                </p>


              <span class="close">X</span>

            </div>
          </div>
      </li>
      <li>


        <span rel="dc:creator" class="author">
          <span class="person" property="foaf:name" typeof="foaf:Person">
            Erin Conwell
          </span>
        </span>

          <div class="author_meta">
            <div class="author_inner">


              
              

                <p>Affiliation:
                  Department of Cognitive and Linguistic Sciences, Brown University, Providence, Rhode Island, United States of America
                </p>


              <span class="close">X</span>

            </div>
          </div>
      </li>
  </ul>
  <ul class="date-doi-line">
    <li>Published: November 21, 2007</li>
    <li>DOI: 10.1371/journal.pone.0001223</li>
  </ul>


</div><!--end header-->
<div class="main cf" id="pjax-container">
  

<div class="nav items-5" id="nav-article">
  <ul>
  <li>
        <span class="active" name="article">Article</span>
  </li>
  <li>
      <a href="/article/authors/info%3Adoi%2F10.1371%2Fjournal.pone.0001223" name="authors">About the Authors</a>
  </li>
  <li>
      <a href="/article/metrics/info%3Adoi%2F10.1371%2Fjournal.pone.0001223" name="metrics">Metrics</a>
  </li>
  <li>
      <a href="/article/comments/info%3Adoi%2F10.1371%2Fjournal.pone.0001223" name="comments">Comments</a>
  </li>
  <li>
      <a href="/article/related/info%3Adoi%2F10.1371%2Fjournal.pone.0001223" name="related">Related Content</a>
  </li>
  </ul>
</div>

<script type="text/javascript">
  var selected_tab = "article";
</script>
  <div id="figure-thmbs" class="carousel cf">
    <div class="wrapper">
      <div class="slider">
              <div class="item">
                <a href="#pone-0001223-g001" data-doi="info:doi/10.1371/journal.pone.0001223" data-uri="info:doi/10.1371/journal.pone.0001223.g001" title="Figure 1">
                  <span class="thmb-wrap">
                    <img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001223.g001&representation=PNG_I" alt="">
                  </span>
                </a>
              </div>
              <div class="item">
                <a href="#pone-0001223-g002" data-doi="info:doi/10.1371/journal.pone.0001223" data-uri="info:doi/10.1371/journal.pone.0001223.g002" title="Figure 2">
                  <span class="thmb-wrap">
                    <img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001223.g002&representation=PNG_I" alt="">
                  </span>
                </a>
              </div>
              <div class="item">
                <a href="#pone-0001223-g003" data-doi="info:doi/10.1371/journal.pone.0001223" data-uri="info:doi/10.1371/journal.pone.0001223.g003" title="Figure 3">
                  <span class="thmb-wrap">
                    <img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001223.g003&representation=PNG_I" alt="">
                  </span>
                </a>
              </div>
      </div>
    </div>
  </div>

  <div class="nav-col">
    <div class="nav" id="nav-article-page">
      <ul>
        <li class="nav-col-comments"><a href="/article/comments/info%3Adoi%2F10.1371%2Fjournal.pone.0001223">Reader Comments (0)</a></li>
          <li id="nav-figures"><a data-doi="info:doi/10.1371/journal.pone.0001223" >Figures</a></li>
      </ul>
    </div>
  </div>

  <div class="article">







<div class="abstract"><a id="abstract0" name="abstract0" toc="abstract0" title="Abstract"></a><h2>Abstract</h2>
<h3>Background</h3>
<a id="article1.front1.article-meta1.abstract1.sec1.p1" name="article1.front1.article-meta1.abstract1.sec1.p1"></a><p>Previous studies have explored the effects of familiarity on various kinds of visual face judgments, yet the role of familiarity in face processing is not fully understood. Across different face judgments and stimulus sets, the data is equivocal as to whether or not familiarity impacts recognition processes.</p>


<h3>Methodology/Principal Findings</h3>
<a id="article1.front1.article-meta1.abstract1.sec2.p1" name="article1.front1.article-meta1.abstract1.sec2.p1"></a><p>Here, we examine the effect of real-world personal familiarity in three simple delayed-match-to-sample tasks in which subjects were required to match faces on the basis of orientation (upright v. inverted), gender and identity. We find that subjects had a significant speed advantage with familiar faces in all three tasks, with large effects for the gender and identity matching tasks.</p>


<h3>Conclusion/Significance</h3>
<a id="article1.front1.article-meta1.abstract1.sec3.p1" name="article1.front1.article-meta1.abstract1.sec3.p1"></a><p>Our data indicates that real-world experience with a face exerts a powerful influence on face processing in tasks where identity information is irrelevant, even in tasks that could in principle be solved via low-level cues. These results underscore the importance of experience in shaping visual recognition processes.</p>

</div>


<div class="articleinfo"><p><strong>Citation: </strong>Balas B, Cox D, Conwell E (2007) The Effect of Real-World Personal Familiarity on the Speed of Face Information Processing. PLoS ONE 2(11):
          e1223.
            doi:10.1371/journal.pone.0001223</p><p><strong>Academic Editor: </strong>Sheng He, University of Minnesota, United States of America</p><p><strong>Received:</strong> August 23, 2007; <strong>Accepted:</strong> November 3, 2007; <strong>Published:</strong> November 21, 2007</p><p><strong>Copyright:</strong> © 2007 Balas et al. This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p><p><strong>Funding: </strong>This research was supported by NDSEG fellowships to BJB and DC.The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</p><p><strong>Competing interests:</strong> The authors have declared that no competing interests exist.</p></div>





<div id="section1" class="section"><a id="s1" name="s1" toc="s1" title="Introduction"></a><h3>Introduction</h3><a id="article1.body1.sec1.p1" name="article1.body1.sec1.p1"></a><p>The human visual system effortlessly and automatically extracts a wealth of information from face stimuli, including identity, gender, expression, race, age, and a host of other properties. For the most part, the ability to extract such information from a given face does not require extensive exposure to that particular face, and judgments of properties such as gender or race are performed with high accuracy even on completely novel faces. Even so, humans tend to encounter a relatively small number of faces repeatedly, and it is not surprising that these familiar faces may enjoy some processing advantages relative to unfamiliar faces. <a href="#pone.0001223-Ge1">[1]</a>,<a href="#pone.0001223-ODonnell1">[2]</a>,<a href="#pone.0001223-Bruce1">[3]</a>,<a href="#pone.0001223-Hancock1">[4]</a>. However, it is not clear <em>a priori</em> that all face judgments should necessarily benefit from familiarity, nor is there any reason to believe that various kinds of judgment should benefit equally from familiarity. Variation in the advantage conferred by familiarity across tasks could provide important clues to the nature of face representations.</p>
<a id="article1.body1.sec1.p2" name="article1.body1.sec1.p2"></a><p>The conceptual orthogonality of many face judgments (e.g. the expression of a face is independent of its gender) has led to the early idea that various face recognition tasks might be executed by parallel, non-overlapping “modules” <a href="#pone.0001223-Bruce2">[5]</a>. Since face familiarity ostensibly depends on the identity of a face, a strong formulation of the modular model might suggest that face familiarity should not affect other tasks, such as gender judgments, because “identity” and “gender” would be processed by separate, non-interacting modules. Along these lines, there are some results that indicate familiarity does not appear to affect gender recognition <a href="#pone.0001223-Bruce3">[6]</a> or expression classification <a href="#pone.0001223-Young1">[7]</a>.</p>
<a id="article1.body1.sec1.p3" name="article1.body1.sec1.p3"></a><p>More recently, substantial evidence has emerged that familiarity does influence other “orthogonal” face judgments. Using images that were parametrically morphed along a continuum between trained (“familiar”) and untrained (“unfamiliar”) faces, Rossion demonstrated significantly faster response times (RTs) for sex classification of the familiar stimuli compared to the unfamiliar images <a href="#pone.0001223-Rossion1">[8]</a>. Likewise, other researchers have pointed out cases where it appears that there are interactions between the familiarity of a face and the processing of race <a href="#pone.0001223-Bruyer1">[9]</a>, expression <a href="#pone.0001223-Gallegos1">[10]</a>,<a href="#pone.0001223-Kaufmann1">[11]</a>, and even speech <a href="#pone.0001223-Walker1">[12]</a>. Taken together, these studies suggest that experience with faces might exert a strong influence on tasks beyond those that are explicitly related to identity.</p>
<a id="article1.body1.sec1.p4" name="article1.body1.sec1.p4"></a><p>In the present study, we sought to further extend what is known about facial familiarity in three simple delayed-match-to-sample tasks in which subjects were required to match faces on the basis of orientation (upright or inverted), gender, or identity. We assess the extent to which familiarity with a face lessens the response time for accurate classification across these three judgments. There are several reasons why we believe this experiment fills important gaps in our understanding of familiar face processing. First, the use of a matching task minimizes the memory and training requirements necessary to carry out these three recognition tasks. Furthermore, regardless of whether the subject was matching a face according to gender, identity, or orientation, the format of the task – a binary left/right choice – was held constant across tasks. This is preferable to comparing behavior across tasks of varying formats (e.g. a binary choice such as male/female in one task and a multiple-category choice such as expression or identity in another). In addition, our use of <em>personally</em> familiar faces obviates the need for training on novel images (which may not lead to complete “familiarity”) or the use of celebrity faces (which may be more distinctive than typical faces). There is also reason to believe that personal acquaintances should give rise to the strongest familiarity effects <a href="#pone.0001223-Clutterbuck1">[13]</a>. Finally, by asking subjects to perform relatively easy matching tasks, we avoid the possibility of a speed-accuracy trade-off by looking for variations in RT while all subjects are performing highly accurately.</p>
</div>

<div id="section2" class="section"><a id="s2" name="s2" toc="s2" title="Materials and Methods"></a><h3>Materials and Methods</h3>
<h4>Stimuli</h4>
<a id="article1.body1.sec2.sec1.p1" name="article1.body1.sec2.sec1.p1"></a><p>We used a database of faces depicting residents and affiliates of a large (roughly 150-person) undergraduate dorm at MIT. The full image set contains 190 unique individuals, half men and half women. Each individual is pictured in left and right profile, left and right ¾ view, and in two different frontal images. The pictures were initially full-color and 640×480 pixels in size.</p>
<a id="article1.body1.sec2.sec1.p2" name="article1.body1.sec2.sec1.p2"></a><p>For presentation, the images were resized to 128×96 pixels, and reduced to grayscale so that broad color cues could not facilitate recognition of targets. To make the matching tasks less trivial, target faces were also Gaussian-blurred in Adobe Photoshop to approximately 6 cycles across the face. Blurring was intended to discourage subjects from performing matching based on small-scale details like moles or blemishes on the face.</p>
<a id="article1.body1.sec2.sec1.p3" name="article1.body1.sec2.sec1.p3"></a><p>Cue images were generated for the “Gender” and “Orientation” tasks by creating facial morphs of the images in our database using MorphMan. The orientation cue image was the result of morphing together all faces in the database. For the “Gender” task, male and female cue images were created by morphing together all the men and women in the database respectively.</p>


<h4>Subjects</h4>
<a id="article1.body1.sec2.sec2.p1" name="article1.body1.sec2.sec2.p1"></a><p>Twenty-four subjects (four men and twenty women, aged 18–25, from the MIT undergraduate community) participated in this study. Subjects were shown a collection of 190 faces and were asked to select 18 for use in the experiment. For twelve of the subjects, half of the stimuli were chosen from the subset of individuals who were highly familiar (meaning that the subject encountered these individuals multiple times per day and had known them for at least a full semester), and half of the stimuli were chosen from the subset of individuals who were not familiar (meaning that the subject could not recall having seen these individuals before). The remaining twelve subjects were gender-matched controls who had no acquaintances among the individuals in the database. Each control subject was assigned stimuli that matched the set of images selected by a subject in the experimental group. All subjects were compensated for their participation in this study.</p>


<h4>Procedure</h4>
<a id="article1.body1.sec2.sec3.p1" name="article1.body1.sec2.sec3.p1"></a><p>Subjects were seated approximately 0.5 m from a computer monitor with no restrictions on head position. Before beginning, subjects in the experimental group were shown the entire set of individuals in the database and asked to select 9 individuals familiar to them, 5 of which were to be of their gender. They were then asked to select an additional 9 individuals (5 gender matched) who they had never seen before, or seen only infrequently (meaning once or twice). Each gender-matched control was shown the faces selected by their experimental group counterpart and asked if they recognized anyone. Volunteers for the control group who indicated that they did recognize individuals in the array were asked to participate in a different experiment not related to the present study.</p>
<a id="article1.body1.sec2.sec3.p2" name="article1.body1.sec2.sec3.p2"></a><p>Each subject participated in the “Orientation”, “Gender,” and “Identity” tasks, with task order balanced across subjects. In each task, a trial began with the presentation of a cue image for 500ms in the center of the screen. After a 500ms pause, the subject was then presented with two images (left and right), one that matched the cue image with regard to the current task and another that did not (<a href="#pone-0001223-g001">Figure 1</a>). Subjects were asked to indicate which stimulus matched the cue via button presses as quickly and accurately as possible. Target images remained on screen until the subject made a response. Location of the target was randomized across trials.</p>
<div class="figure" id="pone-0001223-g001"><div class="img"><a name="pone-0001223-g001" title="Click for larger image " href="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001223.g001&amp;representation=PNG_M" data-doi="info:doi/10.1371/journal.pone.0001223" data-uri="info:doi/10.1371/journal.pone.0001223.g001"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001223.g001&amp;representation=PNG_I" alt="thumbnail" class="thumbnail"></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001223.g001/powerpoint">
                    PPT
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001223.g001/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001223.g001/largerimage">
                    PNG
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001223.g001/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pone.0001223.g001.PNG_L"></span>)
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001223.g001/originalimage">
                    TIFF
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001223.g001/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pone.0001223.g001.TIF"></span>)
                </a></li></ul></div><p><strong>Figure 1.  <span>Delayed-Match-to-Sample task.</span></strong></p><a id="article1.body1.sec2.sec3.fig1.caption1.p1" name="article1.body1.sec2.sec3.fig1.caption1.p1"></a><p>An illustration of the cued 2AFC task used in all three tasks. An “Identity” trial is depicted here, with the correct answer being the right-most image.</p>
<span>doi:10.1371/journal.pone.0001223.g001</span></div><a id="article1.body1.sec2.sec3.p3" name="article1.body1.sec2.sec3.p3"></a><p>In the “Orientation” task, the cue stimulus was always the grand average morph described previously, presented upright, unblurred and in full-color. Test images were blurred, grayscale frontal images of one individual, one presented upside-down and the other presented upright. Each individual was used 4 times in this experiment, for a grand total of 36 “familiar” trials and 36 “unfamiliar” trials per subject.</p>
<a id="article1.body1.sec2.sec3.p4" name="article1.body1.sec2.sec3.p4"></a><p>In the “Gender” task, the cue image was either the average female or average male morph described previously. The cue was presented upright, unblurred and in full-color followed by blurred, grayscale test images. Test images always displayed one male and female, both drawn from the “Familiar” pool or the “Unfamiliar” pool for the subject in question. Each possible pair of differently gendered faces of the same familiarity was used twice, once with the male image as a cue, once with the female image as a cue, for a grand total of 40 trials per condition. To limit subjects' ability to utilize “pictorial information” <a href="#pone.0001223-Bruce4">[14]</a> to perform the task, the particular view used for each pair of test images was rotated through the two unique frontal views and the two ¾ views available for each person.</p>
<a id="article1.body1.sec2.sec3.p5" name="article1.body1.sec2.sec3.p5"></a><p>Finally, in the “Identity” task, subjects were cued with unblurred, upright, full-color profile images of the individuals in their stimulus set. Test images were blurred, grayscale images and also matched at test for familiarity as described above. Each individual was used as a cue 4 times, for a grand total of 36 trials per condition. As in the “Gender” task, the view selected for the test images was rotated through the frontal and ¾ views for each individual.</p>
<a id="article1.body1.sec2.sec3.p6" name="article1.body1.sec2.sec3.p6"></a><p>All stimulus presentation parameters and response collection were carried out with the use of the Matlab Psychophysics Toolbox <a href="#pone.0001223-Brainard1">[15]</a>,<a href="#pone.0001223-Pelli1">[16]</a>.</p>

</div>

<div id="section3" class="section"><a id="s3" name="s3" toc="s3" title="Results"></a><h3>Results</h3><a id="article1.body1.sec3.p1" name="article1.body1.sec3.p1"></a><p>If the relevant cognitive processes are truly independent of familiarity, we expect that responses to “Familiar” faces should be no faster than those to “Unfamiliar” faces. However, if facial familiarity does affect any of the recognition processes recruited to complete the three tasks described here, we should see evidence of reduced response time for correct judgments of orientation, gender, or identity matching in the experimental group. Given that the tasks we present are not difficult, we do not expect to see any variation in accuracy across subjects or tasks. To control for the fact that some faces may be easier than others to classify according to gender (or orientation and identity), we shall also directly compare the speed advantage for “Familiar” v. “Unfamiliar” faces in our experimental group to that derived from the control group. In doing so, we are able to rule out any effects of potentially confusing images that are only accurately classified if one has personal knowledge of the individual depicted.</p>

<h4>Accuracy</h4>
<a id="article1.body1.sec3.sec1.p1" name="article1.body1.sec3.sec1.p1"></a><p>Average performance for all subjects across all three tasks exceeded 95% correct. A two-way ANOVA with subject group and task as factors yielded no significant main effects or interactions (F&lt;1 in all cases). As we expected, all subjects found the three implementations of this matching task very easy.</p>


<h4>Response Time</h4>
<a id="article1.body1.sec3.sec2.p1" name="article1.body1.sec3.sec2.p1"></a><p>Subjects in the Experimental group were significantly faster at matching familiar faces than unfamiliar faces in all three tasks (”Identity” p&lt;0.001, “Gender” p&lt;0.005, “Orientation” p&lt;0.005; planned, one-tailed t-tests; see <a href="#pone-0001223-g002">Figure 2a</a>). This advantage was substantial for the “Identity” and “Gender” tasks (100 ms and 88 ms, respectively), and smaller (but still significant) for the “Orientation” task (17 ms) (<a href="#pone-0001223-g002">Figure 2b</a>). Interestingly, for familiar faces, the “Identity” matching task could be performed as fast as the “Orientation” task, in spite of the fact that the “Orientation” task involved very large image-level differences (see <a href="#pone-0001223-g003">Figure 3</a>) and could, in principle, be solved using purely low-level image cues.</p>
<div class="figure" id="pone-0001223-g002"><div class="img"><a name="pone-0001223-g002" title="Click for larger image " href="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001223.g002&amp;representation=PNG_M" data-doi="info:doi/10.1371/journal.pone.0001223" data-uri="info:doi/10.1371/journal.pone.0001223.g002"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001223.g002&amp;representation=PNG_I" alt="thumbnail" class="thumbnail"></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001223.g002/powerpoint">
                    PPT
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001223.g002/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001223.g002/largerimage">
                    PNG
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001223.g002/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pone.0001223.g002.PNG_L"></span>)
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001223.g002/originalimage">
                    TIFF
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001223.g002/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pone.0001223.g002.TIF"></span>)
                </a></li></ul></div><p><strong>Figure 2.  <span>Familiarity effects on Response Time across tasks.</span></strong></p><a id="article1.body1.sec3.sec2.fig1.caption1.p1" name="article1.body1.sec3.sec2.fig1.caption1.p1"></a><p>(a) Average RT for matching across task for experimental group subjects. There is a clear advantage for familiar face matching according to gender or identity, as well as a small but significant advantage for orientation matching. (b) The mean RT advantage by task for both the Experimental and Control groups. The speed advantage conferred by familiarity for Gender and Identity matching is significantly larger in the Experimental group. The Orientation speed advantage for the Control group is not significantly greater than zero, but is also not significantly smaller than the speed advantage seen in the Experimental group. In both panels, error bars represent +/− 1 S.E.M.</p>
<span>doi:10.1371/journal.pone.0001223.g002</span></div><div class="figure" id="pone-0001223-g003"><div class="img"><a name="pone-0001223-g003" title="Click for larger image " href="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001223.g003&amp;representation=PNG_M" data-doi="info:doi/10.1371/journal.pone.0001223" data-uri="info:doi/10.1371/journal.pone.0001223.g003"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001223.g003&amp;representation=PNG_I" alt="thumbnail" class="thumbnail"></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001223.g003/powerpoint">
                    PPT
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001223.g003/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001223.g003/largerimage">
                    PNG
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001223.g003/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pone.0001223.g003.PNG_L"></span>)
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001223.g003/originalimage">
                    TIFF
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001223.g003/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pone.0001223.g003.TIF"></span>)
                </a></li></ul></div><p><strong>Figure 3.  <span>Examples of upright and inverted stimuli.</span></strong></p><a id="article1.body1.sec3.sec2.fig2.caption1.p1" name="article1.body1.sec3.sec2.fig2.caption1.p1"></a><p>Examples of upright and inverted stimuli presented to subjects in the Orientation matching task. Despite the lack of a significant two-sample difference between performance in the experimental and control groups, the profound low-level differences in these two images make it unlikely that the set of familiar faces selected by the Experimental group introduces a confounding factor in this task.</p>
<span>doi:10.1371/journal.pone.0001223.g003</span></div><a id="article1.body1.sec3.sec2.p2" name="article1.body1.sec3.sec2.p2"></a><p>We continue by examining the RT data from our Control group, matched for age and gender to the Experimental group, for whom none of the faces were familiar. Each subject in this group was shown the same images shown to one subject in the original group, and their data was analyzed with the sham labels “Familiar” and “Unfamiliar,” taken from the original subjects' assessment of familiarity. These subjects showed no advantage in any of the three tasks (”Identity” p = 0.58, “Gender” p = 0.38, “Orientation” p = 0.14, one-tailed t-tests). The mean RT advantage across tasks in the Control group is displayed in <a href="#pone-0001223-g002">Figure 2b</a>.</p>
<a id="article1.body1.sec3.sec2.p3" name="article1.body1.sec3.sec2.p3"></a><p>Comparison of the RT advantages in the Experimental group versus the Control group yielded a robustly significant difference (of approximately the same magnitude) for the “Gender” and “Identity” tasks. The same comparison yields a non-significant difference in the “Orientation” task. It is unclear whether the lack of significance in this last comparison is due to inadequate power (arising from a small sample size, and the use of a two-sample test as compared to a one-sample test, in the original analysis), masking of the effect due to low inter-trial variability in this task compared to the others, or a genuine lack of any difference between conditions. The substantial low-level differences between upright and inverted stimuli in our task make it difficult to imagine that face familiarity was somehow confounded with ease of orientation matching (<a href="#pone-0001223-g003">Figure 3</a>), but we cannot completely rule out this possibility. In any event, we would stress caution in over-interpreting the observed effect of familiarity in the “Orientation” task data from our Experimental group, as it is small in magnitude.</p>

</div>

<div id="section4" class="section"><a id="s4" name="s4" toc="s4" title="Discussion"></a><h3>Discussion</h3><a id="article1.body1.sec4.p1" name="article1.body1.sec4.p1"></a><p>We have found that real-world familiarity with a given face confers an advantage in a range of tasks, including tasks that could, in principle, be solved without processing facial identity at all. This result at least rules out the most simplistically modular models of face recognition and suggests that real-world experience with a face can exert influence over a wide range of face processing beyond the processing of facial identity.</p>
<a id="article1.body1.sec4.p2" name="article1.body1.sec4.p2"></a><p>It is not clear whether the face processing advantages seen here across tasks arise from the same basic mechanism, or from separate ones. One possibility is that early processing stages held in common between these tasks (and indeed, perhaps for all face processing) are rendered more efficient through enhanced experience, and thus all tasks are equivalently speeded. However, it is also possible that multiple, separate processes are made more efficient for faces that are familiar; the current experiments cannot distinguish between these possibilities.</p>
<a id="article1.body1.sec4.p3" name="article1.body1.sec4.p3"></a><p>A variety of mechanisms could result in speeded processing of familiar faces. One possibility is that familiarity induces a change in processing strategy. Young and colleagues <a href="#pone.0001223-Young2">[17]</a>, for instance, have previously suggested that subjects shift from using primarily external face features for recognition to relying more heavily on internal features. Such shifts in attention for often-viewed faces might better focus on information that is relevant for the tasks tested here. Techniques that can shed light on which features are useful for a given task, such as the “bubbles” paradigm <a href="#pone.0001223-Gosselin1">[18]</a>,<a href="#pone.0001223-Schyns1">[19]</a>, could be particularly useful in testing this possibility. An additional possibility that is particularly relevant for our identity and gender tasks is that familiarity with a face leads to a more robust view-invariant representation of individual appearance. The influence of face familiarity on view-invariance has been discussed in some previous studies <a href="#pone.0001223-Hill1">[20]</a>,<a href="#pone.0001223-Troje1">[21]</a>,<a href="#pone.0001223-Troje2">[22]</a> and our results are consistent with existing data.</p>
<a id="article1.body1.sec4.p4" name="article1.body1.sec4.p4"></a><p>Another possibility is that the speed-up observed in gender processing results from the use of an alternate path to retrieve gender information. Specifically, one could imagine that identity is recognized first, and then gender is “looked up” based on stored information about this individual. Indeed, much of the speed-up enjoyed by familiar faces might result from obligatory recognition processes. If identification is automatic, familiar faces would certainly enjoy an advantage over unfamiliar faces since rapid individuation could free up resources useful for other tasks (potentially including low-level judgments like face orientation). This account requires face identification to be completed quickly relative to other processes, which is consistent with our RT data. We also note that intermixing “familiar” and “unfamiliar” faces as we have done here may have encouraged subjects to attempt to identify each presented face since there was no way to predict when a familiar individual would appear.</p>
<a id="article1.body1.sec4.p5" name="article1.body1.sec4.p5"></a><p>Finally, one might speculate that increased exposure induces the visual system to allocate relatively more representational resources to a familiar face. This idea is consistent with prototype-based accounts of face recognition <a href="#pone.0001223-Valentine1">[23]</a>,<a href="#pone.0001223-Leopold1">[24]</a>, and evidence for processing advantages for faces of one's own ethnicity (which tend to be seen more often) <a href="#pone.0001223-Sporer1">[25]</a>. Even subtle shifts in representation “weight” could have a potentially large impact on face processing, as the visual system becomes better tuned to the features or configurations of features found in more frequently observed faces (e.g. more or more sharply tuned neurons tuned to features present in familiar faces). In such a scenario, more information might be available across the board for familiar faces, and this could lead to faster threshold-crossing in decisions about a variety of properties of a familiar face.</p>
<a id="article1.body1.sec4.p6" name="article1.body1.sec4.p6"></a><p>A variety of studies <a href="#pone.0001223-Gauthier1">[26]</a>,<a href="#pone.0001223-Wallis1">[27]</a>,<a href="#pone.0001223-Cox1">[28]</a> have suggested that visual experience can powerfully influence visual representations and can serve as a tool to provide hints about how visual processing is organized. In that vein, the present study offers evidence that natural, real-world familiarity with a particular facial identity can influence a variety of different face tasks. Given the central importance of face processing to humans and other social primates, it is perhaps not surprising that the visual system adapts to process common facial inputs more efficiently. Further investigation of the mechanisms of such adaptation has the potential to teach us much about face processing and vision in general.</p>
</div>





<div><a id="ack" name="ack" toc="ack" title="Acknowledgments"></a><h3>Acknowledgments</h3>
<a id="article1.back1.ack1.p1" name="article1.back1.ack1.p1"></a><p>The authors would like to thank Pawan Sinha for his support of this work. Also, we thank Richard Russell for the use of his face database, as well as the residents of MIT's Senior Haus for posing for the photos.</p>
</div><div class="contributions"><a id="authcontrib" name="authcontrib" toc="authcontrib" title="Author Contributions"></a><h3>Author Contributions</h3><p>Conceived and designed the experiments: BB DC EC. Performed the experiments: BB EC. Analyzed the data: BB DC EC. Wrote the paper: BB DC EC.</p></div><div><a id="references" name="references" toc="references" title="References"></a><h3>References</h3><ol class="references"><li><span class="label">1.
              </span><a name="pone.0001223-Ge1" id="pone.0001223-Ge1"></a>Ge L, Luo J, Nishimuira M, Lee K (2003) The lasting impression of chairman Mao: hyperfidelity of familiar-face memory. Perception  32(5): 601–614.  <ul class="find" data-citedArticleID="1021944" data-doi="10.1068/p5022"><li><a href="http://dx.doi.org/10.1068/p5022" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=The+lasting+impression+of+chairman+Mao%3A+hyperfidelity+of+familiar-face+memory." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22The+lasting+impression+of+chairman+Mao%3A+hyperfidelity+of+familiar-face+memory.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">2.
              </span><a name="pone.0001223-ODonnell1" id="pone.0001223-ODonnell1"></a>O'Donnell C, Bruce V (2001) Familiarisation with faces selectively enhances sensitivity to changes made to the eyes. Perception  30: 755–764.  <ul class="find" data-citedArticleID="1021956" data-doi="10.1068/p3027"><li><a href="http://dx.doi.org/10.1068/p3027" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Familiarisation+with+faces+selectively+enhances+sensitivity+to+changes+made+to+the+eyes." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Familiarisation+with+faces+selectively+enhances+sensitivity+to+changes+made+to+the+eyes.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">3.
              </span><a name="pone.0001223-Bruce1" id="pone.0001223-Bruce1"></a>Bruce V, Henderson Z, Newman C, Burton AM (2001) Matching identities of familiar and unfamiliar faces caught on CCTV images. Journal of Experimental Psychology (Applied)  7(3): 207–218.  <ul class="find" data-citedArticleID="1021926" data-doi="10.1037/1076-898x.7.3.207"><li><a href="http://dx.doi.org/10.1037/1076-898x.7.3.207" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Matching+identities+of+familiar+and+unfamiliar+faces+caught+on+CCTV+images." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Matching+identities+of+familiar+and+unfamiliar+faces+caught+on+CCTV+images.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">4.
              </span><a name="pone.0001223-Hancock1" id="pone.0001223-Hancock1"></a>Hancock PJB, Bruce V, Burton M (2000) Recognition of unfamiliar faces. Trends in Cognitive Science  4(9): 330–337.  <ul class="find" data-citedArticleID="1021948" data-doi="10.1016/s1364-6613(00)01519-9"><li><a href="http://dx.doi.org/10.1016/s1364-6613(00)01519-9" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Recognition+of+unfamiliar+faces." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Recognition+of+unfamiliar+faces.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">5.
              </span><a name="pone.0001223-Bruce2" id="pone.0001223-Bruce2"></a>Bruce V, Young A (1986) Understanding face recognition. British Journal of Psychology  77: 305–327.  <ul class="find" data-citedArticleID="1021928" data-doi="10.1111/j.2044-8295.1986.tb02199.x"><li><a href="http://dx.doi.org/10.1111/j.2044-8295.1986.tb02199.x" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Understanding+face+recognition." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Understanding+face+recognition.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">6.
              </span><a name="pone.0001223-Bruce3" id="pone.0001223-Bruce3"></a>Bruce V (1986) Influences of familiarity on the processing of faces. Perception  15: 387–397.  <ul class="find" data-citedArticleID="1021930" data-doi="10.1068/p150387"><li><a href="http://dx.doi.org/10.1068/p150387" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Influences+of+familiarity+on+the+processing+of+faces." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Influences+of+familiarity+on+the+processing+of+faces.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">7.
              </span><a name="pone.0001223-Young1" id="pone.0001223-Young1"></a>Young AW, McWeeny KH, Hay DC, Ellis AW (1986) Matching familiar and unfamilar faces on identity and expression. Psychological Research  48: 63–68.  <ul class="find" data-citedArticleID="1021976" data-doi="10.1007/bf00309318"><li><a href="http://dx.doi.org/10.1007/bf00309318" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Matching+familiar+and+unfamilar+faces+on+identity+and+expression." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Matching+familiar+and+unfamilar+faces+on+identity+and+expression.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">8.
              </span><a name="pone.0001223-Rossion1" id="pone.0001223-Rossion1"></a>Rossion B (2002) Is sex categorisation from faces really parallel to face recognition? Visual Cognition  9: 1003–1020.  <ul class="find" data-citedArticleID="1021960" data-doi="10.1080/13506280143000485"><li><a href="http://dx.doi.org/10.1080/13506280143000485" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Is+sex+categorisation+from+faces+really+parallel+to+face+recognition%3F" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Is+sex+categorisation+from+faces+really+parallel+to+face+recognition%3F%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">9.
              </span><a name="pone.0001223-Bruyer1" id="pone.0001223-Bruyer1"></a>Bruyer R, Leclere S, Quinet P (2004) Ethnic categorisation of faces is not independent of face identity. Perception  33(2): 169–179.  <ul class="find" data-citedArticleID="1021934" data-doi="10.1068/p5094"><li><a href="http://dx.doi.org/10.1068/p5094" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Ethnic+categorisation+of+faces+is+not+independent+of+face+identity." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Ethnic+categorisation+of+faces+is+not+independent+of+face+identity.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">10.
              </span><a name="pone.0001223-Gallegos1" id="pone.0001223-Gallegos1"></a>Gallegos DR, Tranel D (2005) Positive facial affect facilitates the identification of famous faces. Brain Lang.  93(3): 338–348.  <ul class="find" data-citedArticleID="1021940" data-doi="10.1016/j.bandl.2004.11.001"><li><a href="http://dx.doi.org/10.1016/j.bandl.2004.11.001" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Positive+facial+affect+facilitates+the+identification+of+famous+faces." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Positive+facial+affect+facilitates+the+identification+of+famous+faces.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">11.
              </span><a name="pone.0001223-Kaufmann1" id="pone.0001223-Kaufmann1"></a>Kaufmann JM, Schweinberger SR (2004) Expression influences the recognition of familiar faces. Perception  33(4): 399–408.  <ul class="find" data-citedArticleID="1021952" data-doi="10.1068/p5083"><li><a href="http://dx.doi.org/10.1068/p5083" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Expression+influences+the+recognition+of+familiar+faces." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Expression+influences+the+recognition+of+familiar+faces.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">12.
              </span><a name="pone.0001223-Walker1" id="pone.0001223-Walker1"></a>Walker S, Bruce V, O'Malley C (1995) Facial identity and facial speech processing: familiar faces and voices in the McGurk effect. Perceptual Psychophysics  57(8): 1124–33.  <ul class="find" data-citedArticleID="1021972" data-doi="10.3758/bf03208369"><li><a href="http://dx.doi.org/10.3758/bf03208369" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Facial+identity+and+facial+speech+processing%3A+familiar+faces+and+voices+in+the+McGurk+effect." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Facial+identity+and+facial+speech+processing%3A+familiar+faces+and+voices+in+the+McGurk+effect.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">13.
              </span><a name="pone.0001223-Clutterbuck1" id="pone.0001223-Clutterbuck1"></a>Clutterbuck R, Johnston RA (2002) Exploring levels of face familiarity by using an indirect face-matching measure. Perception  31: 985–994.  <ul class="find" data-citedArticleID="1021936" data-doi="10.1068/p3335"><li><a href="http://dx.doi.org/10.1068/p3335" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Exploring+levels+of+face+familiarity+by+using+an+indirect+face-matching+measure." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Exploring+levels+of+face+familiarity+by+using+an+indirect+face-matching+measure.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">14.
              </span><a name="pone.0001223-Bruce4" id="pone.0001223-Bruce4"></a>Bruce V (1983) Recognising faces. Philosophical Transactions of the Royal Society of London, Series B: Biological Sciences  302: 423–436.  <ul class="find" data-citedArticleID="1021932" data-doi="10.1098/rstb.1983.0065"><li><a href="http://dx.doi.org/10.1098/rstb.1983.0065" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Recognising+faces." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Recognising+faces.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">15.
              </span><a name="pone.0001223-Brainard1" id="pone.0001223-Brainard1"></a>Brainard DH (1997) The psychophysics toolbox. Spatial Vision  10: 433–436.  <ul class="find" data-citedArticleID="1021924" data-doi="10.1163/156856897x00357"><li><a href="http://dx.doi.org/10.1163/156856897x00357" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=The+psychophysics+toolbox." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22The+psychophysics+toolbox.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">16.
              </span><a name="pone.0001223-Pelli1" id="pone.0001223-Pelli1"></a>Pelli DG (1997) The VideoToolbox software for visual psychophysics: transforming numbers into movies. Spatial Vision  10: 437–442.  <ul class="find" data-citedArticleID="1021958" data-doi="10.1163/156856897x00366"><li><a href="http://dx.doi.org/10.1163/156856897x00366" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=The+VideoToolbox+software+for+visual+psychophysics%3A+transforming+numbers+into+movies." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22The+VideoToolbox+software+for+visual+psychophysics%3A+transforming+numbers+into+movies.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">17.
              </span><a name="pone.0001223-Young2" id="pone.0001223-Young2"></a>Young AW, Hay DC, McWeeny KH, Flude BM, Ellis AW (1985) Matching familiar and unfamiliar faces on internal and external features. Perception  14: 737–746.  <ul class="find" data-citedArticleID="1021978" data-doi="10.1068/p140737"><li><a href="http://dx.doi.org/10.1068/p140737" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Matching+familiar+and+unfamiliar+faces+on+internal+and+external+features." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Matching+familiar+and+unfamiliar+faces+on+internal+and+external+features.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">18.
              </span><a name="pone.0001223-Gosselin1" id="pone.0001223-Gosselin1"></a>Gosselin F, Schyns PG (2001) Bubbles: a technique to reveal the use of information in recognition tasks. Vision Research  41(17): 2261–2271.  <ul class="find" data-citedArticleID="1021946" data-doi="10.1016/s0042-6989(01)00097-9"><li><a href="http://dx.doi.org/10.1016/s0042-6989(01)00097-9" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Bubbles%3A+a+technique+to+reveal+the+use+of+information+in+recognition+tasks." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Bubbles%3A+a+technique+to+reveal+the+use+of+information+in+recognition+tasks.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">19.
              </span><a name="pone.0001223-Schyns1" id="pone.0001223-Schyns1"></a>Schyns PG, Bonnar L, Gosselin F (2002) Show me the features! Understanding recognition from the use of visual information. Psychological Science  13: 402–409.  <ul class="find" data-citedArticleID="1021962" data-doi="10.1111/1467-9280.00472"><li><a href="http://dx.doi.org/10.1111/1467-9280.00472" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Show+me+the+features%21+Understanding+recognition+from+the+use+of+visual+information." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Show+me+the+features%21+Understanding+recognition+from+the+use+of+visual+information.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">20.
              </span><a name="pone.0001223-Hill1" id="pone.0001223-Hill1"></a>Hill H, Bruce V (1996) Effects of lighting on the perception of facial surfaces. Journal of Experimental Psychology: Human Perception and Performance  22(4): 986–1004.  <ul class="find" data-citedArticleID="1021950" data-doi="10.1037/0096-1523.22.4.986"><li><a href="http://dx.doi.org/10.1037/0096-1523.22.4.986" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Effects+of+lighting+on+the+perception+of+facial+surfaces." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Effects+of+lighting+on+the+perception+of+facial+surfaces.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">21.
              </span><a name="pone.0001223-Troje1" id="pone.0001223-Troje1"></a>Troje NF, Bulthoff HH (1996) Face recognition under varying poses: the role of texture and shape. Vision Research  36(12): 1761–1771.  <ul class="find" data-citedArticleID="1021966" data-doi="10.1016/0042-6989(95)00230-8"><li><a href="http://dx.doi.org/10.1016/0042-6989(95)00230-8" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Face+recognition+under+varying+poses%3A+the+role+of+texture+and+shape." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Face+recognition+under+varying+poses%3A+the+role+of+texture+and+shape.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">22.
              </span><a name="pone.0001223-Troje2" id="pone.0001223-Troje2"></a>Troje NF, Kersten D (1999) Viewpoint-dependent recognition of familiar faces. Perception  28: 483–487.  <ul class="find" data-citedArticleID="1021968" data-doi="10.1068/p2901"><li><a href="http://dx.doi.org/10.1068/p2901" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Viewpoint-dependent+recognition+of+familiar+faces." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Viewpoint-dependent+recognition+of+familiar+faces.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">23.
              </span><a name="pone.0001223-Valentine1" id="pone.0001223-Valentine1"></a>Valentine T (1991) A unified account of the effects of distinctiveness, inversion, and race in face recognition. Quarterly Journal of Experimental Psychology A  43A: 89–94.  <ul class="find" data-citedArticleID="1021970" data-doi="10.1080/14640749108400966"><li><a href="http://dx.doi.org/10.1080/14640749108400966" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=A+unified+account+of+the+effects+of+distinctiveness%2C+inversion%2C+and+race+in+face+recognition." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22A+unified+account+of+the+effects+of+distinctiveness%2C+inversion%2C+and+race+in+face+recognition.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">24.
              </span><a name="pone.0001223-Leopold1" id="pone.0001223-Leopold1"></a>Leopold DA, O'Toole AJ, Vetter T, Blanz V (2001) Prototype-referenced shape encoding revealed by high-level after-effects. Nature Neuroscience  4: 84–89.  <ul class="find" data-citedArticleID="1021954"><li><a href="http://www.crossref.org/guestquery/?auth2=&amp;atitle2=Prototype-referenced+shape+encoding+revealed+by+high-level+after-effects.&amp;auth=&amp;atitle=Prototype-referenced+shape+encoding+revealed+by+high-level+after-effects." target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Prototype-referenced+shape+encoding+revealed+by+high-level+after-effects." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Prototype-referenced+shape+encoding+revealed+by+high-level+after-effects.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">25.
              </span><a name="pone.0001223-Sporer1" id="pone.0001223-Sporer1"></a>Sporer SL (2001) Recognizing Faces of Other Ethnic Groups: An Integration of Theories. Psychology, Public Policy, and Law  7(1): 36–97.  <ul class="find" data-citedArticleID="1021964" data-doi="10.1037/1076-8971.7.1.36"><li><a href="http://dx.doi.org/10.1037/1076-8971.7.1.36" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Recognizing+Faces+of+Other+Ethnic+Groups%3A+An+Integration+of+Theories." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Recognizing+Faces+of+Other+Ethnic+Groups%3A+An+Integration+of+Theories.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">26.
              </span><a name="pone.0001223-Gauthier1" id="pone.0001223-Gauthier1"></a>Gauthier I, Tarr MJ (1997) Becoming a “Greeble” expert: Exploring the face recognition mechanism. Vision Research  37(12): 1673–1682.  <ul class="find" data-citedArticleID="1021942" data-doi="10.1016/s0042-6989(96)00286-6"><li><a href="http://dx.doi.org/10.1016/s0042-6989(96)00286-6" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Becoming+a+%E2%80%9CGreeble%E2%80%9D+expert%3A+Exploring+the+face+recognition+mechanism." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Becoming+a+%E2%80%9CGreeble%E2%80%9D+expert%3A+Exploring+the+face+recognition+mechanism.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">27.
              </span><a name="pone.0001223-Wallis1" id="pone.0001223-Wallis1"></a>Wallis G, Bulthoff HH (2001) Effects of temporal association on recognition memory. Proceedings of the National Academy of Sciences  98(8): 4800–4804.  <ul class="find" data-citedArticleID="1021974" data-doi="10.1073/pnas.071028598"><li><a href="http://dx.doi.org/10.1073/pnas.071028598" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Effects+of+temporal+association+on+recognition+memory." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Effects+of+temporal+association+on+recognition+memory.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">28.
              </span><a name="pone.0001223-Cox1" id="pone.0001223-Cox1"></a>Cox DD, Meier P, Oertelt N, DiCarlo JJ (2005) ‘Breaking’ position-invariant object recognition. Nature Neuroscience  8(9): 1145–1147.  <ul class="find" data-citedArticleID="1021938" data-doi="10.1038/nn1519"><li><a href="http://dx.doi.org/10.1038/nn1519" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=%E2%80%98Breaking%E2%80%99+position-invariant+object+recognition." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22%E2%80%98Breaking%E2%80%99+position-invariant+object+recognition.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li></ol></div>

  </div>

      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001223.XML" value="48713"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001223.PDF" value="123569"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001223.g001.PNG_L" value="360110"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001223.g001.PNG_M" value="55184"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001223.g001.PNG_S" value="11083"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001223.g001.TIF" value="1284756"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001223.g001.PNG_I" value="46485"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001223.g002.PNG_L" value="676445"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001223.g002.PNG_M" value="47606"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001223.g002.PNG_S" value="8354"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001223.g002.TIF" value="1507724"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001223.g002.PNG_I" value="15950"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001223.g003.PNG_L" value="165224"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001223.g003.PNG_M" value="75955"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001223.g003.PNG_S" value="5347"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001223.g003.TIF" value="134712"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001223.g003.PNG_I" value="26034"/>

</div>
<div class="sidebar">

  <div class="article-actions cf">
      <div class="download">
        <span class="btn"><a href="/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0001223&amp;representation=PDF" title="Download" target="_blank">Download PDF</a></span>
      </div>
      <div class="btn-reveal dropdown">
        <div class="dropdown-icon">
          <span class="btn">&nbsp;</span>
        </div>

        <div class="content">
          <ul class="bullet">
            <li><a href="/article/citationList.action?articleURI=info%3Adoi%2F10.1371%2Fjournal.pone.0001223" title="Download citations">Citation</a></li>
            <li><a href="/article/fetchObjectAttachment.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0001223&amp;representation=XML" title="Download article XML">XML</a></li>
          </ul>
        </div>
      </div> <!-- end btn-reveal dropdown-->


    <div class="btn-reveal flt-l">
        <span class="btn">Print</span>
        <div class="content">
            <ul class="bullet">
                <li id="print-article"><a href="#" onclick="if(typeof(_gaq) != 'undefined'){ _gaq.push(['_trackEvent','Article', 'Print', 'Click']); } window.print(); return false;" title="Print Article">Print article</a></li>
                <li>
                  <a href="https://www.odysseypress.com/onlinehost/reprint_order.php?type=A&page=0&journal=7&doi=10.1371/journal.pone.0001223&volume=&issue=&title=The Effect of Real-World Personal Familiarity on the Speed of Face Information Processing&author_name=Benjamin%20Balas%2C%20David%20Cox%2C%20Erin%20Conwell&start_page=1&end_page=5" title="Odyssey Press">EzReprint</a>
                </li>
            </ul>
        </div>
    </div>

    <div class="btn-reveal flt-r">
        <span class="btn">Share</span>
        <div class="content">
            <ul class="social">
                <li><a href="http://www.reddit.com/submit?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0001223" target="_blank" title="Submit to Reddit"><img src="/images/icon.reddit.16.png" width="16" height="16" alt="Reddit">Reddit</a></li>

                <li><a href="https://plus.google.com/share?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0001223" target="_blank" title="Share on Google+"><img src="/images/icon.gplus.16.png" width="16" height="16" alt="Google+">Google+</a></li>

                <li><a href="http://www.stumbleupon.com/submit?url=http%3A%2F%2Fwww.plosone.org%2Farticle%2Finfo%253Adoi%252F10.1371%252Fjournal.pone.0001223" target="_blank" title="Add to StumbleUpon"><img src="/images/icon.stumble.16.png" width="16" height="16" alt="StumbleUpon">StumbleUpon</a></li>

                <li><a href="http://www.facebook.com/share.php?u=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0001223&amp;t=The%20Effect%20of%20Real-World%20Personal%20Familiarity%20on%20the%20Speed%20of%20Face%20Information%20Processing" target="_blank" title="Share on Facebook"><img src="/images/icon.fb.16.png" width="16" height="16" alt="Facebook">Facebook</a></li>

                <li><a href="http://www.linkedin.com/shareArticle?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0001223&title=The%20Effect%20of%20Real-World%20Personal%20Familiarity%20on%20the%20Speed%20of%20Face%20Information%20Processing&summary=Checkout%20this%20article%20I%20found%20at%20PLOS" target="_blank" title="Add to LinkedIn"><img src="/images/icon.linkedin.16.png" width="16" height="16" alt="Mendeley">LinkedIn</a></li>

                <li><a href="http://www.citeulike.org/posturl?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0001223&amp;title=The%20Effect%20of%20Real-World%20Personal%20Familiarity%20on%20the%20Speed%20of%20Face%20Information%20Processing" target="_blank" title="Add to CiteULike"><img src="/images/icon.cul.16.png" width="16" height="16" alt="CiteULike">CiteULike</a></li>

                <li><a href="http://www.mendeley.com/import/?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0001223" target="_blank" title="Add to Mendeley"><img src="/images/icon.mendeley.16.png" width="16" height="16" alt="Mendeley">Mendeley</a></li>

                <li><a href="https://www.pubchase.com/library?add_aid=10.1371%2Fjournal.pone.0001223&amp;source=plos" target="_blank" title="Add to PubChase"><img src="/images/icon.pc.16.png" width="16" height="16" alt="PubChase">PubChase</a></li>


                <script type="text/javascript">
                    // replace tweet with one that's pre-shortened to 140 chars
                    function truncateTweetText() {
                        var twtTitle = 'The Effect of Real-World Personal Familiarity on the Speed of Face Information Processing';
                        var twtUrl = 'http://dx.plos.org/10.1371/journal.pone.0001223';
                        // all URLs posted to twitter get auto-shortened to 20 chars.
                        var maxLength = 140 - (20 + 1);
                        // truncate the title to include space for twtTag and ellipsis (here, 10 = tag length + space + ellipsis)
                        if (twtTitle.length > maxLength) { twtTitle = twtTitle.substr(0, (maxLength - 10)) + '...'; }
                        // set the href to use the shortened tweet
                        $('#twitter-share-link').prop('href', 'http://twitter.com/intent/tweet?text=' + encodeURIComponent('#PLOSONE: ' + twtTitle + ' ' + twtUrl));
                    }
                </script>
                <li><a href="http://twitter.com/intent/tweet?text=#PLOSONE%3A%20The%20Effect%20of%20Real-World%20Personal%20Familiarity%20on%20the%20Speed%20of%20Face%20Information%20Processing http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0001223" onclick="truncateTweetText();" target="_blank" title="Share on Twitter" id="twitter-share-link"><img src="/images/icon.twtr.16.png" width="16" height="16" alt="Twitter">Twitter</a></li>

                <li><a href="/article/email/info%3Adoi%2F10.1371%2Fjournal.pone.0001223" title="Email this article"><img src="/images/icon.email.16.png" width="16" height="16" alt="Email">Email</a></li>
            </ul>
        </div>
    </div><!--end btn-reveal flt-r-->
</div><!-- end article-actions-->

<!-- begin Crossmark -->

<a id="open-crossmark" href="#" style="margin-top: -28px; display:block"><img style="border: 0; display: none;
 padding: 10px 0 18px 0;"  id="crossmark-icon" src="/images/logo-crossmark-bw.png" /></a>
<div id="crossmark-dialog" style="display: none;" title="">
    <!-- the external CrossMark data is loaded inside this iframe -->
    <iframe id="crossmark-dialog-frame" frameborder="0"></iframe>
</div>

<!-- end crossmark -->


<div class="block" id="subject-area-sidebar-block">
    <div class="header">
        <h3>Subject Areas</h3><div title="More information" id="subject-area-sidebar-block-help-icon"><img align="right"
                                                                                                           alt="info" src="/images/button_info.png"/><div id="subject-area-sidebar-block-help"><img align="right"
                                                                                                                                                                                                    src="/images/button_info.png"/><p>
        <b>We want your feedback.</b> Do these subject areas make sense for this article? If not, click the flag
        next to the incorrect subject area and we will review it. Thanks for your help!
    </p></div></div>
    </div>


    <ul id="subject-area-sidebar-list">

















          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Data+processing%22" title="Search for articles in the subject area:'Data processing'"><div class="flagText">Data processing</div></a>
              <div data-categoryid="18073" data-articleid="25278"
                   data-categoryname="Data processing"
                   class="flagImage" title="Flag 'Data processing' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Decision+making%22" title="Search for articles in the subject area:'Decision making'"><div class="flagText">Decision making</div></a>
              <div data-categoryid="18211" data-articleid="25278"
                   data-categoryname="Decision making"
                   class="flagImage" title="Flag 'Decision making' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Face%22" title="Search for articles in the subject area:'Face'"><div class="flagText">Face</div></a>
              <div data-categoryid="48035" data-articleid="25278"
                   data-categoryname="Face"
                   class="flagImage" title="Flag 'Face' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Face+recognition%22" title="Search for articles in the subject area:'Face recognition'"><div class="flagText">Face recognition</div></a>
              <div data-categoryid="46303" data-articleid="25278"
                   data-categoryname="Face recognition"
                   class="flagImage" title="Flag 'Face recognition' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Grayscale%22" title="Search for articles in the subject area:'Grayscale'"><div class="flagText">Grayscale</div></a>
              <div data-categoryid="23975" data-articleid="25278"
                   data-categoryname="Grayscale"
                   class="flagImage" title="Flag 'Grayscale' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Neuronal+tuning%22" title="Search for articles in the subject area:'Neuronal tuning'"><div class="flagText">Neuronal tuning</div></a>
              <div data-categoryid="19781" data-articleid="25278"
                   data-categoryname="Neuronal tuning"
                   class="flagImage" title="Flag 'Neuronal tuning' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Undergraduates%22" title="Search for articles in the subject area:'Undergraduates'"><div class="flagText">Undergraduates</div></a>
              <div data-categoryid="40381" data-articleid="25278"
                   data-categoryname="Undergraduates"
                   class="flagImage" title="Flag 'Undergraduates' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Visual+system%22" title="Search for articles in the subject area:'Visual system'"><div class="flagText">Visual system</div></a>
              <div data-categoryid="34487" data-articleid="25278"
                   data-categoryname="Visual system"
                   class="flagImage" title="Flag 'Visual system' as inappropriate"></div>
          </li>
    </ul>
</div>

<div class="ad">
    <div class="title">Advertisement</div>






  <iframe id='a0852f54' name='a0852f54'
    src='http://ads.plos.org/www/delivery/afr.php?zoneid=381&amp;cb=8860'
    frameborder='0' scrolling='no' width='160' height='600'>
    <a href='http://ads.plos.org/www/delivery/ck.php?n=a0852f54&amp;cb=4454'
      target='_top'><img src='http://ads.plos.org/www/delivery/avw.php?zoneid=381&amp;cb=8367&amp;n=a0852f54'
      border='0' alt=''/>
    </a>
  </iframe>



</div>

<div id="twitter-alm-timeline" class="twitter-alm-timeline"></div>


</div><!-- sidebar -->
    </div>
  </div>
</div>
<script src="http://wl.figshare.com/static/p_widget.js" type="text/javascript"></script><div id="pageftr">
  <div class="ftr-cols cf">
    <div class="col col-1">
      <img src="/images/logo-plos-footer.png" alt="PLOS Logo" class="logo" />
      <p><a href="/static/releaseNotes">Ambra 2.9.16</a> Managed Colocation provided <br />by <a href="http://www.isc.org/">Internet Systems Consortium</a>.<p>
      <div class="nav nav-aux">
        <a href="/static/privacy">Privacy Policy</a> |
        <a href="/static/terms">Terms of Use</a> |
        <a href="http://www.plos.org/advertise/">Advertise</a> |
        <a href="http://www.plos.org/about/media-inquiries/">Media Inquiries</a>
      </div>
    </div>
    <div class="col col-2">
      <p><a href="http://www.plos.org/publications/journals/">Publications</a></p>
      <div class="nav">
        <ul>
          <li><a href="http://www.plosbiology.org">PLOS Biology</a></li>
          <li><a href="http://www.plosmedicine.org">PLOS Medicine</a></li>
          <li><a href="http://www.ploscompbiol.org">PLOS Computational Biology</a></li>
          <li><a href="http://currents.plos.org">PLOS Currents</a></li>
          <li><a href="http://www.plosgenetics.org">PLOS Genetics</a></li>
          <li><a href="http://www.plospathogens.org">PLOS Pathogens</a></li>
          <li><a href="http://www.plosone.org">PLOS ONE</a></li>
          <li><a href="http://www.plosntds.org">PLOS Neglected Tropical Diseases</a></li>
        </ul>
      </div>
    </div>
    <div class="col col-3">
      <div class="nav">
        <p><a href="http://www.plos.org">plos.org</a></p>
        <p><a href="http://blogs.plos.org">Blogs</a></p>
        <p><a href="http://www.ploscollections.org">Collections</a></p>
        <p><a href="/feedback/new">Send us feedback</a></p>

        <p>California (US) corporation #C2354500, based in San Francisco</p>
      </div>
    </div>
  </div>
</div><!-- pageftr -->

</div><!-- end page-wrap, this div is in header.ftl -->
<script type="text/javascript" src="/javascript/jquery-1.8.1-min.js?v=Tm7VCOzZz3lE03ghpkS6SWkHbyI"></script>
<script type="text/javascript" src="/javascript/ga-min.js?v=lNQ4gt8QcPDatjsdOFl_FGpPhLY"></script>
<script type="text/javascript" src="/javascript/jquery.hoverIntent-min.js?v=mRiGNYY9cIXxVb8u0K_MdW7hHnc"></script>
<script type="text/javascript" src="/javascript/jquery.placeholder-min.js?v=21Pn56Ur9h1N4K4VZDa0nqI3Pxo"></script>
<script type="text/javascript" src="/javascript/jquery.jsonp-2.4.0-min.js?v=lqTpzoHfSq3I5Ygo01qq5WankEo"></script>
<script type="text/javascript" src="/javascript/jquery-ui-1.9.2.custom-min.js?v=raSSlfNO0YsV5uUpAKmTB9n5VTc"></script>
<script type="text/javascript" src="/javascript/jquery.tooltip-min.js?v=cw+6Smh+mdryIA25xvqIvHMrnZM"></script>
<script type="text/javascript" src="/javascript/jquery.uniform-min.js?v=kYUAnX6W2W_2fK3RIuQ2m_YFG9U"></script>
<script type="text/javascript" src="/javascript/jquery.pjax-min.js?v=939kLBjL5_YKbx71T1RHjYaD4l8"></script>
<script type="text/javascript" src="/javascript/imagesloaded-min.js?v=XeuAp8Gc3mvQUo+wZCSF8ttPwvw"></script>
<script type="text/javascript" src="/javascript/figviewer-min.js?v=yPUa0sUQ_iHkI+IRv2i9bjyZJFo"></script>
<script type="text/javascript" src="/javascript/global-min.js?v=0Q3PwjeaWtXYDnqIsQvnL_ou0qs"></script>
<script type="text/javascript" src="/javascript/jquery.touchswipe-min.js?v=huaek_e6HqTduvCNAN91dJolTyw"></script>
<script type="text/javascript" src="/javascript/jquery.base64-min.js?v=VwV1zeVqKZj5FCAdlK0q5NRxbBg"></script>
<script type="text/javascript" src="/javascript/alm-min.js?v=Y5gm6B0b4Kx2YHNObNrgEeBgXlY"></script>
<script type="text/javascript" src="/javascript/taxonomy-browser-min.js?v=vBVMuDMYkGJCXIUxLe35GoyiJNw"></script>
<script type="text/javascript" src="/javascript/jquery.filterize-min.js?v=j0ZKVnHyk2nhFy8eIuNJkp7xaM0"></script>
<script type="text/javascript" src="/javascript/plosone-min.js?v=TK4H4arL_XBSwwJq+K1N3kqYfAI"></script>
<script type="text/javascript" src="/javascript/twitter-min.js?v=xKgcxLsQFXy+at1ao1NVke8nFlM"></script>
<script type="text/javascript" src="/javascript/crossmark.1.4-min.js?v=3FO4k0SjwTaGNnKGNSqthar1080"></script>
<script type="text/javascript">
  var _sf_async_config={uid:16579,domain:"plosone.org"};
  (function(){
    function loadChartbeat() {
      window._sf_endpt=(new Date()).getTime();
      var e = document.createElement('script');
      e.setAttribute('language', 'javascript');
      e.setAttribute('type', 'text/javascript');
      e.setAttribute('src',
          (("https:" == document.location.protocol) ? "https://a248.e.akamai.net/chartbeat.download.akamai.com/102508/" : "http://static.chartbeat.com/") +
              "js/chartbeat.js");
      document.body.appendChild(e);
    }
    var oldonload = window.onload;
    window.onload = (typeof window.onload != 'function') ?
        loadChartbeat : function() { oldonload(); loadChartbeat(); };
  })();
</script>
<!-- <script type="application/javascript" src="http://crossmark.crossref.org/javascripts/v1.3/crossmark.min.js"></script> -->

</body>
</html>
