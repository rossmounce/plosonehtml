

 



<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"
      xmlns:foaf="http://xmlns.com/foaf/0.1/"
      xmlns:dc="http://purl.org/dc/terms/"
      xmlns:doi="http://dx.doi.org/"
      xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
      xmlns:xsd="http://www.w3.org/2001/XMLSchema-datatypes#"
      lang="en" xml:lang="en"
      itemscope itemtype="http://schema.org/Article"
      class="no-js">
<head prefix="og: http://ogp.me/ns#">
  <title>PLOS ONE: Using fMRI Brain Activation to Identify Cognitive States Associated with Perception of Tools and Dwellings</title>


<link rel="stylesheet" type="text/css"  href="/css/global-min.css?v=izteQ6tu7kgsJZW_xmrYizvKiHM" />


    <!--[if lte IE 7]>
<link rel="stylesheet" type="text/css"  href="/css/lte_ie7-min.css?v=3bykQUyQmReeuobVyPozcJ9LxRc" />
    <![endif]-->


<link rel="stylesheet" type="text/css"  href="/css/jquery-ui-min.css?v=eXDHTEJM0lIAmDe5k0I0Ad4nxNo" />


<link rel="stylesheet" type="text/css"  href="/css/journal.css?v=T7ZVxJfgk9jNxLAJ2qHz1vZpgYU" />


<link rel="stylesheet" type="text/css" media="print" href="/css/print-min.css?v=T5lb0B3q6EXBsuDluc5V5w+AkRc" />


  <link rel="stylesheet" href="http://f.fontdeck.com/s/css/js/www.plosone.org/24557.css" type="text/css"/>

  <!--chartbeat -->
  <script type="text/javascript">var _sf_startpt = (new Date()).getTime()</script>
  <script>document.documentElement.className += ' js';</script>

  <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <meta http-equiv="X-UA-Compatible" content="IE=EmulateIE7; IE=EmulateIE9"/>
  <meta name="description" content="PLOS ONE: an inclusive, peer-reviewed, open-access resource from the PUBLIC LIBRARY OF SCIENCE. Reports of well-performed scientific studies from all disciplines freely available to the whole world."/>
  <meta name="keywords" content="PLOS, Public Library of Science, Open Access, Open-Access, Science, Medicine, Biology, Research, Peer-review, Inclusive, Interdisciplinary, Ante-disciplinary, Physics, Chemistry, Engineering"/>
  <meta name="almHost" content="http://alm.plos.org/api/v3/articles"/>
  <meta name="searchHost" content="http://api.plos.org/search" />
  <meta name="termsHost" content="http://api.plos.org/terms" />
  <meta name="solrApiKey" content="plos"/>
  <meta name="almAPIKey" content="3pezRBRXdyzYW6ztfwft" />
  <meta name="currentJournal" content="PLoSONE" />
  <meta name="almRequestBatchSize" content="" />

  <meta name="citation_publisher" content="Public Library of Science"/>
  <meta name="citation_doi" content="10.1371/journal.pone.0001394"/>
  <meta name="dc.identifier" content="10.1371/journal.pone.0001394" />

    <meta name="citation_title" content="Using fMRI Brain Activation to Identify Cognitive States Associated with Perception of Tools and Dwellings"/>
    <meta itemprop="name" content="Using fMRI Brain Activation to Identify Cognitive States Associated with Perception of Tools and Dwellings"/>

      <meta name="citation_author" content="Svetlana V. Shinkareva"/>
            <meta name="citation_author_institution" content="Department of Psychology, Carnegie Mellon University, Pittsburgh, Pennsylvania, United States of America"/>
            <meta name="citation_author_institution" content="Machine Learning Department, School of Computer Science, Carnegie Mellon University, Pittsburgh, Pennsylvania, United States of America"/>
      <meta name="citation_author" content="Robert A. Mason"/>
            <meta name="citation_author_institution" content="Department of Psychology, Carnegie Mellon University, Pittsburgh, Pennsylvania, United States of America"/>
      <meta name="citation_author" content="Vicente L. Malave"/>
            <meta name="citation_author_institution" content="Department of Psychology, Carnegie Mellon University, Pittsburgh, Pennsylvania, United States of America"/>
      <meta name="citation_author" content="Wei Wang"/>
            <meta name="citation_author_institution" content="Machine Learning Department, School of Computer Science, Carnegie Mellon University, Pittsburgh, Pennsylvania, United States of America"/>
      <meta name="citation_author" content="Tom M. Mitchell"/>
            <meta name="citation_author_institution" content="Machine Learning Department, School of Computer Science, Carnegie Mellon University, Pittsburgh, Pennsylvania, United States of America"/>
      <meta name="citation_author" content="Marcel Adam Just"/>
            <meta name="citation_author_institution" content="Department of Psychology, Carnegie Mellon University, Pittsburgh, Pennsylvania, United States of America"/>

    <meta name="citation_date" content="2008/1/2"/>

  <meta name="citation_pdf_url" content="http://dx.plos.org/10.1371/journal.pone.0001394.pdf" />

      <meta name="citation_journal_title" content="PLOS ONE" />
    <meta name="citation_firstpage" content="e1394"/>
    <meta name="citation_issue" content="1"/>
    <meta name="citation_volume" content="3"/>
    <meta name="citation_issn" content="1932-6203"/>

    <meta name="citation_journal_abbrev" content="PLoS ONE" />

      <meta name="citation_reference" content="citation_title=Combinatorial codes in ventral temporal lobe for object recognition: Haxby (2001) revisited: is there a “face” area?; citation_author=SJ Hanson; citation_author=T Matsuka; citation_author=JV Haxby; citation_journal_title=NeuroImage; citation_volume=23; citation_number=1; citation_pages=156-166; citation_date=2004; " />
      <meta name="citation_reference" content="citation_title=Functional magnetic resonance imaging (fMRI) “brain reading”: detecting and classifying distributed patterns of fMRI activity in human visual cortex.; citation_author=DD Cox; citation_author=RL Savoy; citation_journal_title=NeuroImage; citation_volume=19; citation_number=2; citation_pages=261-270; citation_date=2003; " />
      <meta name="citation_reference" content="citation_title=Patterns of activity in the categorical representations of objects.; citation_author=TA Carlson; citation_author=P Schrater; citation_author=S He; citation_journal_title=Journal of Cognitive Neuroscience; citation_volume=15; citation_number=3; citation_pages=704-717; citation_date=2003; " />
      <meta name="citation_reference" content="citation_title=Classifying brain states and determining the discriminating activation patterns: support vector machine on functional MRI data.; citation_author=J Mourao-Miranda; citation_author=AL Bokde; citation_author=C Born; citation_author=H Hampel; citation_author=M Stetter; citation_journal_title=NeuroImage; citation_volume=28; citation_number=4; citation_pages=980-995; citation_date=2005; " />
      <meta name="citation_reference" content="citation_title=Support vector machines for temporal classification of block design fMRI data.; citation_author=S LaConte; citation_author=S Strother; citation_author=V Cherkassky; citation_author=J Anderson; citation_author=X Hu; citation_journal_title=NeuroImage; citation_volume=26; citation_number=5; citation_pages=317-329; citation_date=2005; " />
      <meta name="citation_reference" content="citation_title=Category-specific cortical activity precedes retrieval during memory search.; citation_author=SM Polyn; citation_author=VS Natu; citation_author=JD Cohen; citation_author=KA Norman; citation_journal_title=Science; citation_volume=310; citation_number=6; citation_pages=1963-1966; citation_date=2005; " />
      <meta name="citation_reference" content="citation_title=Distributed and overlapping representations of faces and objects in ventral temporal cortex.; citation_author=JV Haxby; citation_author=MI Gobbini; citation_author=ML Furey; citation_author=A Ishai; citation_author=JL Schouten; citation_journal_title=Science; citation_volume=293; citation_number=7; citation_pages=2425-2430; citation_date=2001; " />
      <meta name="citation_reference" content="citation_title=Partially distributed representations of objects and faces in ventral temporal cortex.; citation_author=A O'Toole; citation_author=F Jiang; citation_author=H Abdi; citation_author=JV Haxby; citation_journal_title=Journal of Cognitive Neuroscience; citation_volume=17; citation_number=8; citation_pages=580-590; citation_date=2005; " />
      <meta name="citation_reference" content="citation_title=Basic objects in natural categories.; citation_author=E Rosch; citation_author=CB Mervis; citation_author=WD Gray; citation_author=DM Johnson; citation_author=P Boyes-Braem; citation_journal_title=Cognitive Psychology; citation_volume=8; citation_number=9; citation_pages=382-439; citation_date=1976; " />
      <meta name="citation_reference" content="citation_title=Automated anatomical labeling of activations in SPM using a macroscopic anatomical parcellation of the MNI MRI single-subject brain.; citation_author=N Tzourio-Mazoyer; citation_author=B Landeau; citation_author=D Papathanassiou; citation_author=F Crivello; citation_author=O Etard; citation_journal_title=NeuroImage; citation_volume=15; citation_number=10; citation_pages=273-289; citation_date=2002; " />
      <meta name="citation_reference" content="citation_title=Human cerebral cortex: localization, parcellation and morphometry with magnetic resonance imaging.; citation_author=J Rademacher; citation_author=AM Galaburda; citation_author=DN Kennedy; citation_author=PA Filipek; citation_author=VS Caviness; citation_journal_title=Journal of Cognitive Neuroscience; citation_volume=4; citation_number=11; citation_pages=352-374; citation_date=1992; " />
      <meta name="citation_reference" content="citation_title=Stereotaxic display of brain lesions.; citation_author=C Rorden; citation_author=M Brett; citation_journal_title=Behavioural Neurology; citation_volume=12; citation_number=12; citation_pages=191-200; citation_date=2000; " />
      <meta name="citation_reference" content="citation_title=Learning to decode cognitive states from brain images.; citation_author=TM Mitchell; citation_author=R Hutchinson; citation_author=RS Niculescu; citation_author=F Pereira; citation_author=X Wang; citation_journal_title=Machine Learning; citation_volume=57; citation_number=13; citation_pages=145-175; citation_date=2004; " />
      <meta name="citation_reference" content="citation_title=The ACT (STATIS method).; citation_author=C Lavit; citation_author=Y Escoufier; citation_author=R Sabatier; citation_author=P Traissac; citation_journal_title=Computational statistics and data analysis; citation_volume=18; citation_number=14; citation_pages=97-119; citation_date=1994; " />
      <meta name="citation_reference" content="citation_title=STATIS.; citation_author=H Abdi; citation_author=D Valentin; citation_number=15; citation_pages=955-962; citation_date=2007; citation_publisher=Sage; " />
      <meta name="citation_reference" content="citation_title=A unifying tool for linear multivariate statistical methods: the RV-coefficient.; citation_author=P Robert; citation_author=Y Escoufier; citation_journal_title=Applied Statistics; citation_volume=25; citation_number=16; citation_pages=257-265; citation_date=1976; " />
      <meta name="citation_reference" content="citation_title=Group analysis in functional neuroimaging: selecting subjects using similarity measures.; citation_author=F Kherif; citation_author=JB Poline; citation_author=S Meriaux; citation_author=H Benali; citation_author=G Flandin; citation_journal_title=NeuroImage; citation_volume=20; citation_number=17; citation_pages=2197-2208; citation_date=2003; " />
      <meta name="citation_reference" content="citation_title=Classification of functional brain images with a spatio-temporal dissimilarity map.; citation_author=SV Shinkareva; citation_author=HC Ombao; citation_author=BP Sutton; citation_author=A Mohanty; citation_author=GA Miller; citation_journal_title=NeuroImage; citation_volume=33; citation_number=18; citation_pages=63-71; citation_date=2006; " />
      <meta name="citation_reference" content="citation_title=Representation of manipulable man-made objects in the dorsal stream.; citation_author=LL Chao; citation_author=A Martin; citation_journal_title=NeuroImage; citation_volume=12; citation_number=19; citation_pages=478-484; citation_date=2000; " />
      <meta name="citation_reference" content="citation_title=Can segregation within the semantic system account for category-specific deficits?; citation_author=JA Phillips; citation_author=U Noppeney; citation_author=GW Humphreys; citation_author=CJ Price; citation_journal_title=Brain; citation_volume=125; citation_number=20; citation_pages=2067-2080; citation_date=2002; " />
      <meta name="citation_reference" content="citation_title=Human parietal cortex in action.; citation_author=JC Culham; citation_author=KF Valyear; citation_journal_title=Current Opinion in Neurobiology; citation_volume=16; citation_number=21; citation_pages=205-212; citation_date=2006; " />
      <meta name="citation_reference" content="citation_title=The parahippocampal place area: recognition, navigation, or encoding?; citation_author=R Epstein; citation_author=A Harris; citation_author=D Stanley; citation_author=N Kanwisher; citation_journal_title=Neuron; citation_volume=23; citation_number=22; citation_pages=115-125; citation_date=1999; " />
      <meta name="citation_reference" content="citation_title=Differential effects of viewpoint on object-driven activation in dorsal and ventral streams.; citation_author=TW James; citation_author=GK Humphrey; citation_author=JS Gati; citation_author=RS Menon; citation_author=MA Goodale; citation_journal_title=Neuron; citation_volume=35; citation_number=23; citation_pages=793-801; citation_date=2002; " />
      <meta name="citation_reference" content="citation_title=Differential processing of objects under various viewing conditions in the human lateral occipital complex.; citation_author=K Grill-Spector; citation_author=T Kushnir; citation_author=S Edelman; citation_author=G Avidan; citation_author=Y Itzchak; citation_journal_title=Neuron; citation_volume=24; citation_number=24; citation_pages=187-203; citation_date=1999; " />
      <meta name="citation_reference" content="citation_title=Multiple levels of visual object constancy revealed by event-related fMRI of repetition priming.; citation_author=P Vuilleumier; citation_author=RN Henson; citation_author=J Driver; citation_author=RJ Dolan; citation_journal_title=Nature Neuroscience; citation_volume=5; citation_number=25; citation_pages=491-499; citation_date=2002; " />
      <meta name="citation_reference" content="citation_title=Intersubject synchronization of cortical activity during natural vision.; citation_author=U Hasson; citation_author=Y Nir; citation_author=I Levy; citation_author=G Fuhrmann; citation_author=R Malach; citation_journal_title=Science; citation_volume=303; citation_number=26; citation_pages=1634-1640; citation_date=2004; " />
      <meta name="citation_reference" content="citation_title=The representation of objects in the human occipital and temporal cortex.; citation_author=A Ishai; citation_author=LG Ungerleider; citation_author=A Martin; citation_author=JV Haxby; citation_journal_title=Journal of Cognitive Neuroscience; citation_volume=12; citation_number=27; citation_pages=35-51; citation_date=2000; " />
      <meta name="citation_reference" content="citation_title=Distributed representation of objects in human ventral visual pathway.; citation_author=A Ishai; citation_author=LG Ungerleider; citation_author=A Martin; citation_author=JL Schouten; citation_author=JV Haxby; citation_journal_title=Proceedings of the National Academy of Sciences; citation_volume=96; citation_number=28; citation_pages=9379-9384; citation_date=1999; " />
      <meta name="citation_reference" content="citation_title=Where bottom-up meets top-down: neuronal interactions during perception and imagery.; citation_author=A Mechelli; citation_author=C Price; citation_author=KJ Friston; citation_author=A Ishai; citation_journal_title=Cerebral Cortex; citation_volume=14; citation_number=29; citation_pages=1256-1265; citation_date=2004; " />
      <meta name="citation_reference" content="citation_title=Category specificity and the brain: the sensory/motor model of semantic representations of objects.; citation_author=A Martin; citation_author=LG Ungerleider; citation_author=JV Haxby; citation_number=30; citation_pages=1023-1035; citation_date=2000; citation_publisher=MIT Press; " />
      <meta name="citation_reference" content="citation_title=Perceptual knowledge retrieval activates sensory brain regions.; citation_author=RF Goldberg; citation_author=CA Perfetti; citation_author=W Schneider; citation_journal_title=Journal of Neuroscience; citation_volume=26; citation_number=31; citation_pages=4917-4921; citation_date=2006; " />
      <meta name="citation_reference" content="citation_title=The parahippocampal cortex mediates spatial and nonspatial associations.; citation_author=E Aminoff; citation_author=N Gronau; citation_author=M Bar; citation_journal_title=Cerebral Cortex; citation_volume=17; citation_number=32; citation_pages=1493-1503; citation_date=2006; " />

  <link rel="canonical" href="http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0001394" />

    <meta name="twitter:card" content="summary"/>
    <meta name="twitter:site" content="@plosone"/>
    <meta name="twitter:title" content="Using fMRI Brain Activation to Identify Cognitive States Associated with Perception of Tools and Dwellings"/>
    <meta name="twitter:description" content="Previous studies have succeeded in identifying the cognitive state corresponding to the perception of a set of depicted categories, such as tools, by analyzing the accompanying pattern of brain activity, measured with fMRI. The current research focused on identifying the cognitive state associated with a 4s viewing of an individual line drawing (1 of 10 familiar objects, 5 tools and 5 dwellings, such as a hammer or a castle). Here we demonstrate the ability to reliably (1) identify which of the 10 drawings a participant was viewing, based on that participant's characteristic whole-brain neural activation patterns, excluding visual areas; (2) identify the category of the object with even higher accuracy, based on that participant's activation; and (3) identify, for the first time, both individual objects and the category of the object the participant was viewing, based only on other participants' activation patterns. The voxels important for category identification were located similarly across participants, and distributed throughout the cortex, focused in ventral temporal perceptual areas but also including more frontal association areas (and somewhat left-lateralized). These findings indicate the presence of stable, distributed, communal, and identifiable neural states corresponding to object concepts."/>
      <meta name="twitter:image" content="http://dx.plos.org/10.1371/journal.pone.0001394.g007"/>

  <meta property="og:title" content="Using fMRI Brain Activation to Identify Cognitive States Associated with Perception of Tools and Dwellings" />
  <meta property="og:type" content="article" />
  <meta property="og:url" content="http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0001394" />

 <!--end articleInfoX-->

  <link rel="pingback" href="http://www.plosone.org/pingback" />


  <link rel="shortcut icon" href="/images/favicon.ico" type="image/x-icon"/>
  <link rel="home" title="home" href="/"/>
  <link rel="alternate" type="application/rss+xml"
        title="PLOS ONE: New Articles"
        href="http://www.plosone.org/article/feed"/>
</head>
<body>

  <div id="page-wrap">
    <div id="topbanner" class="cf">

<!-- Div for the ad at the top of journal home page-->
<div class="center">
  <div class="title">Advertisement</div>
  <iframe id='a3ac9da4' name='a3ac9da4'
    src='http://ads.plos.org/www/delivery/afr.php?zoneid=345&amp;cb=3618'
    frameborder='0' scrolling='no' width='730' height='90'>
    <a href='http://ads.plos.org/www/delivery/ck.php?n=a3ac9da4&amp;cb=1762'
      target='_top'><img src='http://ads.plos.org/www/delivery/avw.php?zoneid=345&amp;cb=9877&amp;n=a3ac9da4'
      border='0' alt=''/>
    </a>
  </iframe>
</div>    </div>

    <div id="pagehdr-wrap">
      <div id="pagehdr">
        <div id="user" class="nav">
          <ul>
            <li><a href="http://www.plos.org">plos.org</a></li>
            <li><a href="https://register.plos.org/ambra-registration/register.action">create account</a></li>
            <li class="btn-style"><a
              href="/user/secure/secureRedirect.action?goTo=%2Farticle%2FfetchArticle.action%3FarticleURI%3Dinfo%253Adoi%252F10.1371%252Fjournal.pone.0001394">sign in</a>
            </li>
          </ul>
        </div>
        <div class="logo">
          <a href="/"><img src="/images/logo.png" alt="PLOS ONE"></a>
        </div>

<div id="nav-main" class="nav">
  <ul>
        <li id="mn-01"><a href="/taxonomy" class="areas-link">Subject Areas</a></li>
    <li id="mn-02"><a href="javascript:void(0);">For Authors</a>
      <div class="submenu" style="width: 540px; margin-left: -300px;">
        <div class="block">
          <div class="submit-script">
            <h3>Submit your Manuscript</h3>
            <ul>
              <li>Fair, rigorous peer review</li>
              <li>Broad scope and wide reach</li>
            </ul>
            <a href="/static/submissionInstructions" class="btn">get started</a>
          </div>
        </div>
        <div class="menu">
          <ul>
            <li><a href="/static/publish">Why Publish with PLOS ONE</a></li>
            <li><a href="/static/publication">Publication Criteria</a></li>
            <li><a href="/static/editorial">Editorial Policies</a></li>
            <li><a href="/static/guidelines">Preparing A Manuscript</a></li>
            <li><a href="/static/figureGuidelines">Figure and Table Guidelines</a></li>
          <li><a href="/static/supportingInformation">Supporting Information Guidelines</a></li>
            <li><a href="/static/submissionInstructions">Submitting a Manuscript</a></li>
          </ul>
        </div>
      </div>
    </li>

    <li id="mn-03"><a href="javascript:void(0);">About Us</a>
      <div class="submenu" style="width:248px; margin-left:-30px;">
        <div class="menu">
          <ul>
            <li><a href="/static/information">Journal Information</a></li>
            <li><a href="/static/edboard">Editorial Board</a></li>
            <li><a href="/static/reviewerGuidelines">Reviewer Guidelines</a></li>
            <li><a href="/static/almInfo">Article-Level Metrics</a></li>
            <li><a href="/static/license">Open-Access License</a></li>
            <li><a href="/static/downloads">Media Downloads</a></li>
            <li><a href="/static/commentGuidelines">Guidelines for Comments</a></li>
            <li><a href="/static/corrections">Corrections</a></li>
            <li><a href="/static/help">Help Using this Site</a></li>
            <li><a href="/static/contact">Contact Us</a></li>
          </ul>
        </div>
      </div>
    </li>
  </ul>
<div id="db">
  <form name="searchForm" action="/search/simple?noSearchFlag=true&amp;query=&amp;articleURI=info%3Adoi%2F10.1371%2Fjournal.pone.0001394" method="get" >
<input type="hidden" name="from" value="globalSimpleSearch" id="from"/><input type="hidden" name="filterJournals" value="PLoSONE" id="filterJournals"/>    <fieldset>
      <legend>Search</legend>
      <label for="search">Search</label>
      <div class="wrap">
        <input id="search" type="text" name="query" placeholder="Search">
        <input type="image" alt="SEARCH" src="/images/icon.search.gif">
      </div>
    </fieldset>
  </form>
    <a id="advSearch" href="/search/advanced?noSearchFlag=true&amp;query=&amp;articleURI=info%3Adoi%2F10.1371%2Fjournal.pone.0001394&filterJournals=PLoSONE">advanced search</a>
</div></div>

      </div>
      <!-- pagehdr-->
    </div>
    <!-- pagehdr-wrap -->

  <!--body and html tags gets closed in global_footer.ftl-->
<script type="text/javascript" src="/javascript/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<div id="pagebdy-wrap">
  <div id="pagebdy">

    <div id="article-block" class="cf">

<div class="article-meta cf">
  <ul id="almSignPost" style="display: none;"></ul>
  <div class="article-type">
    <span class="type oa">Open Access</span>
      <span class="type pr">Peer-Reviewed</span>
  </div>
</div>

<div class="header" id="hdr-article">

<div class="article-kicker">
      <span id="article-type-heading">
        Research Article
      </span>
</div>  <h1 property="dc:title" datatype="" rel="dc:type" href="http://purl.org/dc/dcmitype/Text">
    Using fMRI Brain Activation to Identify Cognitive States Associated with Perception of Tools and Dwellings
  </h1>

  <ul class="authors">
      <li>


        <span rel="dc:creator" class="author">
          <span class="person" property="foaf:name" typeof="foaf:Person">
            Svetlana V. Shinkareva
              <span class="corresponding">mail</span>, 
          </span>
        </span>

          <div class="author_meta">
            <div class="author_inner">


              
              <p><span class="email">*</span>To whom correspondence should be addressed. E-mail: <a href="mailto:shinkareva@sc.edu">shinkareva@sc.edu</a></p>

                <p>Affiliations:
                  Department of Psychology, Carnegie Mellon University, Pittsburgh, Pennsylvania, United States of America, 
                  Machine Learning Department, School of Computer Science, Carnegie Mellon University, Pittsburgh, Pennsylvania, United States of America
                </p>


              <span class="close">X</span>

            </div>
          </div>
      </li>
      <li>


        <span rel="dc:creator" class="author">
          <span class="person" property="foaf:name" typeof="foaf:Person">
            Robert A. Mason, 
          </span>
        </span>

          <div class="author_meta">
            <div class="author_inner">


              
              

                <p>Affiliation:
                  Department of Psychology, Carnegie Mellon University, Pittsburgh, Pennsylvania, United States of America
                </p>


              <span class="close">X</span>

            </div>
          </div>
      </li>
      <li>


        <span rel="dc:creator" class="author">
          <span class="person" property="foaf:name" typeof="foaf:Person">
            Vicente L. Malave, 
          </span>
        </span>

          <div class="author_meta">
            <div class="author_inner">


              
              

                <p>Affiliation:
                  Department of Psychology, Carnegie Mellon University, Pittsburgh, Pennsylvania, United States of America
                </p>


              <span class="close">X</span>

            </div>
          </div>
      </li>
      <li>


        <span rel="dc:creator" class="author">
          <span class="person" property="foaf:name" typeof="foaf:Person">
            Wei Wang, 
          </span>
        </span>

          <div class="author_meta">
            <div class="author_inner">


              
              

                <p>Affiliation:
                  Machine Learning Department, School of Computer Science, Carnegie Mellon University, Pittsburgh, Pennsylvania, United States of America
                </p>


              <span class="close">X</span>

            </div>
          </div>
      </li>
      <li>


        <span rel="dc:creator" class="author">
          <span class="person" property="foaf:name" typeof="foaf:Person">
            Tom M. Mitchell, 
          </span>
        </span>

          <div class="author_meta">
            <div class="author_inner">


              
              

                <p>Affiliation:
                  Machine Learning Department, School of Computer Science, Carnegie Mellon University, Pittsburgh, Pennsylvania, United States of America
                </p>


              <span class="close">X</span>

            </div>
          </div>
      </li>
      <li>


        <span rel="dc:creator" class="author">
          <span class="person" property="foaf:name" typeof="foaf:Person">
            Marcel Adam Just
          </span>
        </span>

          <div class="author_meta">
            <div class="author_inner">


              
              

                <p>Affiliation:
                  Department of Psychology, Carnegie Mellon University, Pittsburgh, Pennsylvania, United States of America
                </p>


              <span class="close">X</span>

            </div>
          </div>
      </li>
  </ul>
  <ul class="date-doi-line">
    <li>Published: January 02, 2008</li>
    <li>DOI: 10.1371/journal.pone.0001394</li>
  </ul>


</div><!--end header-->
<div class="main cf" id="pjax-container">
  

<div class="nav items-5" id="nav-article">
  <ul>
  <li>
        <span class="active" name="article">Article</span>
  </li>
  <li>
      <a href="/article/authors/info%3Adoi%2F10.1371%2Fjournal.pone.0001394" name="authors">About the Authors</a>
  </li>
  <li>
      <a href="/article/metrics/info%3Adoi%2F10.1371%2Fjournal.pone.0001394" name="metrics">Metrics</a>
  </li>
  <li>
      <a href="/article/comments/info%3Adoi%2F10.1371%2Fjournal.pone.0001394" name="comments">Comments</a>
  </li>
  <li>
      <a href="/article/related/info%3Adoi%2F10.1371%2Fjournal.pone.0001394" name="related">Related Content</a>
  </li>
  </ul>
</div>

<script type="text/javascript">
  var selected_tab = "article";
</script>
  <div id="figure-thmbs" class="carousel cf">
    <div class="wrapper">
      <div class="slider">
              <div class="item">
                <a href="#pone-0001394-g001" data-doi="info:doi/10.1371/journal.pone.0001394" data-uri="info:doi/10.1371/journal.pone.0001394.g001" title="Figure 1">
                  <span class="thmb-wrap">
                    <img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001394.g001&representation=PNG_I" alt="">
                  </span>
                </a>
              </div>
              <div class="item">
                <a href="#pone-0001394-g002" data-doi="info:doi/10.1371/journal.pone.0001394" data-uri="info:doi/10.1371/journal.pone.0001394.g002" title="Figure 2">
                  <span class="thmb-wrap">
                    <img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001394.g002&representation=PNG_I" alt="">
                  </span>
                </a>
              </div>
              <div class="item">
                <a href="#pone-0001394-g003" data-doi="info:doi/10.1371/journal.pone.0001394" data-uri="info:doi/10.1371/journal.pone.0001394.g003" title="Figure 3">
                  <span class="thmb-wrap">
                    <img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001394.g003&representation=PNG_I" alt="">
                  </span>
                </a>
              </div>
              <div class="item">
                <a href="#pone-0001394-g004" data-doi="info:doi/10.1371/journal.pone.0001394" data-uri="info:doi/10.1371/journal.pone.0001394.g004" title="Figure 4">
                  <span class="thmb-wrap">
                    <img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001394.g004&representation=PNG_I" alt="">
                  </span>
                </a>
              </div>
              <div class="item">
                <a href="#pone-0001394-g005" data-doi="info:doi/10.1371/journal.pone.0001394" data-uri="info:doi/10.1371/journal.pone.0001394.g005" title="Figure 5">
                  <span class="thmb-wrap">
                    <img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001394.g005&representation=PNG_I" alt="">
                  </span>
                </a>
              </div>
              <div class="item">
                <a href="#pone-0001394-g006" data-doi="info:doi/10.1371/journal.pone.0001394" data-uri="info:doi/10.1371/journal.pone.0001394.g006" title="Figure 6">
                  <span class="thmb-wrap">
                    <img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001394.g006&representation=PNG_I" alt="">
                  </span>
                </a>
              </div>
              <div class="item">
                <a href="#pone-0001394-t001" data-doi="info:doi/10.1371/journal.pone.0001394" data-uri="info:doi/10.1371/journal.pone.0001394.t001" title="Table 1">
                  <span class="thmb-wrap">
                    <img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001394.t001&representation=PNG_I" alt="">
                  </span>
                </a>
              </div>
              <div class="item">
                <a href="#pone-0001394-g007" data-doi="info:doi/10.1371/journal.pone.0001394" data-uri="info:doi/10.1371/journal.pone.0001394.g007" title="Figure 7">
                  <span class="thmb-wrap">
                    <img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001394.g007&representation=PNG_I" alt="">
                  </span>
                </a>
              </div>
      </div>
    </div>
  </div>

  <div class="nav-col">
    <div class="nav" id="nav-article-page">
      <ul>
        <li class="nav-col-comments"><a href="/article/comments/info%3Adoi%2F10.1371%2Fjournal.pone.0001394">Reader Comments (1)</a></li>
          <li id="nav-figures"><a data-doi="info:doi/10.1371/journal.pone.0001394" >Figures</a></li>
      </ul>
    </div>
  </div>

  <div class="article">







<div class="abstract"><a id="abstract0" name="abstract0" toc="abstract0" title="Abstract"></a><h2>Abstract</h2><a id="article1.front1.article-meta1.abstract1.p1" name="article1.front1.article-meta1.abstract1.p1"></a><p>Previous studies have succeeded in identifying the cognitive state corresponding to the perception of a set of depicted categories, such as <em>tools</em>, by analyzing the accompanying pattern of brain activity, measured with fMRI. The current research focused on identifying the cognitive state associated with a 4s viewing of an individual line drawing (1 of 10 familiar objects, 5 <em>tools</em> and 5 <em>dwellings</em>, such as a <em>hammer</em> or a <em>castle</em>). Here we demonstrate the ability to reliably (1) identify which of the 10 drawings a participant was viewing, based on that participant's characteristic whole-brain neural activation patterns, excluding visual areas; (2) identify the category of the object with even higher accuracy, based on that participant's activation; and (3) identify, for the first time, both individual objects and the category of the object the participant was viewing, based only on other participants' activation patterns. The voxels important for category identification were located similarly across participants, and distributed throughout the cortex, focused in ventral temporal perceptual areas but also including more frontal association areas (and somewhat left-lateralized). These findings indicate the presence of stable, distributed, communal, and identifiable neural states corresponding to object concepts.</p>
</div>


<div class="articleinfo"><p><strong>Citation: </strong>Shinkareva SV, Mason RA, Malave VL, Wang W, Mitchell TM, et al.  (2008) Using fMRI Brain Activation to Identify Cognitive States Associated with Perception of Tools and Dwellings. PLoS ONE 3(1):
          e1394.
            doi:10.1371/journal.pone.0001394</p><p><strong>Academic Editor: </strong>Olaf Sporns, Indiana University, United States of America</p><p><strong>Received:</strong> June 7, 2007; <strong>Accepted:</strong> December 9, 2007; <strong>Published:</strong> January 2, 2008</p><p><strong>Copyright:</strong> © 2008 Shinkareva et al. This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p><p><strong>Funding: </strong>This research was supported by the W. M. Keck Foundation and by the National Science Foundation-Collaborative Research in Computational Neuroscience Grant IIS-0423070.</p><p><strong>Competing interests:</strong> The authors have declared that no competing interests exist.</p></div>





<div id="section1" class="section"><a id="s1" name="s1" toc="s1" title="Introduction"></a><h3>Introduction</h3><a id="article1.body1.sec1.p1" name="article1.body1.sec1.p1"></a><p>It has been a lasting challenge to establish the correspondence between a simple cognitive state (such as the thought of a <em>hammer</em>) and the underlying brain activity. Moreover, it is unknown whether the correspondence is the same across individuals. A recent approach to studying brain function uses machine learning techniques to identify the neural pattern of brain activity underlying various thought processes. Previous studies using a machine learning approach have been able to identify the cognitive states associated with viewing an object category, such as houses <a href="#pone.0001394-Hanson1">[1]</a>, <a href="#pone.0001394-Cox1">[2]</a>, <a href="#pone.0001394-Carlson1">[3]</a>, <a href="#pone.0001394-MouraoMiranda1">[4]</a>, <a href="#pone.0001394-LaConte1">[5]</a>, <a href="#pone.0001394-Polyn1">[6]</a>, <a href="#pone.0001394-Haxby1">[7]</a>, <a href="#pone.0001394-OToole1">[8]</a>. The central characteristic of this approach (compared to a conventional statistical parametric mapping-like approach) is its identification of a multivariate pattern of voxels and their characteristic activation levels that collectively identify the neural response to a stimulus. These machine learning methods have the potential to be particularly useful in uncovering how semantic information about objects is represented in the cerebral cortex because they can determine the topographic distribution of the activation and distinguish the content of the information in various parts of the cortex. In the study reported below, the neural patterns associated with individual objects as well as with object categories <a href="#pone.0001394-Rosch1">[9]</a> were identified using a machine learning algorithm applied to activation distributed throughout the cortex. This study also investigated the degree to which objects and categories are similarly represented neurally across different people.</p>
<a id="article1.body1.sec1.p2" name="article1.body1.sec1.p2"></a><p>We analyzed the brain activity of participants who were viewing a line drawing of an object from the categories of <em>tools</em> or <em>dwellings</em>, of the type shown in <a href="#pone-0001394-g001">Figure 1</a>. We were able to train classifiers to identify which of ten object exemplars and two object categories a participant was viewing. We discovered a common neural pattern across participants, and used this to train a classifier to identify the correct object category and object exemplar from the fMRI data of new participants who were not involved in training the classifier.</p>
<div class="figure" id="pone-0001394-g001"><div class="img"><a name="pone-0001394-g001" title="Click for larger image " href="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001394.g001&amp;representation=PNG_M" data-doi="info:doi/10.1371/journal.pone.0001394" data-uri="info:doi/10.1371/journal.pone.0001394.g001"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001394.g001&amp;representation=PNG_I" alt="thumbnail" class="thumbnail"></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001394.g001/powerpoint">
                    PPT
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001394.g001/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001394.g001/largerimage">
                    PNG
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001394.g001/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pone.0001394.g001.PNG_L"></span>)
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001394.g001/originalimage">
                    TIFF
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001394.g001/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pone.0001394.g001.TIF"></span>)
                </a></li></ul></div><p><strong>Figure 1.  <span>Schematic depiction of presentation timing.</span></strong></p><span>doi:10.1371/journal.pone.0001394.g001</span></div></div>

<div id="section2" class="section"><a id="s2" name="s2" toc="s2" title="Materials and Methods"></a><h3>Materials and Methods</h3>
<h4>Participants</h4>
<a id="article1.body1.sec2.sec1.p1" name="article1.body1.sec2.sec1.p1"></a><p>Twelve right-handed adults (8 female) from the Carnegie Mellon community participated and gave written informed consent approved by the University of Pittsburgh and Carnegie Mellon Institutional Review Boards. Six additional participants were excluded from the analysis due to head motion greater than 2.5 mm.</p>


<h4>Experimental paradigm</h4>
<a id="article1.body1.sec2.sec2.p1" name="article1.body1.sec2.sec2.p1"></a><p>The stimuli depicted concrete objects from two semantic categories (<em>tools</em> and <em>dwellings</em>), and took the form of white line drawings on a black background. There were five exemplars per category; the objects were <em>drill</em>, <em>hammer</em>, <em>screwdriver</em>, <em>pliers</em>, <em>saw</em>, <em>apartment</em>, <em>castle</em>, <em>house</em>, <em>hut</em>, and <em>igloo</em>. The drawings of the ten objects were presented six times (in six random permutation orders) to each participant. Participants were asked to think of the same object properties each time they saw a given object, to encourage activation of multiple attributes of the depicted object, in addition to those used for visual recognition. The intention was to foster the retrieval and assessment of the most salient properties of an object. To ensure that each participant had a consistent set of properties to think about, he or she was asked to generate a set of properties for each exemplar prior to the scanning session (such as <em>cold</em>, <em>knights</em>, and <em>stone</em> for <em>castle</em>). However, nothing was done to elicit consistency across participants.</p>
<a id="article1.body1.sec2.sec2.p2" name="article1.body1.sec2.sec2.p2"></a><p>Each stimulus was presented for 3s, followed by a 7s rest period, during which the participants were instructed to fixate on an X displayed in the center of the screen. There were six additional presentations of a fixation X, 21s each, distributed across the session to provide a baseline measure of activation. A schematic representation of the presentation timing is shown in <a href="#pone-0001394-g001">Figure 1</a>.</p>


<h4>fMRI procedure</h4>
<a id="article1.body1.sec2.sec3.p1" name="article1.body1.sec2.sec3.p1"></a><p>Functional images were acquired on a Siemens Allegra 3.0T scanner (Siemens, Erlangen, Germany) at the Brain Imaging Research Center of Carnegie Mellon University and the University of Pittsburgh using a gradient echo EPI pulse sequence with TR = 1000 ms, TE = 30 ms, and a 60° flip angle. Seventeen 5-mm thick oblique-axial slices were imaged with a gap of 1 mm between slices. The acquisition matrix was 64×64 with 3.125×3.125×5 mm<sup>3</sup> voxels.</p>


<h4>fMRI data processing and analysis</h4>
<a id="article1.body1.sec2.sec4.p1" name="article1.body1.sec2.sec4.p1"></a><p>Data processing and statistical analysis were performed with Statistical Parametric Mapping software (SPM99, Wellcome Department of Imaging Neuroscience, London, UK). The data were corrected for slice timing, motion, linear trend, and were temporally smoothed with a high-pass filter using a 190 s cutoff. The data were normalized to the Montreal Neurological Institute (MNI) template brain image using a 12-parameter affine transformation. Group contrast maps were constructed using a height threshold of p&lt;0.001 (uncorrected) and an extent threshold of 160 voxels, resulting in the cluster-level threshold of p&lt;0.05, corrected for multiple comparisons.</p>
<a id="article1.body1.sec2.sec4.p2" name="article1.body1.sec2.sec4.p2"></a><p>Analyses of a single brain region at a time used region definitions derived from the Anatomical Automatic Labeling (AAL) system <a href="#pone.0001394-TzourioMazoyer1">[10]</a>. In addition to existing AAL regions, left and right intraparietal sulcus (IPS) regions were defined, and superior, middle, and inferior temporal gyrus regions were separated into anterior, middle, and posterior sections based on planes F and D from the Rademacher scheme <a href="#pone.0001394-Rademacher1">[11]</a>, for a total of 71 regions.</p>
<a id="article1.body1.sec2.sec4.p3" name="article1.body1.sec2.sec4.p3"></a><p>The data were prepared for machine learning methods by spatially normalizing the images into MNI space and resampling to 3×3×6 mm<sup>3</sup> voxels. Voxels outside the brain or absent from at least one participant were excluded from further analysis. The percent signal change (PSC) relative to the fixation condition was computed at each voxel for each object presentation. The mean PSC of the four images acquired within a 4s window, offset 4s from the stimulus onset (to account for the delay in hemodynamic response) provided the main input measure for the machine learning classifiers. The PSC data for each object-presentation were further normalized to have mean zero and variance one to equalize the between-participants variation in exemplars.</p>


<h4>Machine learning methods</h4>
<a id="article1.body1.sec2.sec5.p1" name="article1.body1.sec2.sec5.p1"></a><p>Classifiers were trained to identify cognitive states associated with viewing drawings, using the evoked pattern of functional activity (mean PSC). Classifiers were functions <em>f</em> of the form: <em>f: mean_PSC→Y<sub>j</sub></em>, <em>j</em> = 1, …, <em>m</em>, where <em>Y<sub>j</sub></em> were either categories (<em>tools</em>, <em>dwellings</em>) or ten exemplars (<em>hammer</em>, <em>pliers</em>, …, <em>house</em>), where <em>m</em> was either 2 or 10, accordingly, and where <em>mean_PSC</em> was a vector of mean PSC voxel activations. To evaluate classification performance, trials were divided into disjoint training and test sets. Prior to classification, relevant features (voxels) were extracted (as described below) to reduce the dimensionality of the data, using only the training set for this selection. A classifier was built from the training set, using these selected features. Classification performance was then evaluated on only the left-out test set, to ensure unbiased estimation of the classification error. Our previous exploration indicated that several feature selection methods and classifiers produce comparable results. Here we report results from one feature selection method and one classifier, chosen for simplicity.</p>


<h4>Feature selection</h4>
<a id="article1.body1.sec2.sec6.p1" name="article1.body1.sec2.sec6.p1"></a><p>Feature selection first identified the voxels whose responses were the most stable over six presentations of objects within a participant, and then selected from among the stable voxels those that best discriminated among objects within the training set, using only the data in the training set. The 400 most stable voxels were selected, where voxel stability was computed as the average pairwise correlation between 10-object vectors across six presentations. In the second step, all of the stable voxels were assessed for how discriminating they were, by training a logistic regression classifier to discriminate among object exemplars or categories on various subsets of only the training set. Finally, from among the 400 voxels selected for stability, discriminating subsets of sizes 10, 25, 50, 75, 100, 200, and 400 voxels were selected based on having the highest (absolute valued) regression weights in the logistic regression. Locations of these selected voxels (henceforth, diagnostic voxels) were visualized on a standard brain template using MRIcro <a href="#pone.0001394-Rorden1">[12]</a>.</p>


<h4>Classification</h4>
<a id="article1.body1.sec2.sec7.p1" name="article1.body1.sec2.sec7.p1"></a><p>The Gaussian Naïve Bayes (GNB) pooled variance classifier was used <a href="#pone.0001394-Mitchell1">[13]</a>. It is a generative classifier that models the joint distribution of a class <em>Y</em> and attributes <em>X</em>, and assumes the attributes <em>X<sub>1</sub></em>, …, <em>X<sub>n</sub></em> are conditionally independent given <em>Y</em>. The classification rule is:<a name="" id=""></a><span class="equation"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001394.e001&amp;representation=PNG" class="inline-graphic"></span><br>In this experiment classes were equally frequent. Classification results were evaluated using <em>k</em>-fold cross-validation, where one example per class was left out for each fold. For each participant, a classifier was trained to identify either which of 10 object exemplars or which of two object categories that participant was viewing, based on only 4 s of fMRI data per object presentation. In all analyses, the accuracy of identification was based only on test data that was completely disjoint from the training data. With a two-class classification problem, the chance level is 0.5. With the ten-class classification problem, <em>rank accuracy</em> was used <a href="#pone.0001394-Mitchell1">[13]</a>. The list of potential classes was rank-ordered from most to least likely, and the normalized rank of a correct class in a sorted list was computed. Rank accuracy ranges from 0 to 1, and the chance level is 0.5.</p>
<a id="article1.body1.sec2.sec7.p2" name="article1.body1.sec2.sec7.p2"></a><p>Peak classification accuracy over the previously defined subsets having different numbers of voxels, e.g., 10, 25, …, 400, was reported. To evaluate the statistical significance of this observed classification accuracy, the result was compared to a permutation distribution. For each of the 1,000 non-informative permutations of labels in the training set, permutation classification accuracies for every set of features were computed, and the best permutation accuracy over the subsets with different numbers of voxels was recorded. The observed accuracy was then compared to the distribution of recorded permutation classification accuracies; if the observed accuracy had a p-value of at most 0.001, then the result was considered statistically significant.</p>


<h4>Analyses of a single brain region at a time</h4>
<a id="article1.body1.sec2.sec8.p1" name="article1.body1.sec2.sec8.p1"></a><p>Single anatomical brain regions that consistently identified object exemplars or categories across participants were selected using cross-validation, and the significance of those identifications was tested across participants. Within each participant, a cross-validated accuracy for each region was computed by a logistic regression classifier using all the voxels from that anatomical region. The mean classification accuracy was computed for each anatomical region across participants, and compared to a binomial distribution. The obtained p-values (computed using a normal approximation) were compared to the level of significance α = 0.001, using the Bonferroni correction to account for the multiple comparisons.</p>


<h4>Analysis of the confusion patterns</h4>
<a id="article1.body1.sec2.sec9.p1" name="article1.body1.sec2.sec9.p1"></a><p>Single brain regions were compared in terms of their confusion patterns using a generalization of the principal components analysis method <a href="#pone.0001394-Lavit1">[14]</a>, <a href="#pone.0001394-Abdi1">[15]</a>. Within each participant, for each of the selected regions, a confusion matrix was constructed based on the most likely prediction of the classifier. Next, a regions-by-regions dissimilarity matrix was constructed for each participant, where the dissimilarity between any two anatomical regions was measured as one minus the correlation coefficient of the off-diagonal elements of the corresponding confusion matrices. Each dissimilarity matrix was transformed to a cross-product matrix and normalized by the first eigenvalue.</p>
<a id="article1.body1.sec2.sec9.p2" name="article1.body1.sec2.sec9.p2"></a><p>A compromise matrix, representing the agreement across participants, was constructed as a weighted average of all the participants' regions-by-regions cross-product matrices. Participants' weights were computed from the first principal component of the participants-by-participants similarity matrix (the first principal component is proportional to the mean of the participant matrices). Each entry in the participants-by-participants similarity matrix was computed by the RV-coefficient <a href="#pone.0001394-Robert1">[16]</a>, which is a multivariate extension of the Pearson correlation coefficient, and indicates the overall similarity of the two matrices:<a name="" id=""></a><span class="equation"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001394.e002&amp;representation=PNG" class="inline-graphic"></span><br>The RV-coefficient has been previously used in the fMRI literature <a href="#pone.0001394-Kherif1">[17]</a>, <a href="#pone.0001394-Shinkareva1">[18]</a>. The compromise matrix was further analyzed by principal components analysis.</p>


<h4>Multiple participant analysis</h4>
<a id="article1.body1.sec2.sec10.p1" name="article1.body1.sec2.sec10.p1"></a><p>Data from all but one participant were used to train a classifier to identify the data from the left-out participant. This process was repeated so that it reiteratively left out each of the participants. Feature selection was done by pooling the data of all participants but the one left out. Discriminating voxel subsets of sizes 10, 25, 50, 75, 100, 200, 400, 1000, and 2000 were selected on the basis of logistic regression weights.</p>

</div>

<div id="section3" class="section"><a id="s3" name="s3" toc="s3" title="Results"></a><h3>Results</h3>
<h4>Identifying object exemplars: whole brain</h4>
<a id="article1.body1.sec3.sec1.p1" name="article1.body1.sec3.sec1.p1"></a><p>The highest rank accuracy achieved for any participant while identifying individual object exemplars was 0.94. (The identification process obtained this rank accuracy by correctly identifying the object on its first-ranked guess in 40 out of 60 presentations, on its second-ranked guess in 10 presentations, and on its third- and fourth-ranked guesses in 10 other presentations.) Reliable (p&lt;0.001) classification accuracy for individual object exemplars was reached for eleven out of twelve participants (as shown by the filled bars in <a href="#pone-0001394-g002">Figure 2</a>). The mean classification rank accuracy over all 12 participants was 0.78 (SD = 0.11).</p>
<div class="figure" id="pone-0001394-g002"><div class="img"><a name="pone-0001394-g002" title="Click for larger image " href="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001394.g002&amp;representation=PNG_M" data-doi="info:doi/10.1371/journal.pone.0001394" data-uri="info:doi/10.1371/journal.pone.0001394.g002"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001394.g002&amp;representation=PNG_I" alt="thumbnail" class="thumbnail"></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001394.g002/powerpoint">
                    PPT
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001394.g002/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001394.g002/largerimage">
                    PNG
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001394.g002/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pone.0001394.g002.PNG_L"></span>)
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001394.g002/originalimage">
                    TIFF
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001394.g002/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pone.0001394.g002.TIF"></span>)
                </a></li></ul></div><p><strong>Figure 2.  <span>High classification rank accuracies for object exemplars.</span></strong></p><a id="article1.body1.sec3.sec1.fig1.caption1.p1" name="article1.body1.sec3.sec1.fig1.caption1.p1"></a><p>Reliable (p&lt;0.001) accuracies for the classification of object exemplars within participants (filled bars) were reached for eleven out of twelve participants, and reliable (p&lt;0.001) accuracies for the classification of object exemplars when training on the union of data from eleven participants (unfilled bars) were reached for eight out of twelve participants. The dashed line indicates the highest mean of the permutation distribution across participants under the null hypothesis of no difference, i.e., chance level, among object exemplars for cross-participants object exemplar identification.</p>
<span>doi:10.1371/journal.pone.0001394.g002</span></div><a id="article1.body1.sec3.sec1.p2" name="article1.body1.sec3.sec1.p2"></a><p>The locations of voxels that underpinned this accurate object exemplar identification (i.e., the diagnostic voxels), were similar (at a gyral level) across participants, and were distributed across the cortex (as shown in <a href="#pone-0001394-g003">Figure 3</a>). They were located in the left inferior frontal gyrus (LIFG), left inferior parietal lobule (LIPL), and bilateral medial frontal gyrus, precentral gyrus, posterior cingulate, parahippocampal gyrus, cuneus, lingual gyrus, fusiform gyrus, superior parietal lobule (SPL), superior temporal gyrus, and middle temporal gyrus. The number of voxels (each 3.125×3.125×6 mm<sup>3</sup> or 59 mm<sup>3</sup> in volume) for which object exemplar identification accuracy was greatest (as plotted in <a href="#pone-0001394-g002">Figure 2</a>) ranged from 25 to 400 voxels, depending on the participant (<a href="#pone.0001394.s001">Table S1</a>). (Although the results are reported here for voxel set sizes that have been tuned for individual participants, the results are not substantially different when a fixed set size of voxels is used for item and category classification, within and between participants. For example, for the within-participant identification of individual items, the mean accuracy (over participants) decreases by 2.7% (from 0.78) when a fixed size of 120 voxels is used for all participants. Thus, the optimization of voxel set size is not critical to our main arguments, and a modal fixed value of 120 voxels can provide similar outcomes.)</p>
<div class="figure" id="pone-0001394-g003"><div class="img"><a name="pone-0001394-g003" title="Click for larger image " href="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001394.g003&amp;representation=PNG_M" data-doi="info:doi/10.1371/journal.pone.0001394" data-uri="info:doi/10.1371/journal.pone.0001394.g003"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001394.g003&amp;representation=PNG_I" alt="thumbnail" class="thumbnail"></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001394.g003/powerpoint">
                    PPT
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001394.g003/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001394.g003/largerimage">
                    PNG
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001394.g003/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pone.0001394.g003.PNG_L"></span>)
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001394.g003/originalimage">
                    TIFF
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001394.g003/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pone.0001394.g003.TIF"></span>)
                </a></li></ul></div><p><strong>Figure 3.  <span>Locations of the diagnostic voxels in object exemplar classification for the three participants having the highest accuracies are shown on the three-dimensional rendering of the T1 MNI single-subject brain.</span></strong></p><a id="article1.body1.sec3.sec1.fig2.caption1.p1" name="article1.body1.sec3.sec1.fig2.caption1.p1"></a><p>Yellow ellipses indicate the commonality of the voxel locations for object identification in LIPL across participants.</p>
<span>doi:10.1371/journal.pone.0001394.g003</span></div>

<h4>Identifying object exemplars: single brain regions</h4>
<a id="article1.body1.sec3.sec2.p1" name="article1.body1.sec3.sec2.p1"></a><p>Previous studies have focused on one particular region, the ventral temporal cortex, in an attempt to relate cognitive states to activation patterns in a particular region (e.g., <a href="#pone.0001394-Haxby1">[7]</a>; Sayres, Ress, and Grill-Spector, 2005, in Proceedings of Neural Information Processing Systems). To determine whether it was possible to identify cognitive states on the basis of the activation in only a single brain region, classifiers were trained using voxels from only one anatomical region (such as LIFG) at a time. The accuracies obtained in this ancillary analysis were surprisingly high. For example, for one participant whose object exemplar identification accuracy based on the whole cortex was 0.94, the single-region accuracy was 0.77 for left superior extrastriate (SES), 0.77 for LIPL, and 0.82 for left inferior extrastriate cortex (IES). The regions that generated reliable accuracies across participants in this single-region identification were bilateral SES, IES, calcarine sulcus, fusiform gyrus, IPS, left IPL, posterior superior, middle and inferior temporal gyri, postcentral gyrus, and hippocampus. Thus, many brain regions contain information about the object exemplars.</p>
<a id="article1.body1.sec3.sec2.p2" name="article1.body1.sec3.sec2.p2"></a><p>These analyses provide two important clues about object representations in the cortex. First, they indicate that the discrimination of objects was not just mediated by basic retinotopic representations in the visual cortex, or by eye movements. Other brain areas also carry reliable information about individual <em>tools</em> and <em>dwellings</em>, demonstrating that the exemplar identification can be based on the neural representations of higher-level facets of the object properties. Second, they indicate that the activation of many regions individually can discriminate among exemplars, thus providing an important clue concerning the neural representations in different regions, which we explore below.</p>


<h4>Identifying object categories: whole brain</h4>
<a id="article1.body1.sec3.sec3.p1" name="article1.body1.sec3.sec3.p1"></a><p>A classifier was trained to decode which category that the object a participant was viewing belonged to, i.e., whether it was a <em>tool</em> or a <em>dwelling</em>. Accuracies of at least 0.97 (correct category identification in at least 58 out of 60 object presentations) were attained for four of the participants, including perfect accuracy for one of the participants (correct category identification in 60 out of 60 object presentations) (filled bars in <a href="#pone-0001394-g004">Figure 4</a>). Reliable (p&lt;0.001) classification accuracies were reached for all participants. The mean classification accuracy for category identification across twelve participants was 0.87 (SD = 0.10).</p>
<div class="figure" id="pone-0001394-g004"><div class="img"><a name="pone-0001394-g004" title="Click for larger image " href="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001394.g004&amp;representation=PNG_M" data-doi="info:doi/10.1371/journal.pone.0001394" data-uri="info:doi/10.1371/journal.pone.0001394.g004"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001394.g004&amp;representation=PNG_I" alt="thumbnail" class="thumbnail"></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001394.g004/powerpoint">
                    PPT
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001394.g004/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001394.g004/largerimage">
                    PNG
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001394.g004/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pone.0001394.g004.PNG_L"></span>)
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001394.g004/originalimage">
                    TIFF
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001394.g004/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pone.0001394.g004.TIF"></span>)
                </a></li></ul></div><p><strong>Figure 4.  <span>High classification accuracies for object categories.</span></strong></p><a id="article1.body1.sec3.sec3.fig1.caption1.p1" name="article1.body1.sec3.sec3.fig1.caption1.p1"></a><p>Reliable (p&lt;0.001) accuracies for classification of objects by category (filled bars) were reached for all participants and reliable (p&lt;0.001) accuracies for classification of objects by category when training on the union of data from eleven participants (unfilled bars) were reached for ten out of twelve participants. The dashed line indicates the highest mean of the permutation distribution across participants under the null hypothesis of no difference among the categories (i.e., chance level) for cross-participants category identification.</p>
<span>doi:10.1371/journal.pone.0001394.g004</span></div><a id="article1.body1.sec3.sec3.p2" name="article1.body1.sec3.sec3.p2"></a><p>The locations of the diagnostic voxels were distributed across the cortex. Similarity across participants in the locations of these diagnostic voxels is illustrated in <a href="#pone-0001394-g005">Figure 5</a>. The cortical locations of these voxels provide some face validity for the approach, because they are in areas previously associated with mental functions that bear a good correspondence to the stimuli used here. For example, voxels contributing to the identification of <em>tools</em> were mostly in the left hemisphere, and the largest subsets were located in the ventral premotor cortex and posterior parietal cortex. These areas were previously implicated in motor representation associated with tool usage <a href="#pone.0001394-Chao1">[19]</a>, <a href="#pone.0001394-Phillips1">[20]</a>, <a href="#pone.0001394-Culham1">[21]</a>. Some of the voxels contributing to the identification of <em>dwellings</em> were located in the right parahippocampal gyrus and were within 9 mm of the previously reported parahippocampal place area (PPA) <a href="#pone.0001394-Epstein1">[22]</a>. The number of voxels for which the object category identification accuracy was greatest ranged from 10 to 100 voxels, depending on the participant (<a href="#pone.0001394.s002">Table S2</a>). For comparison, SPM contrast maps showing areas of greater activity for the objects compared to fixation, and for <em>tools</em> compared to <em>dwellings</em>, are shown in <a href="#pone-0001394-g006">Figure 6</a>. Similar to the locations of the diagnostic voxels, the activation for <em>tools</em> relative to <em>dwellings</em> was left-lateralized, and included posterior parietal cortex. In the machine learning analysis, the spatial distribution of the diagnostic voxels was more fine-grained, with some spatial interspersing of voxels between categories, compared to the SPM contrasts.</p>
<div class="figure" id="pone-0001394-g005"><div class="img"><a name="pone-0001394-g005" title="Click for larger image " href="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001394.g005&amp;representation=PNG_M" data-doi="info:doi/10.1371/journal.pone.0001394" data-uri="info:doi/10.1371/journal.pone.0001394.g005"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001394.g005&amp;representation=PNG_I" alt="thumbnail" class="thumbnail"></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001394.g005/powerpoint">
                    PPT
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001394.g005/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001394.g005/largerimage">
                    PNG
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001394.g005/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pone.0001394.g005.PNG_L"></span>)
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001394.g005/originalimage">
                    TIFF
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001394.g005/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pone.0001394.g005.TIF"></span>)
                </a></li></ul></div><p><strong>Figure 5.  <span>Commonality in voxel locations across the three participants having the highest category classification accuracies.</span></strong></p><a id="article1.body1.sec3.sec3.fig2.caption1.p1" name="article1.body1.sec3.sec3.fig2.caption1.p1"></a><p>Voxel locations for the <em>tools</em> category are shown in red, and voxel locations for the <em>dwellings</em> category are shown in blue on the three-dimensional rendering of the T1 MNI single-subject brain. Yellow ellipses indicate commonality in voxel locations for the <em>tools</em> category in LIPL across participants.</p>
<span>doi:10.1371/journal.pone.0001394.g005</span></div><div class="figure" id="pone-0001394-g006"><div class="img"><a name="pone-0001394-g006" title="Click for larger image " href="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001394.g006&amp;representation=PNG_M" data-doi="info:doi/10.1371/journal.pone.0001394" data-uri="info:doi/10.1371/journal.pone.0001394.g006"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001394.g006&amp;representation=PNG_I" alt="thumbnail" class="thumbnail"></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001394.g006/powerpoint">
                    PPT
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001394.g006/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001394.g006/largerimage">
                    PNG
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001394.g006/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pone.0001394.g006.PNG_L"></span>)
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001394.g006/originalimage">
                    TIFF
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001394.g006/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pone.0001394.g006.TIF"></span>)
                </a></li></ul></div><p><strong>Figure 6.  <span>Brain activation showing areas of greater activity for (A) objects compared to fixation, and (B) <em>tools</em> compared to <em>dwellings.</em></span></strong></p><a id="article1.body1.sec3.sec3.fig3.caption1.p1" name="article1.body1.sec3.sec3.fig3.caption1.p1"></a><p>Activation is projected onto a surface rendering.</p>
<span>doi:10.1371/journal.pone.0001394.g006</span></div>

<h4>Identifying object categories: single brain regions</h4>
<a id="article1.body1.sec3.sec4.p1" name="article1.body1.sec3.sec4.p1"></a><p>As was the case for exemplar identification, the accuracies of the category identification using voxels from only a single anatomical region were high; in some cases, these approached the accuracy obtained when the whole cortex was used (0.93 for left IES cortex, 0.83 for left SES cortex, and 0.82 for LIPL, vs. 0.98 for the whole cortex, for one of the participants). The regions that generated reliable accuracies across participants in this single-region identification analysis were bilateral SES, calcarine, IES, SPL, IPL, IPS, fusiform, posterior superior and middle temporal, posterior inferior temporal gyri, cerebellum, and left precentral, superior frontal, inferior frontal triangularis, insula, and postcentral gyri (<a href="#pone-0001394-t001">Table 1</a>). Although the semantic category of the objects can be accurately identified on the basis of a single region, it is even more accurately identified when the whole cortex is taken into account. Similarly to the case of identifying individual object exemplars, reliable information about <em>tools</em> and <em>dwellings</em> categories resides not only in low-level visual brain areas but also in brain areas that are typically associated with higher-level properties.</p>
<div class="figure" id="pone-0001394-t001"><div class="img"><a name="pone-0001394-t001" title="Click for larger image " href="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001394.t001&amp;representation=PNG_M" data-doi="info:doi/10.1371/journal.pone.0001394" data-uri="info:doi/10.1371/journal.pone.0001394.t001"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001394.t001&amp;representation=PNG_I" alt="thumbnail" class="thumbnail"></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001394.t001/powerpoint">
                    PPT
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001394.t001/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001394.t001/largerimage">
                    PNG
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001394.t001/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pone.0001394.t001.PNG_L"></span>)
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001394.t001/originalimage">
                    TIFF
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001394.t001/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pone.0001394.t001.TIF"></span>)
                </a></li></ul></div><p><strong>Table 1.  <span>Anatomical regions (out of 71) that singly produced reliable average classification accuracies across the twelve participants for category identification.</span></strong></p><span>doi:10.1371/journal.pone.0001394.t001</span></div><a id="article1.body1.sec3.sec4.p2" name="article1.body1.sec3.sec4.p2"></a><p>The results above, along with previously published results, indicate that an object is encoded by a pattern of brain activation that is broadly distributed across the brain. The fact that it is possible to accurately identify the stimuli based on several different single regions alone raises a question of whether multiple brain regions redundantly encode the same information about the object, or whether each part of the brain encodes somewhat different information, reflecting its specialization. One way to compare the content of the neural representations in different regions is to compare the object confusion errors (incorrect first guesses) that the classifier makes when it uses input from various single regions, such as misidentifying a <em>hammer</em> as a <em>drill</em> based on only the left calcarine sulcus.</p>
<a id="article1.body1.sec3.sec4.p3" name="article1.body1.sec3.sec4.p3"></a><p>Suggestive evidence that the regions systematically differ from each other in terms of the confusion errors they generate was obtained from a principal components analysis (PCA) of the single regions' dissimilarity matrix. This matrix was constructed as a weighted average across participants (and captured 51% of the variability in the data, despite considerable variation among participants in their region-specific confusion matrices). When the confusion matrices generated by various single-region classifications were compared, a number of systematicities emerged, indicating that in fact the different regions were encoding different information. For example, a set of visual regions (CALC, FUSIFORM, IES, SES) were similar to each other with respect to the confusion errors that they generated, and they differed from a set of frontal regions (SUPFRONT, TRIA, PRECENT) in terms of their confusion errors. <a href="#pone-0001394-g007">Figure 7</a> shows that a PCA of the dissimilarity of the regions' individual confusion errors produces separation of the regions, interpretable in terms of their anatomical locations, indicating that the brain activation that is used in identification differs qualitatively and systematically across regions, such as the posterior visual regions differing from frontal regions.</p>
<div class="figure" id="pone-0001394-g007"><div class="img"><a name="pone-0001394-g007" title="Click for larger image " href="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001394.g007&amp;representation=PNG_M" data-doi="info:doi/10.1371/journal.pone.0001394" data-uri="info:doi/10.1371/journal.pone.0001394.g007"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001394.g007&amp;representation=PNG_I" alt="thumbnail" class="thumbnail"></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001394.g007/powerpoint">
                    PPT
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001394.g007/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001394.g007/largerimage">
                    PNG
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001394.g007/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pone.0001394.g007.PNG_L"></span>)
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001394.g007/originalimage">
                    TIFF
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001394.g007/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pone.0001394.g007.TIF"></span>)
                </a></li></ul></div><p><strong>Figure 7.  <span>Brain regions in the space of the first two principal components of the compromise matrix based on the regions' confusion errors.</span></strong></p><a id="article1.body1.sec3.sec4.fig1.caption1.p1" name="article1.body1.sec3.sec4.fig1.caption1.p1"></a><p>The first principal component separates anterior and posterior regions, and accounts for 8% of the variance. The second principal component separates parietal and temporal regions, and accounts for 6% of the variance in the data. Region labels are color-coded by lobe, and are described in <a href="#pone-0001394-t001">Table 1</a>. The arrows are used to separate labels that are close to each other.</p>
<span>doi:10.1371/journal.pone.0001394.g007</span></div><a id="article1.body1.sec3.sec4.p4" name="article1.body1.sec3.sec4.p4"></a><p>Another observation arising from this principal components analysis was that bilaterally homologous regions were similar to each other with respect to confusion errors, despite being physically distant from each other, suggesting that they represent and process rather similar information. This observation applies to most regions except for the frontal cortex, where the activation in the two hemispheres was more distinct and more left-lateralized. The PCA indicates that there are regularities to be explored, and other methods, such as repetition priming <a href="#pone.0001394-James1">[23]</a>, <a href="#pone.0001394-GrillSpector1">[24]</a>, <a href="#pone.0001394-Vuilleumier1">[25]</a> may additionally be useful to further illuminate which object properties are represented in various regions.</p>


<h4>Commonality of neural representations across participants</h4>
<a id="article1.body1.sec3.sec5.p1" name="article1.body1.sec3.sec5.p1"></a><p>Classifiers were trained on data from 11 of the 12 participants to determine if it was possible to identify object exemplars and categories in the held-out 12<sup>th</sup> participant's data; this procedure was repeated for all participants. For object exemplars, reliable (p&lt;0.001) identification accuracies were reached for eight out of twelve participants (unfilled bars in <a href="#pone-0001394-g002">Figure 2</a>). The highest exemplar identification rank accuracy obtained in this leave-one-participant-out method was 0.81 for one of the participants (compared to an accuracy of 0.53 from random predictions). The number of voxels for which the cross-participant object exemplar identification accuracy was greatest ranged from 50 to 2000 voxels, depending on the participant (<a href="#pone.0001394.s001">Table S1</a>).</p>
<a id="article1.body1.sec3.sec5.p2" name="article1.body1.sec3.sec5.p2"></a><p>For cross-participant identification of the object category, the highest rank accuracy obtained for one of the participants was 0.97 (the category was correctly identified on the first guess in 58 out of 60 object presentations) (unfilled bars in <a href="#pone-0001394-g004">Figure 4</a>). The classifier achieved reliable (p&lt;0.001) accuracy in ten out of twelve participants. The mean accuracy across participants was 0.82 (SD = 0.09). The number of voxels for which the cross-participant category identification accuracy was greatest ranged from 10 to 2000 voxels, depending on the participant (<a href="#pone.0001394.s002">Table S2</a>). Voxel-by-voxel synchronization between individuals has been previously shown during movie watching <a href="#pone.0001394-Hasson1">[26]</a>. The new result demonstrates the ability to identify the category of the object (and to some extent, the specific object) that a participant was viewing based on the neural signature derived from a set of other participants' activations. This finding indicates that much of the activation pattern that enables the identification of a cognitive state has a high degree of commonality across participants.</p>

</div>

<div id="section4" class="section"><a id="s4" name="s4" toc="s4" title="Discussion"></a><h3>Discussion</h3><a id="article1.body1.sec4.p1" name="article1.body1.sec4.p1"></a><p>The two main conceptual advances offered by these findings are that there is an identifiable neural pattern associated with perception and contemplation of individual objects, and that part of the pattern is shared across participants. This neural pattern is characterized by a distribution of activation across many cortical regions, involving locations that encode diverse object properties. The results uncover the biological organization of information about visually depicted objects.</p>

<h4>Distributed representation</h4>
<a id="article1.body1.sec4.sec1.p1" name="article1.body1.sec4.sec1.p1"></a><p>The fact that individual objects, and the categories they belong to, can be accurately decoded from fMRI activity in any of several regions indicates that there are multiple brain regions besides classical object-selective cortex that contain information about the objects and categories. These new findings raise the future research challenge of determining whether these multiple regions all contain similar information about the object (i.e., inter-region representational redundancy), or alternatively, whether each of the regions contains somewhat region-specific information about the object. The distributed patterns of activation evoked by objects which are being visualized include many of the parietal and prefrontal regions that contained diagnostic voxels in our study <a href="#pone.0001394-Haxby1">[7]</a>, <a href="#pone.0001394-Ishai1">[27]</a>, <a href="#pone.0001394-Ishai2">[28]</a>, <a href="#pone.0001394-Mechelli1">[29]</a>. The distributed activation pattern may reflect the distribution across cortical areas that are specialized for various types of object properties <a href="#pone.0001394-Haxby1">[7]</a>, <a href="#pone.0001394-Martin1">[30]</a>, <a href="#pone.0001394-Goldberg1">[31]</a>. For example, the diagnostic voxels from the motor cortex that helped identify the hand <em>tools</em> may have represented the motor actions involved in the use of the tools. Similarly, parahippocampal voxels that were useful for identifying <em>dwellings</em> may have represented contextual information <a href="#pone.0001394-Aminoff1">[32]</a> about some aspect of dwellinghood that has earned this region the label of “parahippocampal place area” <a href="#pone.0001394-Epstein1">[22]</a>. Similarly, other diagnostic regions presumably represented other types of visual and functional properties of the objects. An alternative characterization that is equally compatible with the empirical findings is that there are many ways in which we can think about, perceive, visualize, and interact with objects, for which different brain areas are differentially specialized. In this view, it is not just the isolated, intrinsic properties of the objects that are being represented, but also the different ways that we mentally and physically interact with the objects.</p>
<a id="article1.body1.sec4.sec1.p2" name="article1.body1.sec4.sec1.p2"></a><p>The information content within a number of individual anatomical regions is sufficient for exemplar and category identification, but the content of the representation appears to be somewhat different across regions. Comparison of the confusions in different regions suggests that despite the similarities in the identification accuracies provided by the various regions, anterior and posterior regions may represent different aspects of the objects, and that different brain regions provide the classifier with different kinds of information, likely corresponding to the different types of perceptual, motor, and conceptual processing that is performed in various brain regions.</p>


<h4>Commonality of the neural representation of object categories and exemplars across participants</h4>
<a id="article1.body1.sec4.sec2.p1" name="article1.body1.sec4.sec2.p1"></a><p>The ability to identify object categories across participants reveals the striking commonality of the neural basis of this type of semantic knowledge. The neural invariances, in terms of the locations and activation amplitudes of common diagnostic voxels, emerged despite the methodological difficulty of normalizing the morphological differences among participants. The challenge of comparing the thoughts of different people has been met here in a very limited sense, although there always remains uncertainty about whether the information content corresponding to a diagnostic voxel's activity was the same across participants. Still, the new findings indicate that there is cross-participant commonality in the neural signature at the level of semantic property representations (and not just visual features).</p>
<a id="article1.body1.sec4.sec2.p2" name="article1.body1.sec4.sec2.p2"></a><p>The category and exemplar classification accuracies when training across participants were on average lower than when training within participants, indicating that a critical diagnostic portion of the neural representation of the categories and exemplars is still idiosyncratic to individual participants. There is apparently systematic activation within an individual (permitting better identification of that individual's cognitive state) that lends individuality to object representations.</p>
<a id="article1.body1.sec4.sec2.p3" name="article1.body1.sec4.sec2.p3"></a><p>Even though the classification accuracy was generally higher within as opposed to across participants, for a small number of participants (all of whom had low within-participant identification accuracies), identification based on training data from other participants actually resulted in higher accuracy than when training based on that participant's own data. In these few cases, the individual's idiosyncratic activation pattern may have been too variable over presentations to outperform the communal neural signature. These cases provide a demonstration of the remarkable power of the shared activation pattern to identify the thoughts of others.</p>

</div>

<div id="section5" class="section"><a id="s5" name="s5" toc="s5" title="Supporting Information"></a><h3>Supporting Information</h3><div class="figshare_widget" doi="10.1371/journal.pone.0001394"></div><a name="pone.0001394.s001" id="pone.0001394.s001"></a><p class="siTitle"><strong><a href="/article/fetchSingleRepresentation.action?uri=info:doi/10.1371/journal.pone.0001394.s001">Table S1. </a></strong></p><a id="article1.body1.sec5.supplementary-material1.caption1.p1" name="article1.body1.sec5.supplementary-material1.caption1.p1"></a><p class="preSiDOI">Identification accuracies of object exemplars based on the patterns of functional activity of that or other participants. Observed accuracies, number of voxels, and the p-value based on permutation distribution with 1,000 permutations are reported.</p>
<p class="siDoi">doi:10.1371/journal.pone.0001394.s001</p><a id="article1.body1.sec5.supplementary-material1.caption1.p2" name="article1.body1.sec5.supplementary-material1.caption1.p2"></a><p class="postSiDOI">(0.04 MB DOC)</p>
<a name="pone.0001394.s002" id="pone.0001394.s002"></a><p class="siTitle"><strong><a href="/article/fetchSingleRepresentation.action?uri=info:doi/10.1371/journal.pone.0001394.s002">Table S2. </a></strong></p><a id="article1.body1.sec5.supplementary-material2.caption1.p1" name="article1.body1.sec5.supplementary-material2.caption1.p1"></a><p class="preSiDOI">Identification accuracies of object categories based on the patterns of functional activity of that or other participants. Observed accuracies, number of voxels, and the p-value based on permutation distribution with 1,000 permutations are reported.</p>
<p class="siDoi">doi:10.1371/journal.pone.0001394.s002</p><a id="article1.body1.sec5.supplementary-material2.caption1.p2" name="article1.body1.sec5.supplementary-material2.caption1.p2"></a><p class="postSiDOI">(0.04 MB DOC)</p>
</div>





<div><a id="ack" name="ack" toc="ack" title="Acknowledgments"></a><h3>Acknowledgments</h3>
<a id="article1.back1.ack1.p1" name="article1.back1.ack1.p1"></a><p>We would like to thank Vladimir Cherkassky and Sandesh Aryal for technical assistance, reviewers for helpful comments on the earlier version of the manuscript and Stacey Becker and Rachel Krishnaswami for help in the preparation of the manuscript.</p>
</div><div class="contributions"><a id="authcontrib" name="authcontrib" toc="authcontrib" title="Author Contributions"></a><h3>Author Contributions</h3><p>Conceived and designed the experiments: MJ SS TM. Performed the experiments: SS. Analyzed the data: SS VM. Contributed reagents/materials/analysis tools: SS RM VM WW. Wrote the paper: MJ SS TM.</p></div><div><a id="references" name="references" toc="references" title="References"></a><h3>References</h3><ol class="references"><li><span class="label">1.
              </span><a name="pone.0001394-Hanson1" id="pone.0001394-Hanson1"></a>Hanson SJ, Matsuka T, Haxby JV (2004) Combinatorial codes in ventral temporal lobe for object recognition: Haxby (2001) revisited: is there a “face” area? NeuroImage  23: 156–166.  <ul class="find" data-citedArticleID="1037808" data-doi="10.1016/j.neuroimage.2004.05.020"><li><a href="http://dx.doi.org/10.1016/j.neuroimage.2004.05.020" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Combinatorial+codes+in+ventral+temporal+lobe+for+object+recognition%3A+Haxby+%282001%29+revisited%3A+is+there+a+%E2%80%9Cface%E2%80%9D+area%3F" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Combinatorial+codes+in+ventral+temporal+lobe+for+object+recognition%3A+Haxby+%282001%29+revisited%3A+is+there+a+%E2%80%9Cface%E2%80%9D+area%3F%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">2.
              </span><a name="pone.0001394-Cox1" id="pone.0001394-Cox1"></a>Cox DD, Savoy RL (2003) Functional magnetic resonance imaging (fMRI) “brain reading”: detecting and classifying distributed patterns of fMRI activity in human visual cortex. NeuroImage  19: 261–270.  <ul class="find" data-citedArticleID="1037798" data-doi="10.1016/s1053-8119(03)00049-1"><li><a href="http://dx.doi.org/10.1016/s1053-8119(03)00049-1" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Functional+magnetic+resonance+imaging+%28fMRI%29+%E2%80%9Cbrain+reading%E2%80%9D%3A+detecting+and+classifying+distributed+patterns+of+fMRI+activity+in+human+visual+cortex." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Functional+magnetic+resonance+imaging+%28fMRI%29+%E2%80%9Cbrain+reading%E2%80%9D%3A+detecting+and+classifying+distributed+patterns+of+fMRI+activity+in+human+visual+cortex.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">3.
              </span><a name="pone.0001394-Carlson1" id="pone.0001394-Carlson1"></a>Carlson TA, Schrater P, He S (2003) Patterns of activity in the categorical representations of objects. Journal of Cognitive Neuroscience  15: 704–717.  <ul class="find" data-citedArticleID="1037794" data-doi="10.1162/jocn.2003.15.5.704"><li><a href="http://dx.doi.org/10.1162/jocn.2003.15.5.704" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Patterns+of+activity+in+the+categorical+representations+of+objects." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Patterns+of+activity+in+the+categorical+representations+of+objects.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">4.
              </span><a name="pone.0001394-MouraoMiranda1" id="pone.0001394-MouraoMiranda1"></a>Mourao-Miranda J, Bokde AL, Born C, Hampel H, Stetter M (2005) Classifying brain states and determining the discriminating activation patterns: support vector machine on functional MRI data. NeuroImage  28: 980–995.  <ul class="find" data-citedArticleID="1037832" data-doi="10.1016/j.neuroimage.2005.06.070"><li><a href="http://dx.doi.org/10.1016/j.neuroimage.2005.06.070" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Classifying+brain+states+and+determining+the+discriminating+activation+patterns%3A+support+vector+machine+on+functional+MRI+data." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Classifying+brain+states+and+determining+the+discriminating+activation+patterns%3A+support+vector+machine+on+functional+MRI+data.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">5.
              </span><a name="pone.0001394-LaConte1" id="pone.0001394-LaConte1"></a>LaConte S, Strother S, Cherkassky V, Anderson J, Hu X (2005) Support vector machines for temporal classification of block design fMRI data. NeuroImage  26: 317–329.  <ul class="find" data-citedArticleID="1037822" data-doi="10.1016/j.neuroimage.2005.01.048"><li><a href="http://dx.doi.org/10.1016/j.neuroimage.2005.01.048" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Support+vector+machines+for+temporal+classification+of+block+design+fMRI+data." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Support+vector+machines+for+temporal+classification+of+block+design+fMRI+data.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">6.
              </span><a name="pone.0001394-Polyn1" id="pone.0001394-Polyn1"></a>Polyn SM, Natu VS, Cohen JD, Norman KA (2005) Category-specific cortical activity precedes retrieval during memory search. Science  310: 1963–1966.  <ul class="find" data-citedArticleID="1037838" data-doi="10.1126/science.1117645"><li><a href="http://dx.doi.org/10.1126/science.1117645" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Category-specific+cortical+activity+precedes+retrieval+during+memory+search." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Category-specific+cortical+activity+precedes+retrieval+during+memory+search.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">7.
              </span><a name="pone.0001394-Haxby1" id="pone.0001394-Haxby1"></a>Haxby JV, Gobbini MI, Furey ML, Ishai A, Schouten JL, et al.  (2001) Distributed and overlapping representations of faces and objects in ventral temporal cortex. Science  293: 2425–2430.  <ul class="find" data-citedArticleID="1037812" data-doi="10.1016/s0002-9394(02)01382-x"><li><a href="http://dx.doi.org/10.1016/s0002-9394(02)01382-x" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Distributed+and+overlapping+representations+of+faces+and+objects+in+ventral+temporal+cortex." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Distributed+and+overlapping+representations+of+faces+and+objects+in+ventral+temporal+cortex.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">8.
              </span><a name="pone.0001394-OToole1" id="pone.0001394-OToole1"></a>O'Toole A, Jiang F, Abdi H, Haxby JV (2005) Partially distributed representations of objects and faces in ventral temporal cortex. Journal of Cognitive Neuroscience  17: 580–590.  <ul class="find" data-citedArticleID="1037834" data-doi="10.1162/0898929053467550"><li><a href="http://dx.doi.org/10.1162/0898929053467550" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Partially+distributed+representations+of+objects+and+faces+in+ventral+temporal+cortex." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Partially+distributed+representations+of+objects+and+faces+in+ventral+temporal+cortex.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">9.
              </span><a name="pone.0001394-Rosch1" id="pone.0001394-Rosch1"></a>Rosch E, Mervis CB, Gray WD, Johnson DM, Boyes-Braem P (1976) Basic objects in natural categories. Cognitive Psychology  8: 382–439.  <ul class="find" data-citedArticleID="1037846" data-doi="10.1016/0010-0285(76)90013-x"><li><a href="http://dx.doi.org/10.1016/0010-0285(76)90013-x" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Basic+objects+in+natural+categories." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Basic+objects+in+natural+categories.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">10.
              </span><a name="pone.0001394-TzourioMazoyer1" id="pone.0001394-TzourioMazoyer1"></a>Tzourio-Mazoyer N, Landeau B, Papathanassiou D, Crivello F, Etard O, et al.  (2002) Automated anatomical labeling of activations in SPM using a macroscopic anatomical parcellation of the MNI MRI single-subject brain. NeuroImage  15: 273–289.  <ul class="find" data-citedArticleID="1037850" data-doi="10.1006/nimg.2001.0978"><li><a href="http://dx.doi.org/10.1006/nimg.2001.0978" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Automated+anatomical+labeling+of+activations+in+SPM+using+a+macroscopic+anatomical+parcellation+of+the+MNI+MRI+single-subject+brain." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Automated+anatomical+labeling+of+activations+in+SPM+using+a+macroscopic+anatomical+parcellation+of+the+MNI+MRI+single-subject+brain.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">11.
              </span><a name="pone.0001394-Rademacher1" id="pone.0001394-Rademacher1"></a>Rademacher J, Galaburda AM, Kennedy DN, Filipek PA, Caviness VS (1992) Human cerebral cortex: localization, parcellation and morphometry with magnetic resonance imaging. Journal of Cognitive Neuroscience  4: 352–374.  <ul class="find" data-citedArticleID="1037840" data-doi="10.1162/jocn.1992.4.4.352"><li><a href="http://dx.doi.org/10.1162/jocn.1992.4.4.352" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Human+cerebral+cortex%3A+localization%2C+parcellation+and+morphometry+with+magnetic+resonance+imaging." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Human+cerebral+cortex%3A+localization%2C+parcellation+and+morphometry+with+magnetic+resonance+imaging.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">12.
              </span><a name="pone.0001394-Rorden1" id="pone.0001394-Rorden1"></a>Rorden C, Brett M (2000) Stereotaxic display of brain lesions. Behavioural Neurology  12: 191–200.  <ul class="find" data-citedArticleID="1037844" data-doi="10.1155/2000/421719"><li><a href="http://dx.doi.org/10.1155/2000/421719" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Stereotaxic+display+of+brain+lesions." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Stereotaxic+display+of+brain+lesions.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">13.
              </span><a name="pone.0001394-Mitchell1" id="pone.0001394-Mitchell1"></a>Mitchell TM, Hutchinson R, Niculescu RS, Pereira F, Wang X, et al.  (2004) Learning to decode cognitive states from brain images. Machine Learning  57: 145–175.  <ul class="find" data-citedArticleID="1037830" data-doi="10.1023/b:mach.0000035475.85309.1b"><li><a href="http://dx.doi.org/10.1023/b:mach.0000035475.85309.1b" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Learning+to+decode+cognitive+states+from+brain+images." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Learning+to+decode+cognitive+states+from+brain+images.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">14.
              </span><a name="pone.0001394-Lavit1" id="pone.0001394-Lavit1"></a>Lavit C, Escoufier Y, Sabatier R, Traissac P (1994) The ACT (STATIS method). Computational statistics and data analysis  18: 97–119.  <ul class="find" data-citedArticleID="1037824" data-doi="10.1016/0167-9473(94)90134-1"><li><a href="http://dx.doi.org/10.1016/0167-9473(94)90134-1" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=The+ACT+%28STATIS+method%29." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22The+ACT+%28STATIS+method%29.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">15.
              </span><a name="pone.0001394-Abdi1" id="pone.0001394-Abdi1"></a>Abdi H, Valentin D (2007)  STATIS. In: Salkind NJ, editor. Encyclopedia of measurement and statistics. Thousand Oaks (CA): Sage. pp. 955–962.  <ul class="find-nolinks"></ul></li><li><span class="label">16.
              </span><a name="pone.0001394-Robert1" id="pone.0001394-Robert1"></a>Robert P, Escoufier Y (1976) A unifying tool for linear multivariate statistical methods: the RV-coefficient. Applied Statistics  25: 257–265.  <ul class="find" data-citedArticleID="1037842" data-doi="10.2307/2347233"><li><a href="http://dx.doi.org/10.2307/2347233" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=A+unifying+tool+for+linear+multivariate+statistical+methods%3A+the+RV-coefficient." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22A+unifying+tool+for+linear+multivariate+statistical+methods%3A+the+RV-coefficient.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">17.
              </span><a name="pone.0001394-Kherif1" id="pone.0001394-Kherif1"></a>Kherif F, Poline JB, Meriaux S, Benali H, Flandin G, et al.  (2003) Group analysis in functional neuroimaging: selecting subjects using similarity measures. NeuroImage  20: 2197–2208.  <ul class="find" data-citedArticleID="1037820" data-doi="10.1016/j.neuroimage.2003.08.018"><li><a href="http://dx.doi.org/10.1016/j.neuroimage.2003.08.018" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Group+analysis+in+functional+neuroimaging%3A+selecting+subjects+using+similarity+measures." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Group+analysis+in+functional+neuroimaging%3A+selecting+subjects+using+similarity+measures.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">18.
              </span><a name="pone.0001394-Shinkareva1" id="pone.0001394-Shinkareva1"></a>Shinkareva SV, Ombao HC, Sutton BP, Mohanty A, Miller GA (2006) Classification of functional brain images with a spatio-temporal dissimilarity map. NeuroImage  33: 63–71.  <ul class="find" data-citedArticleID="1037848" data-doi="10.1016/j.neuroimage.2006.06.032"><li><a href="http://dx.doi.org/10.1016/j.neuroimage.2006.06.032" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Classification+of+functional+brain+images+with+a+spatio-temporal+dissimilarity+map." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Classification+of+functional+brain+images+with+a+spatio-temporal+dissimilarity+map.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">19.
              </span><a name="pone.0001394-Chao1" id="pone.0001394-Chao1"></a>Chao LL, Martin A (2000) Representation of manipulable man-made objects in the dorsal stream. NeuroImage  12: 478–484.  <ul class="find" data-citedArticleID="1037796" data-doi="10.1006/nimg.2000.0635"><li><a href="http://dx.doi.org/10.1006/nimg.2000.0635" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Representation+of+manipulable+man-made+objects+in+the+dorsal+stream." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Representation+of+manipulable+man-made+objects+in+the+dorsal+stream.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">20.
              </span><a name="pone.0001394-Phillips1" id="pone.0001394-Phillips1"></a>Phillips JA, Noppeney U, Humphreys GW, Price CJ (2002) Can segregation within the semantic system account for category-specific deficits? Brain  125: 2067–2080.  <ul class="find" data-citedArticleID="1037836" data-doi="10.1093/brain/awf215"><li><a href="http://dx.doi.org/10.1093/brain/awf215" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Can+segregation+within+the+semantic+system+account+for+category-specific+deficits%3F" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Can+segregation+within+the+semantic+system+account+for+category-specific+deficits%3F%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">21.
              </span><a name="pone.0001394-Culham1" id="pone.0001394-Culham1"></a>Culham JC, Valyear KF (2006) Human parietal cortex in action. Current Opinion in Neurobiology  16: 205–212.  <ul class="find" data-citedArticleID="1037800" data-doi="10.1016/j.conb.2006.03.005"><li><a href="http://dx.doi.org/10.1016/j.conb.2006.03.005" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Human+parietal+cortex+in+action." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Human+parietal+cortex+in+action.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">22.
              </span><a name="pone.0001394-Epstein1" id="pone.0001394-Epstein1"></a>Epstein R, Harris A, Stanley D, Kanwisher N (1999) The parahippocampal place area: recognition, navigation, or encoding? Neuron  23: 115–125.  <ul class="find" data-citedArticleID="1037802" data-doi="10.1016/s0896-6273(00)80758-8"><li><a href="http://dx.doi.org/10.1016/s0896-6273(00)80758-8" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=The+parahippocampal+place+area%3A+recognition%2C+navigation%2C+or+encoding%3F" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22The+parahippocampal+place+area%3A+recognition%2C+navigation%2C+or+encoding%3F%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">23.
              </span><a name="pone.0001394-James1" id="pone.0001394-James1"></a>James TW, Humphrey GK, Gati JS, Menon RS, Goodale MA (2002) Differential effects of viewpoint on object-driven activation in dorsal and ventral streams. Neuron  35: 793–801.  <ul class="find" data-citedArticleID="1037818" data-doi="10.1016/s0896-6273(02)00803-6"><li><a href="http://dx.doi.org/10.1016/s0896-6273(02)00803-6" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Differential+effects+of+viewpoint+on+object-driven+activation+in+dorsal+and+ventral+streams." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Differential+effects+of+viewpoint+on+object-driven+activation+in+dorsal+and+ventral+streams.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">24.
              </span><a name="pone.0001394-GrillSpector1" id="pone.0001394-GrillSpector1"></a>Grill-Spector K, Kushnir T, Edelman S, Avidan G, Itzchak Y, et al.  (1999) Differential processing of objects under various viewing conditions in the human lateral occipital complex. Neuron  24: 187–203.  <ul class="find" data-citedArticleID="1037806" data-doi="10.1016/s0896-6273(00)80832-6"><li><a href="http://dx.doi.org/10.1016/s0896-6273(00)80832-6" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Differential+processing+of+objects+under+various+viewing+conditions+in+the+human+lateral+occipital+complex." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Differential+processing+of+objects+under+various+viewing+conditions+in+the+human+lateral+occipital+complex.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">25.
              </span><a name="pone.0001394-Vuilleumier1" id="pone.0001394-Vuilleumier1"></a>Vuilleumier P, Henson RN, Driver J, Dolan RJ (2002) Multiple levels of visual object constancy revealed by event-related fMRI of repetition priming. Nature Neuroscience  5: 491–499.  <ul class="find" data-citedArticleID="1037852" data-doi="10.1038/nn839"><li><a href="http://dx.doi.org/10.1038/nn839" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Multiple+levels+of+visual+object+constancy+revealed+by+event-related+fMRI+of+repetition+priming." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Multiple+levels+of+visual+object+constancy+revealed+by+event-related+fMRI+of+repetition+priming.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">26.
              </span><a name="pone.0001394-Hasson1" id="pone.0001394-Hasson1"></a>Hasson U, Nir Y, Levy I, Fuhrmann G, Malach R (2004) Intersubject synchronization of cortical activity during natural vision. Science  303: 1634–1640.  <ul class="find" data-citedArticleID="1037810" data-doi="10.1126/science.1089506"><li><a href="http://dx.doi.org/10.1126/science.1089506" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Intersubject+synchronization+of+cortical+activity+during+natural+vision." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Intersubject+synchronization+of+cortical+activity+during+natural+vision.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">27.
              </span><a name="pone.0001394-Ishai1" id="pone.0001394-Ishai1"></a>Ishai A, Ungerleider LG, Martin A, Haxby JV (2000) The representation of objects in the human occipital and temporal cortex. Journal of Cognitive Neuroscience  12: 35–51.  <ul class="find" data-citedArticleID="1037814" data-doi="10.1162/089892900564055"><li><a href="http://dx.doi.org/10.1162/089892900564055" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=The+representation+of+objects+in+the+human+occipital+and+temporal+cortex." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22The+representation+of+objects+in+the+human+occipital+and+temporal+cortex.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">28.
              </span><a name="pone.0001394-Ishai2" id="pone.0001394-Ishai2"></a>Ishai A, Ungerleider LG, Martin A, Schouten JL, Haxby JV (1999) Distributed representation of objects in human ventral visual pathway. Proceedings of the National Academy of Sciences  96: 9379–9384.  <ul class="find" data-citedArticleID="1037816" data-doi="10.1073/pnas.96.16.9379"><li><a href="http://dx.doi.org/10.1073/pnas.96.16.9379" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Distributed+representation+of+objects+in+human+ventral+visual+pathway." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Distributed+representation+of+objects+in+human+ventral+visual+pathway.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">29.
              </span><a name="pone.0001394-Mechelli1" id="pone.0001394-Mechelli1"></a>Mechelli A, Price C, Friston KJ, Ishai A (2004) Where bottom-up meets top-down: neuronal interactions during perception and imagery. Cerebral Cortex  14: 1256–1265.  <ul class="find" data-citedArticleID="1037828" data-doi="10.1093/cercor/bhh087"><li><a href="http://dx.doi.org/10.1093/cercor/bhh087" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Where+bottom-up+meets+top-down%3A+neuronal+interactions+during+perception+and+imagery." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Where+bottom-up+meets+top-down%3A+neuronal+interactions+during+perception+and+imagery.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">30.
              </span><a name="pone.0001394-Martin1" id="pone.0001394-Martin1"></a>Martin A, Ungerleider LG, Haxby JV (2000)  Category specificity and the brain: the sensory/motor model of semantic representations of objects. In: Gazzaniga MS, editor. The new cognitive neurosciences. Cambridge: MIT Press. pp. 1023–1035.  <ul class="find-nolinks"></ul></li><li><span class="label">31.
              </span><a name="pone.0001394-Goldberg1" id="pone.0001394-Goldberg1"></a>Goldberg RF, Perfetti CA, Schneider W (2006) Perceptual knowledge retrieval activates sensory brain regions. Journal of Neuroscience  26: 4917–4921.  <ul class="find" data-citedArticleID="1037804" data-doi="10.1523/jneurosci.5389-05.2006"><li><a href="http://dx.doi.org/10.1523/jneurosci.5389-05.2006" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Perceptual+knowledge+retrieval+activates+sensory+brain+regions." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Perceptual+knowledge+retrieval+activates+sensory+brain+regions.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">32.
              </span><a name="pone.0001394-Aminoff1" id="pone.0001394-Aminoff1"></a>Aminoff E, Gronau N, Bar M (2006) The parahippocampal cortex mediates spatial and nonspatial associations. Cerebral Cortex  17: 1493–1503.  <ul class="find" data-citedArticleID="1037792" data-doi="10.1093/cercor/bhl078"><li><a href="http://dx.doi.org/10.1093/cercor/bhl078" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=The+parahippocampal+cortex+mediates+spatial+and+nonspatial+associations." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22The+parahippocampal+cortex+mediates+spatial+and+nonspatial+associations.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li></ol></div>

  </div>

      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001394.XML" value="80789"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001394.PDF" value="353864"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001394.g001.PNG_L" value="21695"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001394.g001.PNG_M" value="13879"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001394.g001.PNG_S" value="4375"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001394.g001.TIF" value="40226"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001394.g001.PNG_I" value="5132"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001394.e001.PNG" value="9645"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001394.e001.TIF" value="25812"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001394.e002.PNG" value="12231"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001394.e002.TIF" value="27736"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001394.g002.PNG_L" value="262001"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001394.g002.PNG_M" value="63533"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001394.g002.PNG_S" value="11518"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001394.g002.TIF" value="1252284"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001394.g002.PNG_I" value="23671"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001394.g003.PNG_L" value="1070926"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001394.g003.PNG_M" value="108801"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001394.g003.PNG_S" value="10426"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001394.g003.TIF" value="1700188"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001394.g003.PNG_I" value="38593"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001394.g004.PNG_L" value="261423"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001394.g004.PNG_M" value="63145"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001394.g004.PNG_S" value="11339"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001394.g004.TIF" value="1328590"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001394.g004.PNG_I" value="23825"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001394.g005.PNG_L" value="1015251"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001394.g005.PNG_M" value="110046"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001394.g005.PNG_S" value="11381"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001394.g005.TIF" value="1645798"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001394.g005.PNG_I" value="39191"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001394.g006.PNG_L" value="2099112"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001394.g006.PNG_M" value="161789"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001394.g006.PNG_S" value="9022"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001394.g006.TIF" value="2894620"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001394.g006.PNG_I" value="55473"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001394.t001.PNG_L" value="127362"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001394.t001.PNG_M" value="103061"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001394.t001.PNG_S" value="16572"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001394.t001.TIF" value="725518"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001394.t001.PNG_I" value="72594"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001394.g007.PNG_L" value="160900"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001394.g007.PNG_M" value="69996"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001394.g007.PNG_S" value="11169"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001394.g007.TIF" value="441856"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001394.g007.PNG_I" value="25717"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001394.s001.DOC" value="43520"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001394.s002.DOC" value="43520"/>

</div>
<div class="sidebar">

  <div class="article-actions cf">
      <div class="download">
        <span class="btn"><a href="/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0001394&amp;representation=PDF" title="Download" target="_blank">Download PDF</a></span>
      </div>
      <div class="btn-reveal dropdown">
        <div class="dropdown-icon">
          <span class="btn">&nbsp;</span>
        </div>

        <div class="content">
          <ul class="bullet">
            <li><a href="/article/citationList.action?articleURI=info%3Adoi%2F10.1371%2Fjournal.pone.0001394" title="Download citations">Citation</a></li>
            <li><a href="/article/fetchObjectAttachment.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0001394&amp;representation=XML" title="Download article XML">XML</a></li>
          </ul>
        </div>
      </div> <!-- end btn-reveal dropdown-->


    <div class="btn-reveal flt-l">
        <span class="btn">Print</span>
        <div class="content">
            <ul class="bullet">
                <li id="print-article"><a href="#" onclick="if(typeof(_gaq) != 'undefined'){ _gaq.push(['_trackEvent','Article', 'Print', 'Click']); } window.print(); return false;" title="Print Article">Print article</a></li>
                <li>
                  <a href="https://www.odysseypress.com/onlinehost/reprint_order.php?type=A&page=0&journal=7&doi=10.1371/journal.pone.0001394&volume=&issue=&title=Using fMRI Brain Activation to Identify Cognitive States Associated with Perception of Tools and Dwellings&author_name=Svetlana%20V.%20Shinkareva%2C%20Robert%20A.%20Mason%2C%20Vicente%20L.%20Malave%2C%20Wei%20Wang%2C%20Tom%20M.%20Mitchell%2C%20Marcel%20Adam%20Just&start_page=1&end_page=9" title="Odyssey Press">EzReprint</a>
                </li>
            </ul>
        </div>
    </div>

    <div class="btn-reveal flt-r">
        <span class="btn">Share</span>
        <div class="content">
            <ul class="social">
                <li><a href="http://www.reddit.com/submit?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0001394" target="_blank" title="Submit to Reddit"><img src="/images/icon.reddit.16.png" width="16" height="16" alt="Reddit">Reddit</a></li>

                <li><a href="https://plus.google.com/share?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0001394" target="_blank" title="Share on Google+"><img src="/images/icon.gplus.16.png" width="16" height="16" alt="Google+">Google+</a></li>

                <li><a href="http://www.stumbleupon.com/submit?url=http%3A%2F%2Fwww.plosone.org%2Farticle%2Finfo%253Adoi%252F10.1371%252Fjournal.pone.0001394" target="_blank" title="Add to StumbleUpon"><img src="/images/icon.stumble.16.png" width="16" height="16" alt="StumbleUpon">StumbleUpon</a></li>

                <li><a href="http://www.facebook.com/share.php?u=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0001394&amp;t=Using%20fMRI%20Brain%20Activation%20to%20Identify%20Cognitive%20States%20Associated%20with%20Perception%20of%20Tools%20and%20Dwellings" target="_blank" title="Share on Facebook"><img src="/images/icon.fb.16.png" width="16" height="16" alt="Facebook">Facebook</a></li>

                <li><a href="http://www.linkedin.com/shareArticle?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0001394&title=Using%20fMRI%20Brain%20Activation%20to%20Identify%20Cognitive%20States%20Associated%20with%20Perception%20of%20Tools%20and%20Dwellings&summary=Checkout%20this%20article%20I%20found%20at%20PLOS" target="_blank" title="Add to LinkedIn"><img src="/images/icon.linkedin.16.png" width="16" height="16" alt="Mendeley">LinkedIn</a></li>

                <li><a href="http://www.citeulike.org/posturl?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0001394&amp;title=Using%20fMRI%20Brain%20Activation%20to%20Identify%20Cognitive%20States%20Associated%20with%20Perception%20of%20Tools%20and%20Dwellings" target="_blank" title="Add to CiteULike"><img src="/images/icon.cul.16.png" width="16" height="16" alt="CiteULike">CiteULike</a></li>

                <li><a href="http://www.mendeley.com/import/?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0001394" target="_blank" title="Add to Mendeley"><img src="/images/icon.mendeley.16.png" width="16" height="16" alt="Mendeley">Mendeley</a></li>

                <li><a href="https://www.pubchase.com/library?add_aid=10.1371%2Fjournal.pone.0001394&amp;source=plos" target="_blank" title="Add to PubChase"><img src="/images/icon.pc.16.png" width="16" height="16" alt="PubChase">PubChase</a></li>


                <script type="text/javascript">
                    // replace tweet with one that's pre-shortened to 140 chars
                    function truncateTweetText() {
                        var twtTitle = 'Using fMRI Brain Activation to Identify Cognitive States Associated with Perception of Tools and Dwellings';
                        var twtUrl = 'http://dx.plos.org/10.1371/journal.pone.0001394';
                        // all URLs posted to twitter get auto-shortened to 20 chars.
                        var maxLength = 140 - (20 + 1);
                        // truncate the title to include space for twtTag and ellipsis (here, 10 = tag length + space + ellipsis)
                        if (twtTitle.length > maxLength) { twtTitle = twtTitle.substr(0, (maxLength - 10)) + '...'; }
                        // set the href to use the shortened tweet
                        $('#twitter-share-link').prop('href', 'http://twitter.com/intent/tweet?text=' + encodeURIComponent('#PLOSONE: ' + twtTitle + ' ' + twtUrl));
                    }
                </script>
                <li><a href="http://twitter.com/intent/tweet?text=#PLOSONE%3A%20Using%20fMRI%20Brain%20Activation%20to%20Identify%20Cognitive%20States%20Associated%20with%20Perception%20of%20Tools%20and%20Dwellings http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0001394" onclick="truncateTweetText();" target="_blank" title="Share on Twitter" id="twitter-share-link"><img src="/images/icon.twtr.16.png" width="16" height="16" alt="Twitter">Twitter</a></li>

                <li><a href="/article/email/info%3Adoi%2F10.1371%2Fjournal.pone.0001394" title="Email this article"><img src="/images/icon.email.16.png" width="16" height="16" alt="Email">Email</a></li>
            </ul>
        </div>
    </div><!--end btn-reveal flt-r-->
</div><!-- end article-actions-->

<!-- begin Crossmark -->

<a id="open-crossmark" href="#" style="margin-top: -28px; display:block"><img style="border: 0; display: none;
 padding: 10px 0 18px 0;"  id="crossmark-icon" src="/images/logo-crossmark-bw.png" /></a>
<div id="crossmark-dialog" style="display: none;" title="">
    <!-- the external CrossMark data is loaded inside this iframe -->
    <iframe id="crossmark-dialog-frame" frameborder="0"></iframe>
</div>

<!-- end crossmark -->


<div class="block" id="subject-area-sidebar-block">
    <div class="header">
        <h3>Subject Areas</h3><div title="More information" id="subject-area-sidebar-block-help-icon"><img align="right"
                                                                                                           alt="info" src="/images/button_info.png"/><div id="subject-area-sidebar-block-help"><img align="right"
                                                                                                                                                                                                    src="/images/button_info.png"/><p>
        <b>We want your feedback.</b> Do these subject areas make sense for this article? If not, click the flag
        next to the incorrect subject area and we will review it. Thanks for your help!
    </p></div></div>
    </div>


    <ul id="subject-area-sidebar-list">























          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Brain%22" title="Search for articles in the subject area:'Brain'"><div class="flagText">Brain</div></a>
              <div data-categoryid="48817" data-articleid="25618"
                   data-categoryname="Brain"
                   class="flagImage" title="Flag 'Brain' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Cognition%22" title="Search for articles in the subject area:'Cognition'"><div class="flagText">Cognition</div></a>
              <div data-categoryid="17733" data-articleid="25618"
                   data-categoryname="Cognition"
                   class="flagImage" title="Flag 'Cognition' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Functional+magnetic+resonance+imaging%22" title="Search for articles in the subject area:'Functional magnetic resonance imaging'"><div class="flagText">Functional magnetic resonance imaging</div></a>
              <div data-categoryid="46285" data-articleid="25618"
                   data-categoryname="Functional magnetic resonance imaging"
                   class="flagImage" title="Flag 'Functional magnetic resonance imaging' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Machine+learning%22" title="Search for articles in the subject area:'Machine learning'"><div class="flagText">Machine learning</div></a>
              <div data-categoryid="33719" data-articleid="25618"
                   data-categoryname="Machine learning"
                   class="flagImage" title="Flag 'Machine learning' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Neuroimaging%22" title="Search for articles in the subject area:'Neuroimaging'"><div class="flagText">Neuroimaging</div></a>
              <div data-categoryid="46063" data-articleid="25618"
                   data-categoryname="Neuroimaging"
                   class="flagImage" title="Flag 'Neuroimaging' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Perception%22" title="Search for articles in the subject area:'Perception'"><div class="flagText">Perception</div></a>
              <div data-categoryid="21209" data-articleid="25618"
                   data-categoryname="Perception"
                   class="flagImage" title="Flag 'Perception' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Permutation%22" title="Search for articles in the subject area:'Permutation'"><div class="flagText">Permutation</div></a>
              <div data-categoryid="33641" data-articleid="25618"
                   data-categoryname="Permutation"
                   class="flagImage" title="Flag 'Permutation' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Vision%22" title="Search for articles in the subject area:'Vision'"><div class="flagText">Vision</div></a>
              <div data-categoryid="32965" data-articleid="25618"
                   data-categoryname="Vision"
                   class="flagImage" title="Flag 'Vision' as inappropriate"></div>
          </li>
    </ul>
</div>

<div class="ad">
    <div class="title">Advertisement</div>






  <iframe id='a0852f54' name='a0852f54'
    src='http://ads.plos.org/www/delivery/afr.php?zoneid=381&amp;cb=5958'
    frameborder='0' scrolling='no' width='160' height='600'>
    <a href='http://ads.plos.org/www/delivery/ck.php?n=a0852f54&amp;cb=167'
      target='_top'><img src='http://ads.plos.org/www/delivery/avw.php?zoneid=381&amp;cb=8791&amp;n=a0852f54'
      border='0' alt=''/>
    </a>
  </iframe>



</div>

<div id="twitter-alm-timeline" class="twitter-alm-timeline"></div>

<div class="block sidebar-comments">
    <div class="header">
        <h3>Comments</h3>
    </div>
      <p><a href="/annotation/listThread.action?root=12133">Native language of subjects</a><br>Posted by flick</p>
</div>

</div><!-- sidebar -->
    </div>
  </div>
</div>
<script src="http://wl.figshare.com/static/p_widget.js" type="text/javascript"></script><div id="pageftr">
  <div class="ftr-cols cf">
    <div class="col col-1">
      <img src="/images/logo-plos-footer.png" alt="PLOS Logo" class="logo" />
      <p><a href="/static/releaseNotes">Ambra 2.9.16</a> Managed Colocation provided <br />by <a href="http://www.isc.org/">Internet Systems Consortium</a>.<p>
      <div class="nav nav-aux">
        <a href="/static/privacy">Privacy Policy</a> |
        <a href="/static/terms">Terms of Use</a> |
        <a href="http://www.plos.org/advertise/">Advertise</a> |
        <a href="http://www.plos.org/about/media-inquiries/">Media Inquiries</a>
      </div>
    </div>
    <div class="col col-2">
      <p><a href="http://www.plos.org/publications/journals/">Publications</a></p>
      <div class="nav">
        <ul>
          <li><a href="http://www.plosbiology.org">PLOS Biology</a></li>
          <li><a href="http://www.plosmedicine.org">PLOS Medicine</a></li>
          <li><a href="http://www.ploscompbiol.org">PLOS Computational Biology</a></li>
          <li><a href="http://currents.plos.org">PLOS Currents</a></li>
          <li><a href="http://www.plosgenetics.org">PLOS Genetics</a></li>
          <li><a href="http://www.plospathogens.org">PLOS Pathogens</a></li>
          <li><a href="http://www.plosone.org">PLOS ONE</a></li>
          <li><a href="http://www.plosntds.org">PLOS Neglected Tropical Diseases</a></li>
        </ul>
      </div>
    </div>
    <div class="col col-3">
      <div class="nav">
        <p><a href="http://www.plos.org">plos.org</a></p>
        <p><a href="http://blogs.plos.org">Blogs</a></p>
        <p><a href="http://www.ploscollections.org">Collections</a></p>
        <p><a href="/feedback/new">Send us feedback</a></p>

        <p>California (US) corporation #C2354500, based in San Francisco</p>
      </div>
    </div>
  </div>
</div><!-- pageftr -->

</div><!-- end page-wrap, this div is in header.ftl -->
<script type="text/javascript" src="/javascript/jquery-1.8.1-min.js?v=Tm7VCOzZz3lE03ghpkS6SWkHbyI"></script>
<script type="text/javascript" src="/javascript/ga-min.js?v=lNQ4gt8QcPDatjsdOFl_FGpPhLY"></script>
<script type="text/javascript" src="/javascript/jquery.hoverIntent-min.js?v=mRiGNYY9cIXxVb8u0K_MdW7hHnc"></script>
<script type="text/javascript" src="/javascript/jquery.placeholder-min.js?v=21Pn56Ur9h1N4K4VZDa0nqI3Pxo"></script>
<script type="text/javascript" src="/javascript/jquery.jsonp-2.4.0-min.js?v=lqTpzoHfSq3I5Ygo01qq5WankEo"></script>
<script type="text/javascript" src="/javascript/jquery-ui-1.9.2.custom-min.js?v=raSSlfNO0YsV5uUpAKmTB9n5VTc"></script>
<script type="text/javascript" src="/javascript/jquery.tooltip-min.js?v=cw+6Smh+mdryIA25xvqIvHMrnZM"></script>
<script type="text/javascript" src="/javascript/jquery.uniform-min.js?v=kYUAnX6W2W_2fK3RIuQ2m_YFG9U"></script>
<script type="text/javascript" src="/javascript/jquery.pjax-min.js?v=939kLBjL5_YKbx71T1RHjYaD4l8"></script>
<script type="text/javascript" src="/javascript/imagesloaded-min.js?v=XeuAp8Gc3mvQUo+wZCSF8ttPwvw"></script>
<script type="text/javascript" src="/javascript/figviewer-min.js?v=yPUa0sUQ_iHkI+IRv2i9bjyZJFo"></script>
<script type="text/javascript" src="/javascript/global-min.js?v=0Q3PwjeaWtXYDnqIsQvnL_ou0qs"></script>
<script type="text/javascript" src="/javascript/jquery.touchswipe-min.js?v=huaek_e6HqTduvCNAN91dJolTyw"></script>
<script type="text/javascript" src="/javascript/jquery.base64-min.js?v=VwV1zeVqKZj5FCAdlK0q5NRxbBg"></script>
<script type="text/javascript" src="/javascript/alm-min.js?v=Y5gm6B0b4Kx2YHNObNrgEeBgXlY"></script>
<script type="text/javascript" src="/javascript/taxonomy-browser-min.js?v=vBVMuDMYkGJCXIUxLe35GoyiJNw"></script>
<script type="text/javascript" src="/javascript/jquery.filterize-min.js?v=j0ZKVnHyk2nhFy8eIuNJkp7xaM0"></script>
<script type="text/javascript" src="/javascript/plosone-min.js?v=TK4H4arL_XBSwwJq+K1N3kqYfAI"></script>
<script type="text/javascript" src="/javascript/twitter-min.js?v=xKgcxLsQFXy+at1ao1NVke8nFlM"></script>
<script type="text/javascript" src="/javascript/crossmark.1.4-min.js?v=3FO4k0SjwTaGNnKGNSqthar1080"></script>
<script type="text/javascript">
  var _sf_async_config={uid:16579,domain:"plosone.org"};
  (function(){
    function loadChartbeat() {
      window._sf_endpt=(new Date()).getTime();
      var e = document.createElement('script');
      e.setAttribute('language', 'javascript');
      e.setAttribute('type', 'text/javascript');
      e.setAttribute('src',
          (("https:" == document.location.protocol) ? "https://a248.e.akamai.net/chartbeat.download.akamai.com/102508/" : "http://static.chartbeat.com/") +
              "js/chartbeat.js");
      document.body.appendChild(e);
    }
    var oldonload = window.onload;
    window.onload = (typeof window.onload != 'function') ?
        loadChartbeat : function() { oldonload(); loadChartbeat(); };
  })();
</script>
<!-- <script type="application/javascript" src="http://crossmark.crossref.org/javascripts/v1.3/crossmark.min.js"></script> -->

</body>
</html>
