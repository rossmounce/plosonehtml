

 



<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"
      xmlns:foaf="http://xmlns.com/foaf/0.1/"
      xmlns:dc="http://purl.org/dc/terms/"
      xmlns:doi="http://dx.doi.org/"
      xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
      xmlns:xsd="http://www.w3.org/2001/XMLSchema-datatypes#"
      lang="en" xml:lang="en"
      itemscope itemtype="http://schema.org/Article"
      class="no-js">
<head prefix="og: http://ogp.me/ns#">
  <title>PLOS ONE: Benefits of Stimulus Congruency for Multisensory Facilitation of Visual Learning</title>


<link rel="stylesheet" type="text/css"  href="/css/global-min.css?v=izteQ6tu7kgsJZW_xmrYizvKiHM" />


    <!--[if lte IE 7]>
<link rel="stylesheet" type="text/css"  href="/css/lte_ie7-min.css?v=3bykQUyQmReeuobVyPozcJ9LxRc" />
    <![endif]-->


<link rel="stylesheet" type="text/css"  href="/css/jquery-ui-min.css?v=eXDHTEJM0lIAmDe5k0I0Ad4nxNo" />


<link rel="stylesheet" type="text/css"  href="/css/journal.css?v=T7ZVxJfgk9jNxLAJ2qHz1vZpgYU" />


<link rel="stylesheet" type="text/css" media="print" href="/css/print-min.css?v=T5lb0B3q6EXBsuDluc5V5w+AkRc" />


  <link rel="stylesheet" href="http://f.fontdeck.com/s/css/js/www.plosone.org/24557.css" type="text/css"/>

  <!--chartbeat -->
  <script type="text/javascript">var _sf_startpt = (new Date()).getTime()</script>
  <script>document.documentElement.className += ' js';</script>

  <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <meta http-equiv="X-UA-Compatible" content="IE=EmulateIE7; IE=EmulateIE9"/>
  <meta name="description" content="PLOS ONE: an inclusive, peer-reviewed, open-access resource from the PUBLIC LIBRARY OF SCIENCE. Reports of well-performed scientific studies from all disciplines freely available to the whole world."/>
  <meta name="keywords" content="PLOS, Public Library of Science, Open Access, Open-Access, Science, Medicine, Biology, Research, Peer-review, Inclusive, Interdisciplinary, Ante-disciplinary, Physics, Chemistry, Engineering"/>
  <meta name="almHost" content="http://alm.plos.org/api/v3/articles"/>
  <meta name="searchHost" content="http://api.plos.org/search" />
  <meta name="termsHost" content="http://api.plos.org/terms" />
  <meta name="solrApiKey" content="plos"/>
  <meta name="almAPIKey" content="3pezRBRXdyzYW6ztfwft" />
  <meta name="currentJournal" content="PLoSONE" />
  <meta name="almRequestBatchSize" content="" />

  <meta name="citation_publisher" content="Public Library of Science"/>
  <meta name="citation_doi" content="10.1371/journal.pone.0001532"/>
  <meta name="dc.identifier" content="10.1371/journal.pone.0001532" />

    <meta name="citation_title" content="Benefits of Stimulus Congruency for Multisensory Facilitation of Visual Learning"/>
    <meta itemprop="name" content="Benefits of Stimulus Congruency for Multisensory Facilitation of Visual Learning"/>

      <meta name="citation_author" content="Robyn S. Kim"/>
            <meta name="citation_author_institution" content="Department of Psychology, University of California Los Angeles, Los Angeles, California, United States of America"/>
      <meta name="citation_author" content="Aaron R. Seitz"/>
            <meta name="citation_author_institution" content="Department of Psychology, Boston University, Boston, Massachusetts, United States of America"/>
      <meta name="citation_author" content="Ladan Shams"/>
            <meta name="citation_author_institution" content="Department of Psychology, University of California Los Angeles, Los Angeles, California, United States of America"/>

    <meta name="citation_date" content="2008/1/30"/>

  <meta name="citation_pdf_url" content="http://dx.plos.org/10.1371/journal.pone.0001532.pdf" />

      <meta name="citation_journal_title" content="PLOS ONE" />
    <meta name="citation_firstpage" content="e1532"/>
    <meta name="citation_issue" content="1"/>
    <meta name="citation_volume" content="3"/>
    <meta name="citation_issn" content="1932-6203"/>

    <meta name="citation_journal_abbrev" content="PLoS ONE" />

      <meta name="citation_reference" content="citation_title=Perceptual learning and top-down influences in primary visual cortex.; citation_author=W Li; citation_author=V Piech; citation_author=CD Gilbert; citation_journal_title=Nat Neurosci; citation_volume=7; citation_number=1; citation_pages=651-657; citation_date=2004; " />
      <meta name="citation_reference" content="citation_title=Practising orientation identification improves orientation coding in V1 neurons.; citation_author=A Schoups; citation_author=R Vogels; citation_author=N Qian; citation_author=G Orban; citation_journal_title=Nature; citation_volume=412; citation_number=2; citation_pages=549-53; citation_date=2001; " />
      <meta name="citation_reference" content="citation_title=Plasticity in the frequency representation of primary auditory cortex following discrimination training in adult owl monkeys.; citation_author=GH Recanzone; citation_author=CE Schreiner; citation_author=MM Merzenich; citation_journal_title=J Neurosci; citation_volume=13; citation_number=3; citation_pages=87-103; citation_date=1993; " />
      <meta name="citation_reference" content="citation_title=Pharmacological modulation of perceptual learning and associated cortical reorganization.; citation_author=HR Dinse; citation_author=P Ragert; citation_author=B Pleger; citation_author=P Schwenkreis; citation_author=M Tegenthoff; citation_journal_title=Science; citation_volume=301; citation_number=4; citation_pages=91-94; citation_date=2003; " />
      <meta name="citation_reference" content="citation_title=Visuo-haptic object-related activation in the ventral visual pathway.; citation_author=A Amedi; citation_author= Malach; citation_author=T Hendler; citation_author=S Peled; citation_author=E Zohary; citation_journal_title=Nature Neuroscience; citation_volume=4; citation_number=5; citation_pages=324-330; citation_date=2001; " />
      <meta name="citation_reference" content="citation_title=Activation of auditory cortex during silent lipreading.; citation_author=GA Calvert; citation_author=ET Bullmore; citation_author=MJ Brammer; citation_author=R Campbell; citation_author=SC Williams; citation_journal_title=Science; citation_volume=276; citation_number=6; citation_pages=593-596; citation_date=1997; " />
      <meta name="citation_reference" content="citation_title=Multisensory auditory-somatosensory interactions in early cortical processing revealed by high-density electrical mapping.; citation_author=JJ Foxe; citation_author=IA Morocz; citation_author=MM Murray; citation_author=BA Higgins; citation_author=DC Javitt; citation_journal_title=Cognitive Brain Research; citation_volume=10; citation_number=7; citation_pages=77-83; citation_date=2000; " />
      <meta name="citation_reference" content="citation_title=Early Modulation of visual cortex by sound: An MEG study.; citation_author=L Shams; citation_author=S Iwaki; citation_author=A Chawla; citation_author=J Bhattacharya; citation_journal_title=Neuroscience Letters; citation_volume=378; citation_number=8; citation_pages=76-81; citation_date=2005; " />
      <meta name="citation_reference" content="citation_title=Sound alters activity in human V1 in association with illusory visual perception.; citation_author=S Watkins; citation_author=L Shams; citation_author=S Tanaka; citation_author=JD Haynes; citation_author=G Rees; citation_journal_title=Neuroimage; citation_volume=31; citation_number=9; citation_pages=1247-1256; citation_date=2006; " />
      <meta name="citation_reference" content="citation_title=Involvement of visual cortex in tactile discrimination of orientation.; citation_author=A Zangaladze; citation_author=CM Epstein; citation_author=ST Grafton; citation_author=K Sathian; citation_journal_title=Nature; citation_volume=401; citation_number=10; citation_pages=587-590; citation_date=1999; " />
      <meta name="citation_reference" content="citation_title=Is neocortex essentially multisensory?; citation_author=AA Ghazanfar; citation_author=CE Schroeder; citation_journal_title=Trends in Cognitive Science; citation_volume=10; citation_number=11; citation_pages=278-85; citation_date=2006; " />
      <meta name="citation_reference" content="citation_title=Intersensory redundancy facilitates discrimination of tempo in 3-month-old infants.; citation_author=LE Bahrick; citation_author=R Flom; citation_author=R Lickliter; citation_journal_title=Developmental Psychobiology; citation_volume=41; citation_number=12; citation_pages=352-63; citation_date=2002; " />
      <meta name="citation_reference" content="citation_title=Augmented prenatal tactile and vestibular stimulation alters postnatal auditory and visual responsiveness in bobwhite quail chicks.; citation_author=R Carlsen; citation_author=R Lickliter; citation_journal_title=Dev Psychobiol; citation_volume=35; citation_number=13; citation_pages=215-225; citation_date=1999; " />
      <meta name="citation_reference" content="citation_title=Sensory modalities are not separate modalities: plasticity and interactions.; citation_author=S Shimojo; citation_author=L Shams; citation_journal_title=Current Opinion in Neurobiology; citation_volume=11; citation_number=14; citation_pages=505-509; citation_date=2001; " />
      <meta name="citation_reference" content="citation_title=Sound Facilitates Visual Learning.; citation_author=A Seitz; citation_author=R Kim; citation_author=L Shams; citation_journal_title=Curr Biol; citation_volume=16; citation_number=15; citation_pages=1422-1427; citation_date=2006; " />
      <meta name="citation_reference" content="citation_title=Task-specific disruption of perceptual learning.; citation_author=AR Seitz; citation_author=N Yamagishi; citation_author=B Werner; citation_author=N Goda; citation_author=M Kawato; citation_journal_title=Proc Natl Acad Sci U S A; citation_volume=102; citation_number=16; citation_pages=14895-14900; citation_date=2005; " />
      <meta name="citation_reference" content="citation_title=The analysis of visual motion: a comparison of neuronal and psychophysical performance.; citation_author=KH Britten; citation_author=MN Shadlen; citation_author=WT Newsome; citation_author=JA Movshon; citation_journal_title=J Neurosci; citation_volume=12; citation_number=17; citation_pages=4745-4765; citation_date=1992; " />
      <meta name="citation_reference" content="citation_title=Adaptive processing of visual motion.; citation_author=K Ball; citation_author=R Sekuler; citation_journal_title=J Exp Psychol Hum Percept Perform; citation_volume=7; citation_number=18; citation_pages=780-794; citation_date=1981; " />
      <meta name="citation_reference" content="citation_title=Using confidence intervals in within-subject designs.; citation_author=GR Loftus; citation_author=ME Masson; citation_journal_title=Psychonomic Bulletin & Review; citation_volume=1; citation_number=19; citation_pages=476-490; citation_date=1994; " />
      <meta name="citation_reference" content="citation_title=Motion opponency in visual cortex.; citation_author=DJ Heeger; citation_author=GM Boynton; citation_author=JB Demb; citation_author=E Seidemann; citation_author=WT Newsome; citation_journal_title=Journal of Neuroscience; citation_volume=19; citation_number=20; citation_pages=7162-74; citation_date=1999; " />
      <meta name="citation_reference" content="citation_title=Transparent motion perception as detection of unbalanced motion signals. I. Psychophysics.; citation_author=N Qian; citation_author=RA Andersen; citation_author=EH Adelson; citation_journal_title=Journal of Neuroscience; citation_volume=14; citation_number=21; citation_pages=7357-66; citation_date=1994; " />
      <meta name="citation_reference" content="citation_title=Inhibition and disinhibition of direction-specific mechanisms in human vision.; citation_author=E Levinson; citation_author=R Sekuler; citation_journal_title=Nature; citation_volume=254; citation_number=22; citation_pages=692-4; citation_date=1975; " />
      <meta name="citation_reference" content="citation_author=C Moore; citation_author=A Nelson; citation_number=23; citation_date=2006; " />
      <meta name="citation_reference" content="citation_title=A Common Framework for Perceptual Learning.; citation_author=A Seitz; citation_author=H Dinse; citation_journal_title=Current Opinion of Neurobiology; citation_volume=17(2); citation_number=24; citation_pages=148-153; citation_date=2007; " />
      <meta name="citation_reference" content="citation_title=Anatomical evidence of multimodal integration in primate striate cortex.; citation_author=A Falchier; citation_author=S Clavagnier; citation_author=P Barone; citation_author=H Kennedy; citation_journal_title=Journal of Neuroscience; citation_volume=22; citation_number=25; citation_pages=5749-5759; citation_date=2002; " />
      <meta name="citation_reference" content="citation_title=Multisensory convergence in calcarine visual areas in macaque monkey.; citation_author=KS Rockland; citation_author=H Ojima; citation_journal_title=International Journal of Psychophysiology; citation_volume=50; citation_number=26; citation_pages=19-26; citation_date=2003; " />

  <link rel="canonical" href="http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0001532" />

    <meta name="twitter:card" content="summary"/>
    <meta name="twitter:site" content="@plosone"/>
    <meta name="twitter:title" content="Benefits of Stimulus Congruency for Multisensory Facilitation of Visual Learning"/>
    <meta name="twitter:description" content="BackgroundStudies of perceptual learning have largely focused on unisensory stimuli. However, multisensory interactions are ubiquitous in perception, even at early processing stages, and thus can potentially play a role in learning. Here, we examine the effect of auditory-visual congruency on visual learning.Methodology/Principle FindingsSubjects were trained over five days on a visual motion coherence detection task with either congruent audiovisual, or incongruent audiovisual stimuli. Comparing performance on visual-only trials, we find that training with congruent audiovisual stimuli produces significantly better learning than training with incongruent audiovisual stimuli or with only visual stimuli.Conclusions/SignificanceThis advantage from stimulus congruency during training suggests that the benefits of multisensory training may result from audiovisual interactions at a perceptual rather than cognitive level."/>
      <meta name="twitter:image" content="http://dx.plos.org/10.1371/journal.pone.0001532.g003"/>

  <meta property="og:title" content="Benefits of Stimulus Congruency for Multisensory Facilitation of Visual Learning" />
  <meta property="og:type" content="article" />
  <meta property="og:url" content="http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0001532" />

 <!--end articleInfoX-->

  <link rel="pingback" href="http://www.plosone.org/pingback" />


  <link rel="shortcut icon" href="/images/favicon.ico" type="image/x-icon"/>
  <link rel="home" title="home" href="/"/>
  <link rel="alternate" type="application/rss+xml"
        title="PLOS ONE: New Articles"
        href="http://www.plosone.org/article/feed"/>
</head>
<body>

  <div id="page-wrap">
    <div id="topbanner" class="cf">

<!-- Div for the ad at the top of journal home page-->
<div class="center">
  <div class="title">Advertisement</div>
  <iframe id='a3ac9da4' name='a3ac9da4'
    src='http://ads.plos.org/www/delivery/afr.php?zoneid=345&amp;cb=1003'
    frameborder='0' scrolling='no' width='730' height='90'>
    <a href='http://ads.plos.org/www/delivery/ck.php?n=a3ac9da4&amp;cb=2211'
      target='_top'><img src='http://ads.plos.org/www/delivery/avw.php?zoneid=345&amp;cb=3226&amp;n=a3ac9da4'
      border='0' alt=''/>
    </a>
  </iframe>
</div>    </div>

    <div id="pagehdr-wrap">
      <div id="pagehdr">
        <div id="user" class="nav">
          <ul>
            <li><a href="http://www.plos.org">plos.org</a></li>
            <li><a href="https://register.plos.org/ambra-registration/register.action">create account</a></li>
            <li class="btn-style"><a
              href="/user/secure/secureRedirect.action?goTo=%2Farticle%2FfetchArticle.action%3FarticleURI%3Dinfo%253Adoi%252F10.1371%252Fjournal.pone.0001532">sign in</a>
            </li>
          </ul>
        </div>
        <div class="logo">
          <a href="/"><img src="/images/logo.png" alt="PLOS ONE"></a>
        </div>

<div id="nav-main" class="nav">
  <ul>
        <li id="mn-01"><a href="/taxonomy" class="areas-link">Subject Areas</a></li>
    <li id="mn-02"><a href="javascript:void(0);">For Authors</a>
      <div class="submenu" style="width: 540px; margin-left: -300px;">
        <div class="block">
          <div class="submit-script">
            <h3>Submit your Manuscript</h3>
            <ul>
              <li>Fair, rigorous peer review</li>
              <li>Broad scope and wide reach</li>
            </ul>
            <a href="/static/submissionInstructions" class="btn">get started</a>
          </div>
        </div>
        <div class="menu">
          <ul>
            <li><a href="/static/publish">Why Publish with PLOS ONE</a></li>
            <li><a href="/static/publication">Publication Criteria</a></li>
            <li><a href="/static/editorial">Editorial Policies</a></li>
            <li><a href="/static/guidelines">Preparing A Manuscript</a></li>
            <li><a href="/static/figureGuidelines">Figure and Table Guidelines</a></li>
          <li><a href="/static/supportingInformation">Supporting Information Guidelines</a></li>
            <li><a href="/static/submissionInstructions">Submitting a Manuscript</a></li>
          </ul>
        </div>
      </div>
    </li>

    <li id="mn-03"><a href="javascript:void(0);">About Us</a>
      <div class="submenu" style="width:248px; margin-left:-30px;">
        <div class="menu">
          <ul>
            <li><a href="/static/information">Journal Information</a></li>
            <li><a href="/static/edboard">Editorial Board</a></li>
            <li><a href="/static/reviewerGuidelines">Reviewer Guidelines</a></li>
            <li><a href="/static/almInfo">Article-Level Metrics</a></li>
            <li><a href="/static/license">Open-Access License</a></li>
            <li><a href="/static/downloads">Media Downloads</a></li>
            <li><a href="/static/commentGuidelines">Guidelines for Comments</a></li>
            <li><a href="/static/corrections">Corrections</a></li>
            <li><a href="/static/help">Help Using this Site</a></li>
            <li><a href="/static/contact">Contact Us</a></li>
          </ul>
        </div>
      </div>
    </li>
  </ul>
<div id="db">
  <form name="searchForm" action="/search/simple?noSearchFlag=true&amp;query=&amp;articleURI=info%3Adoi%2F10.1371%2Fjournal.pone.0001532" method="get" >
<input type="hidden" name="from" value="globalSimpleSearch" id="from"/><input type="hidden" name="filterJournals" value="PLoSONE" id="filterJournals"/>    <fieldset>
      <legend>Search</legend>
      <label for="search">Search</label>
      <div class="wrap">
        <input id="search" type="text" name="query" placeholder="Search">
        <input type="image" alt="SEARCH" src="/images/icon.search.gif">
      </div>
    </fieldset>
  </form>
    <a id="advSearch" href="/search/advanced?noSearchFlag=true&amp;query=&amp;articleURI=info%3Adoi%2F10.1371%2Fjournal.pone.0001532&filterJournals=PLoSONE">advanced search</a>
</div></div>

      </div>
      <!-- pagehdr-->
    </div>
    <!-- pagehdr-wrap -->

  <!--body and html tags gets closed in global_footer.ftl-->
<script type="text/javascript" src="/javascript/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<div id="pagebdy-wrap">
  <div id="pagebdy">

    <div id="article-block" class="cf">

<div class="article-meta cf">
  <ul id="almSignPost" style="display: none;"></ul>
  <div class="article-type">
    <span class="type oa">Open Access</span>
      <span class="type pr">Peer-Reviewed</span>
  </div>
</div>

<div class="header" id="hdr-article">

<div class="article-kicker">
      <span id="article-type-heading">
        Research Article
      </span>
</div>  <h1 property="dc:title" datatype="" rel="dc:type" href="http://purl.org/dc/dcmitype/Text">
    Benefits of Stimulus Congruency for Multisensory Facilitation of Visual Learning
  </h1>

  <ul class="authors">
      <li>


        <span rel="dc:creator" class="author">
          <span class="person" property="foaf:name" typeof="foaf:Person">
            Robyn S. Kim
              <span class="equal-contrib"
                    title="These authors contributed equally to this work">equal contributor</span>, 
          </span>
        </span>

          <div class="author_meta">
            <div class="author_inner">

                <p><span class="equal-contrib" title="These authors contributed equally to this work">equal contributor</span>
                Contributed equally to this work with: Robyn S. Kim, Aaron R. Seitz</p>

              
              

                <p>Affiliation:
                  Department of Psychology, University of California Los Angeles, Los Angeles, California, United States of America
                </p>


              <span class="close">X</span>

            </div>
          </div>
      </li>
      <li>


        <span rel="dc:creator" class="author">
          <span class="person" property="foaf:name" typeof="foaf:Person">
            Aaron R. Seitz
              <span class="equal-contrib"
                    title="These authors contributed equally to this work">equal contributor</span>, 
          </span>
        </span>

          <div class="author_meta">
            <div class="author_inner">

                <p><span class="equal-contrib" title="These authors contributed equally to this work">equal contributor</span>
                Contributed equally to this work with: Robyn S. Kim, Aaron R. Seitz</p>

              
              

                <p>Affiliation:
                  Department of Psychology, Boston University, Boston, Massachusetts, United States of America
                </p>


              <span class="close">X</span>

            </div>
          </div>
      </li>
      <li>


        <span rel="dc:creator" class="author">
          <span class="person" property="foaf:name" typeof="foaf:Person">
            Ladan Shams
              <span class="corresponding">mail</span>
          </span>
        </span>

          <div class="author_meta">
            <div class="author_inner">


              
              <p><span class="email">*</span>To whom correspondence should be addressed. E-mail: <a href="mailto:ladan@psych.ucla.edu">ladan@psych.ucla.edu</a></p>

                <p>Affiliation:
                  Department of Psychology, University of California Los Angeles, Los Angeles, California, United States of America
                </p>


              <span class="close">X</span>

            </div>
          </div>
      </li>
  </ul>
  <ul class="date-doi-line">
    <li>Published: January 30, 2008</li>
    <li>DOI: 10.1371/journal.pone.0001532</li>
  </ul>


</div><!--end header-->
<div class="main cf" id="pjax-container">
  

<div class="nav items-5" id="nav-article">
  <ul>
  <li>
        <span class="active" name="article">Article</span>
  </li>
  <li>
      <a href="/article/authors/info%3Adoi%2F10.1371%2Fjournal.pone.0001532" name="authors">About the Authors</a>
  </li>
  <li>
      <a href="/article/metrics/info%3Adoi%2F10.1371%2Fjournal.pone.0001532" name="metrics">Metrics</a>
  </li>
  <li>
      <a href="/article/comments/info%3Adoi%2F10.1371%2Fjournal.pone.0001532" name="comments">Comments</a>
  </li>
  <li>
      <a href="/article/related/info%3Adoi%2F10.1371%2Fjournal.pone.0001532" name="related">Related Content</a>
  </li>
  </ul>
</div>

<script type="text/javascript">
  var selected_tab = "article";
</script>
  <div id="figure-thmbs" class="carousel cf">
    <div class="wrapper">
      <div class="slider">
              <div class="item">
                <a href="#pone-0001532-g001" data-doi="info:doi/10.1371/journal.pone.0001532" data-uri="info:doi/10.1371/journal.pone.0001532.g001" title="Figure 1">
                  <span class="thmb-wrap">
                    <img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001532.g001&representation=PNG_I" alt="">
                  </span>
                </a>
              </div>
              <div class="item">
                <a href="#pone-0001532-g002" data-doi="info:doi/10.1371/journal.pone.0001532" data-uri="info:doi/10.1371/journal.pone.0001532.g002" title="Figure 2">
                  <span class="thmb-wrap">
                    <img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001532.g002&representation=PNG_I" alt="">
                  </span>
                </a>
              </div>
              <div class="item">
                <a href="#pone-0001532-g003" data-doi="info:doi/10.1371/journal.pone.0001532" data-uri="info:doi/10.1371/journal.pone.0001532.g003" title="Figure 3">
                  <span class="thmb-wrap">
                    <img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001532.g003&representation=PNG_I" alt="">
                  </span>
                </a>
              </div>
      </div>
    </div>
  </div>

  <div class="nav-col">
    <div class="nav" id="nav-article-page">
      <ul>
        <li class="nav-col-comments"><a href="/article/comments/info%3Adoi%2F10.1371%2Fjournal.pone.0001532">Reader Comments (0)</a></li>
          <li id="nav-figures"><a data-doi="info:doi/10.1371/journal.pone.0001532" >Figures</a></li>
      </ul>
    </div>
  </div>

  <div class="article">







<div class="abstract"><a id="abstract0" name="abstract0" toc="abstract0" title="Abstract"></a><h2>Abstract</h2>
<h3>Background</h3>
<a id="article1.front1.article-meta1.abstract1.sec1.p1" name="article1.front1.article-meta1.abstract1.sec1.p1"></a><p>Studies of perceptual learning have largely focused on unisensory stimuli. However, multisensory interactions are ubiquitous in perception, even at early processing stages, and thus can potentially play a role in learning. Here, we examine the effect of auditory-visual congruency on visual learning.</p>


<h3>Methodology/Principle Findings</h3>
<a id="article1.front1.article-meta1.abstract1.sec2.p1" name="article1.front1.article-meta1.abstract1.sec2.p1"></a><p>Subjects were trained over five days on a visual motion coherence detection task with either congruent audiovisual, or incongruent audiovisual stimuli. Comparing performance on visual-only trials, we find that training with congruent audiovisual stimuli produces significantly better learning than training with incongruent audiovisual stimuli or with only visual stimuli.</p>


<h3>Conclusions/Significance</h3>
<a id="article1.front1.article-meta1.abstract1.sec3.p1" name="article1.front1.article-meta1.abstract1.sec3.p1"></a><p>This advantage from stimulus congruency during training suggests that the benefits of multisensory training may result from audiovisual interactions at a perceptual rather than cognitive level.</p>

</div>


<div class="articleinfo"><p><strong>Citation: </strong>Kim RS, Seitz AR, Shams L (2008) Benefits of Stimulus Congruency for Multisensory Facilitation of Visual Learning. PLoS ONE 3(1):
          e1532.
            doi:10.1371/journal.pone.0001532</p><p><strong>Academic Editor: </strong>Michael Herzog, Ecole Polytechnique Federale de Lausanne, Switzerland</p><p><strong>Received:</strong> December 13, 2007; <strong>Accepted:</strong> December 21, 2007; <strong>Published:</strong> January 30, 2008</p><p><strong>Copyright:</strong> © 2008 Kim et al. This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p><p><strong>Funding: </strong>RK was supported by an NSF predoctoral fellowship. This project was partially supported by a UCLA Academic Senate grant and Career Development grant and Naval Research Laboratory grant to LS. The funders had no role in the design and conduct of the study, nor in the collection, analysis, and interpretation of the data, nor in the preparation, review, or approval of the manuscript.</p><p><strong>Competing interests:</strong> The authors have declared that no competing interests exist.</p></div>





<div id="section1" class="section"><a id="s1" name="s1" toc="s1" title="Introduction"></a><h3>Introduction</h3><a id="article1.body1.sec1.p1" name="article1.body1.sec1.p1"></a><p>Perceptual learning has been the subject of extensive study in recent years. This type of learning is particularly interesting because it seems to demonstrate a surprising degree of cortical plasticity even in adult primary visual <a href="#pone.0001532-Li1">[1]</a>–<a href="#pone.0001532-Schoups1">[2]</a>, auditory <a href="#pone.0001532-Recanzone1">[3]</a> and somotosensory areas <a href="#pone.0001532-Dinse1">[4]</a>. To date, most studies of perceptual learning have focused on the learning of sensory features with a single modality. However, crossmodal interactions are ubiquitous in human perception, and can occur even at early stages of processing in areas previously viewed as “sensory-specific” <a href="#pone.0001532-Amedi1">[5]</a>–<a href="#pone.0001532-Ghazanfar1">[11]</a>. Furthermore, crossmodal interactions are known to play an important role in the development of perceptual systems <a href="#pone.0001532-Bahrick1">[12]</a>–<a href="#pone.0001532-Shimojo1">[14]</a>. Therefore, it is likely that crossmodal interactions can mediate perceptual learning in the mature brain as well.</p>
<a id="article1.body1.sec1.p2" name="article1.body1.sec1.p2"></a><p>To explore the possible benefit of multisensory interactions in adult perceptual learning, we recently compared multisensory with unisensory training <a href="#pone.0001532-Seitz1">[15]</a> using a coherent motion detection and discrimination task. Compared to a visually trained (V) group, the audio-visually trained (AV) group showed faster learning on visual trials across the ten training sessions, suggesting that multisensory training promotes more effective encoding of information and/or better retention of learning than unisensory training. Additionally, the results of a direction test showed that performance was significantly greater for the trained than the untrained directions, indicating that the observed improvements reflected perceptual learning.</p>
<a id="article1.body1.sec1.p3" name="article1.body1.sec1.p3"></a><p>However, while this study demonstrated that sound could facilitate visual perceptual learning, it did not address the question of whether the directional congruency of the sound with the visual stimulus was a significant factor in the learning enhancement. Research by Seitz et al. <a href="#pone.0001532-Seitz2">[16]</a> has shown that the presence of incongruent stimuli during training can interfere with learning. This raises the question of whether the presence of incongruent extramodal motion stimuli (for instance auditory motion in the opposite direction to visual motion) will facilitate or inhibit learning. In the current study, we compare the effect of congruent audiovisual training with that of incongruent audiovisual training. If the facilitatory effects are merely due to extra attention/arousal or information, then an incongruent motion stimulus should provide the same level of facilitation as do congruent motion stimuli, as it contains equal stimulus energy and provides the same task-information as the congruent sound. On the other hand, if the facilitation occurs at a perceptual level rather than a cognitive, decisional one, then one may expect inhibition between two motion signals of opposite directions.</p>
<a id="article1.body1.sec1.p4" name="article1.body1.sec1.p4"></a><p>In addition, this experiment rules out a few alternative explanations that the previous experiment did not address. Namely: A) The “visual-alone” trials that were used for comparing the performance of the AV group with the V group did contain sound (stationary noise), which could have caused enhanced attention for the AV group, and thus enabled superior performance. B) While the visual signal exposure was equal between the two groups, the AV group had additional “audio-alone” trials in their training. Longer training sessions and/or the interleaving of “auditory-alone” trials with “visual-alone” trials may have facilitated perceptual learning for the AV group. C) The task for the AV group was to detect and discriminate directional motion (regardless of modality), whereas the task for the V group was to detect and discriminate visual motion. It is possible that the multisensory task for the AV group was more demanding and engaged attention more strongly, and thus produced the improved learning. Crucially, the AV training sessions in the current experiment include true unimodal visual trials (without stationary sound) for comparison with V group performance; also, “audio-alone” trials have been eliminated, thus equating length of training sessions and spacing of unimodal trials for all groups. Finally, the addition of the incongruent AV group addresses the influence of task demands, as their task is identical to that of the congruent AV group. In summary, this study addresses the question of whether the facilitation of learning by sound is specific to situations in which the sound signal is congruent with visual signal.</p>
</div>

<div id="section2" class="section"><a id="s2" name="s2" toc="s2" title="Methods"></a><h3>Methods</h3>
<h4>Participants</h4>
<a id="article1.body1.sec2.sec1.p1" name="article1.body1.sec2.sec1.p1"></a><p>Twenty-one paid subjects (aged 19–39) with normal or corrected-to-normal vision and normal hearing were recruited from the UCLA population and randomly assigned to congruent multisensory (n = 7), incongruent multisensory (n = 7), and unisensory (n = 7) groups. All participants gave informed consent for participation in the study, which was approved by the UCLA institutional review board.</p>


<h4>Task</h4>
<a id="article1.body1.sec2.sec2.p1" name="article1.body1.sec2.sec2.p1"></a><p>The task consisted of a two-interval forced-choice task in which observers reported in which interval (first or second) they detected a directional stimulus by pressing keys on the keyboard (‘a’ for the first interval, ‘z’ for the second). As shown in <a href="#pone-0001532-g001">Figure 1</a>, each trial consisted of a sequence of two visual or audiovisual stimuli (described in more detail in Visual Stimuli and Auditory Stimuli below). In one interval, the stimulus contained directional motion, in the other it contained random motion. After presentation of the two intervals, participants were cued to respond whether they detected motion in the first or the second stimulus-interval. Participants were told they could answer based on either visual or auditory stimulus. Feedback was provided immediately after their response.</p>
<div class="figure" id="pone-0001532-g001"><div class="img"><a name="pone-0001532-g001" title="Click for larger image " href="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001532.g001&amp;representation=PNG_M" data-doi="info:doi/10.1371/journal.pone.0001532" data-uri="info:doi/10.1371/journal.pone.0001532.g001"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001532.g001&amp;representation=PNG_I" alt="thumbnail" class="thumbnail"></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001532.g001/powerpoint">
                    PPT
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001532.g001/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001532.g001/largerimage">
                    PNG
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001532.g001/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pone.0001532.g001.PNG_L"></span>)
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001532.g001/originalimage">
                    TIFF
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001532.g001/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pone.0001532.g001.TIF"></span>)
                </a></li></ul></div><p><strong>Figure 1.  <span>Task Schematic.</span></strong></p><a id="article1.body1.sec2.sec2.fig1.caption1.p1" name="article1.body1.sec2.sec2.fig1.caption1.p1"></a><p>Cartoon depiction of one trial for each training condition. Arrows indicate motion direction of dots, with coherently moving dots represented by blue colored arrows for illustration purposes (in the second interval for these examples). The top row shows one trial for the unisensory (Visual) group, the second row for the congruent multisensory group, and the third for the incongruent multisensory group.</p>
<span>doi:10.1371/journal.pone.0001532.g001</span></div>

<h4>Visual Stimuli</h4>
<a id="article1.body1.sec2.sec3.p1" name="article1.body1.sec2.sec3.p1"></a><p>Visual stimuli were dynamic dot patterns of low motion coherence, presented at three levels of coherence tailored to each subject (see Stimulus Levels, below). A Movshon/Newsome-type motion algorithm <a href="#pone.0001532-Britten1">[17]</a> was employed with white dots (0.2 degree radius) in a 1°–10° annulus with a dot density of 16.7 dots per deg<sup>2</sup>/s and dot speed of 12 deg/s. In this motion algorithm, the subset of coherently moving dots is newly chosen in each frame, and the probability of a given dot lasting more than one frame is the same as the coherence level; positions of noncoherent dots are randomly generated for each frame. Because perception of cardinal directions may be robust to training <a href="#pone.0001532-Ball1">[18]</a>, we chose to train 190° (instead of 180°) for leftward motion.</p>


<h4>Auditory Stimuli</h4>
<a id="article1.body1.sec2.sec4.p1" name="article1.body1.sec2.sec4.p1"></a><p>We designed auditory motion stimuli to be analogous to the visual motion stimuli. Auditory motion was created by varying the amplitude of Gaussian white noise linearly (70–76 dB) between left and right speakers over 300 ms. This produced a percept of a stimulus moving left or right. These were presented at three signal-to-noise levels tailored to each subject (see Stimulus Levels, below) by masking the auditory signal with varying levels of white noise (bandwidth 2–10 KHz, butterworth filtered, ramped). Speakers were placed on the left and right side of the monitor with the midpoint between the speakers aligned with the fixation point. This produced the perception of sounds that were largely co-localized with the visual motion stimulus. Different coherence levels were created by varying the amplitude ratio between the auditory motion and the noise mask (i.e., computing a weighted average of auditory-motion signal and noise mask). Stimuli at all signal-to-noise levels were normalized so that each had the same root mean square and produced a reading of 76 dB on a sound-pressure meter.</p>


<h4>Stimulus Levels</h4>
<a id="article1.body1.sec2.sec5.p1" name="article1.body1.sec2.sec5.p1"></a><p>Visual and auditory levels for each subject were determined through the examination of psychometric functions for each modality using data from the practice tests. For each subject, we chose levels that approximately corresponded to 55%–65%, 65%–75%, and 75%–85% correct detection. The mean coherence levels for the three groups did not significantly differ (congruent multisensory [low 4±.2, mid 8±.5, high 12±.8]; incongruent multisensory [low 4±.3, mid 7±.6, high 11±1]; unisensory [low 4±.3, mid 7±.8, high 11±1.1]).</p>


<h4>Procedure</h4>
<a id="article1.body1.sec2.sec6.p1" name="article1.body1.sec2.sec6.p1"></a><p>For each participant, the experiment spanned eight days. All elements of the experiment were the same between groups except for training. The first day served primarily to acclimate the subjects to the task (and thus minimize task-learning effects during training) and to determine appropriate stimulus levels. This practice was composed of two tests consisting of unimodal stimuli (one audio and one visual) at high coherence (i.e. low difficulty) levels. The order of the two tests was counterbalanced across subjects. The second day consisted of a pre-test of four different visual motion directions (10°, 100°, 190°, 280°) without feedback. The last (eighth) day consisted of an identical post-test. Training sessions were conducted for 5 days (session numbers 3–7) and included feedback. The trained visual direction for all groups was leftward (190°). Training sessions for all groups lasted approximately an hour, and contained a total of 1200 trials: Visual training sessions consisted of three visual levels with 400 trials of each, and audiovisual training sessions consisted of 300 visual-only (silent) trials (three visual levels with 100 trials of each), and 900 audiovisual trials (three visual levels×three auditory levels with 100 trials of each). All trial types were pseudo-randomly interleaved. For the congruent audiovisual group, audiovisual trials always contained congruent directions across modalities (leftward auditory motion); for the incongruent multisensory group, audiovisual trials always contained opposing visual and auditory directions (rightward auditory motion). Within one audiovisual trial, the moving auditory stimulus was always in the same interval as the coherently moving visual stimulus. Coherence levels were not correlated between auditory and visual stimuli.</p>


<h4>Analysis</h4>
<a id="article1.body1.sec2.sec7.p1" name="article1.body1.sec2.sec7.p1"></a><p>In all groups, the analysis is done on data obtained from trials consisting of visual stimuli only. In this way, performance is evaluated for identical trial types for each group. Also, for this study we are most concerned with long-lasting perceptual learning effects (i.e. performance improvements that are retained across days), as opposed to potential improvements within a session. Given this, we consider only the first third of trials in each session for data analysis (results are qualitatively similar for other early phases of the sessions and full sessions). In this way we are able to avoid potential contamination from effects of fast learning and deterioration <a href="#pone.0001532-Seitz1">[15]</a>.</p>

</div>

<div id="section3" class="section"><a id="s3" name="s3" toc="s3" title="Results"></a><h3>Results</h3><a id="article1.body1.sec3.p1" name="article1.body1.sec3.p1"></a><p>In <a href="#pone-0001532-g002">Figure 2</a>, we show performance for the congruent audiovisual trained group (green), incongruent audiovisual trained group (blue), and the unisensory trained group (red), for visual-only trials (solid lines) and audiovisual trials (dashed lines) across the five days of training. While it is evident that there is a tendency for improvement in each group, improvement is clearly greatest for the congruent group. While the change in performance across the five days was highly significant for the congruent group (F(4,24) = 14.158, p&lt;.0001, one-way repeated-measure ANOVA), the performance change was only moderately significant for the unisensory group (F(4,24) = 2.938, p = .04) and marginally significant for the incongruent group (F(4,24) = 2.937, p = .053). Furthermore a 3-way ANOVA (Training Day×Training Condition×Stimulus Level) shows a significant effect of training day (F(1,18) = 62.761, p&lt;.01) and stimulus level (F(1,18) = 77.506, p&lt;.01), and an interaction between training day and training condition between the first and last day of training (F(2,18) = 3.702, p&lt;.05). However, we found no interaction between training day and stimulus level (F(2,26) = 1.144, p = .3299); therefore, we collapse data across stimulus levels.</p>
<div class="figure" id="pone-0001532-g002"><div class="img"><a name="pone-0001532-g002" title="Click for larger image " href="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001532.g002&amp;representation=PNG_M" data-doi="info:doi/10.1371/journal.pone.0001532" data-uri="info:doi/10.1371/journal.pone.0001532.g002"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001532.g002&amp;representation=PNG_I" alt="thumbnail" class="thumbnail"></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001532.g002/powerpoint">
                    PPT
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001532.g002/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001532.g002/largerimage">
                    PNG
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001532.g002/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pone.0001532.g002.PNG_L"></span>)
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001532.g002/originalimage">
                    TIFF
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001532.g002/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pone.0001532.g002.TIF"></span>)
                </a></li></ul></div><p><strong>Figure 2.  <span>Data from each training session for congruent audiovisual group (green), unisensory visual group (red), and incongruent audiovisual group (blue).</span></strong></p><a id="article1.body1.sec3.fig1.caption1.p1" name="article1.body1.sec3.fig1.caption1.p1"></a><p>Ordinate is proportion correct averaged across three signal levels, abscissa reflects training session number. Solid lines reflect performance on visual-only trials over the first third of each session; dashed lines represent performance on audiovisual trials over the first third of each session. Error bars reflect within-group standard error <a href="#pone.0001532-Loftus1">[19]</a>.</p>
<span>doi:10.1371/journal.pone.0001532.g002</span></div><a id="article1.body1.sec3.p2" name="article1.body1.sec3.p2"></a><p>While both congruent and vision groups improved significantly with training, the degree and rate of learning for the congruent audiovisual group surpassed that of the visual trained group; i.e., the congruent group followed a quadratic trajectory (polynomial regression analysis, F(1,32) = 8.67, p&lt;.01) whereas the vision and incongruent groups followed a linear course (polynomial regression analysis, F(1,33) = 12.488, p&lt;.001; F(1,33) = 7.5432, p = .01, respectively).</p>
<a id="article1.body1.sec3.p3" name="article1.body1.sec3.p3"></a><p>During training audiovisual trials were present, in addition to visual trials, in the congruent and incongruent groups (shown by dashed lines in <a href="#pone-0001532-g002">Figure 2</a>). For the congruent group, we found that performance on the audiovisual trials was significantly different than visual-only trials on the first day of training (paired t-test, p&lt;.01), but this difference largely disappeared by the second day of training (paired t-test, p = .13). For the congruent group, we found that audiovisual performance never significantly differed from visual-only performance.</p>
<a id="article1.body1.sec3.p4" name="article1.body1.sec3.p4"></a><p>Tests of multiple motion directions, including the trained direction and three untrained directions, were conducted before and after training to examine the specificity of learning. We compared the change in performance from pre- to post-test for the trained direction with the average change for the three untrained directions. If indeed the learning reflects perceptual learning rather than (or in addition to) general task learning, the improvement should not be equal between trained and untrained directions. <a href="#pone-0001532-g003">Figure 3</a> shows the difference in learning effect (i.e. increase in percent correct from pre- to post-test) between the trained and untrained directions for the audiovisual congruent, visual, and audiovisual incongruent trained groups, respectively. Similarly to performance during training, the congruent audiovisual trained group demonstrates a greater learning effect for the trained compared to untrained directions than the other two groups. The advantage for the trained direction after training is significant for the congruent group and the incongruent group (p&lt;0.05, paired t-test), suggesting that the observed improvement in performance indeed reflects low-level perceptual learning. However, the unisensory group only showed a trend for greater improvement for the trained direction than untrained directions (p = 0.12).</p>
<div class="figure" id="pone-0001532-g003"><div class="img"><a name="pone-0001532-g003" title="Click for larger image " href="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001532.g003&amp;representation=PNG_M" data-doi="info:doi/10.1371/journal.pone.0001532" data-uri="info:doi/10.1371/journal.pone.0001532.g003"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001532.g003&amp;representation=PNG_I" alt="thumbnail" class="thumbnail"></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001532.g003/powerpoint">
                    PPT
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001532.g003/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001532.g003/largerimage">
                    PNG
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001532.g003/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pone.0001532.g003.PNG_L"></span>)
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001532.g003/originalimage">
                    TIFF
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001532.g003/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pone.0001532.g003.TIF"></span>)
                </a></li></ul></div><p><strong>Figure 3.  <span>Difference in learning effect (i.e. increase in percent correct from pre- to post-test) between trained and untrained directions, for congruent, visual, and incongruent training groups.</span></strong></p><a id="article1.body1.sec3.fig2.caption1.p1" name="article1.body1.sec3.fig2.caption1.p1"></a><p>Error bars reflect standard error.</p>
<span>doi:10.1371/journal.pone.0001532.g003</span></div></div>

<div id="section4" class="section"><a id="s4" name="s4" toc="s4" title="Discussion"></a><h3>Discussion</h3><a id="article1.body1.sec4.p1" name="article1.body1.sec4.p1"></a><p>Consistent with our previous study <a href="#pone.0001532-Seitz1">[15]</a>, we found superior learning in a visual task when subjects were trained with congruent audiovisual stimuli as compared to when subjects are trained with purely visual stimuli. This holds even when the length of the training sessions and the trial spacings are equated between groups, and when the visual-alone trials used for comparison between groups are physically identical (i.e., silent).</p>
<a id="article1.body1.sec4.p2" name="article1.body1.sec4.p2"></a><p>Furthermore, we found that training with an incongruent audiovisual stimulus failed to produce a similar facilitating effect during training. If the facilitation of learning was a result of enhanced general alerting caused by sound, then an enhancement of learning should have occurred for both the congruent and incongruent groups. The failure of the incongruent training paradigm to produce enhanced learning also argues against the explanation that the audiovisual condition produces better learning by being more demanding, and therefore more attentionally engaging than the visual alone condition, since both congruent and incongruent training contain audiovisual stimuli. If anything, incongruent training could be considered more difficult than congruent training, leading to the expectation that incongruent training would engage even more attention and produce better performance than congruent training. In actuality, since the task is one of motion detection and not of direction discrimination, the congruence of the audio and visual stimuli should not matter, as the subject need only detect any motion, regardless of direction, to choose the correct interval.</p>
<a id="article1.body1.sec4.p3" name="article1.body1.sec4.p3"></a><p>Another possibility is that the incongruent auditory stimuli did produce some level of alerting, but that these were countered by inhibitory interactions at the sensory level. For instance, inhibitory interactions at the sensory level are well established between opposite directions of visual motion <a href="#pone.0001532-Heeger1">[20]</a>–<a href="#pone.0001532-Levinson1">[22]</a>. Given that recent neuroimaging studies suggest brain areas specialized to process motion (such as MT) may process multisensory information <a href="#pone.0001532-Moore1">[23]</a>, it seems likely that inhibitory interactions may also occur between crossmodal directional stimuli.</p>
<a id="article1.body1.sec4.p4" name="article1.body1.sec4.p4"></a><p>Performance on audiovisual trials differed between congruent and incongruent groups as well. As expected, congruent audiovisual trials yielded better detection than visual-only trials, but surprisingly, incongruent trials did not (see <a href="#pone-0001532-g002">Figure 2</a>), even though sound was equally informative for the task in both conditions. The fact that incongruent sound did not benefit performance on audiovisual trials, whereas congruent sound did, supports the conclusion that the auditory stimuli are not affecting performance at a decisional or cognitive level, but rather at a perceptual level. This differential performance on audiovisual trials may be responsible for the difference in visual learning between the congruent and incongruent trained groups. It is possible that the improved perception (as demonstrated by the enhanced performance) is the underlying factor for the benefit of multisensory training protocol. This would be consistent with the proposal that for learning to take place it requires that neuronal activity exceeds a threshold <a href="#pone.0001532-Seitz3">[24]</a>. The multisensory stimulation causes such enhancement of neural activity (and hence the better performance) and thus facilitates learning when the sensory inputs are congruent, but not when they are incongruent.</p>
<a id="article1.body1.sec4.p5" name="article1.body1.sec4.p5"></a><p>Interestingly, while the congruent trained group initially performs worse on visual-only trials than audiovisual trials, this difference disappears with training and performance on the two trial-types becomes essentially equivalent. This convergence between visual-only and audiovisual performance suggests that crossmodal interactions are responsible for the facilitation of learning; i.e., that training with congruent audiovisual trials enhances visual processing such that the enhancements persist even when the auditory information is no longer present.</p>
<a id="article1.body1.sec4.p6" name="article1.body1.sec4.p6"></a><p>This study provides further evidence that multisensory training can affect visual learning at the perceptual level. While these results cannot speak to the exact mechanism by which multisensory training mediates visual learning, anatomical studies have found evidence for input to lower level visual areas both directly from primary and parabelt auditory cortex <a href="#pone.0001532-Falchier1">[25]</a>, <a href="#pone.0001532-Rockland1">[26]</a>, and also from the superior temporal polysensory area <a href="#pone.0001532-Falchier1">[25]</a>. Thus, the congruent sound may facilitate visual learning through direct connections to visual areas, indirectly through multisensory areas, or through some combination of both. Much remains to be learned about the mechanisms of multisensory learning, but these results are a first step in parsing out how to capitalize on the dynamics of multisensory interactions to optimize learning paradigms.</p>
</div>





<div><a id="ack" name="ack" toc="ack" title="Acknowledgments"></a><h3>Acknowledgments</h3>
<a id="article1.back1.ack1.p1" name="article1.back1.ack1.p1"></a><p>We would like to thank James Thomas for his helpful comments on this manuscript, and his contribution of subject payment funds for this project.</p>
</div><div class="contributions"><a id="authcontrib" name="authcontrib" toc="authcontrib" title="Author Contributions"></a><h3>Author Contributions</h3><p>Conceived and designed the experiments: AS LS. Performed the experiments: RK. Analyzed the data: AS RK. Wrote the paper: AS RK. Other: Contributed to design of experiments: RK. Contributed to data analysis: LS. Revised the paper: LS. Funded the project: LS.</p></div><div><a id="references" name="references" toc="references" title="References"></a><h3>References</h3><ol class="references"><li><span class="label">1.
              </span><a name="pone.0001532-Li1" id="pone.0001532-Li1"></a>Li W, Piech V, Gilbert CD (2004) Perceptual learning and top-down influences in primary visual cortex. Nat Neurosci  7: 651–657.  <ul class="find" data-citedArticleID="1049508" data-doi="10.1038/nn1255"><li><a href="http://dx.doi.org/10.1038/nn1255" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Perceptual+learning+and+top-down+influences+in+primary+visual+cortex." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Perceptual+learning+and+top-down+influences+in+primary+visual+cortex.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">2.
              </span><a name="pone.0001532-Schoups1" id="pone.0001532-Schoups1"></a>Schoups A, Vogels R, Qian N, Orban G (2001) Practising orientation identification improves orientation coding in V1 neurons. Nature  412: 549–53.  <ul class="find" data-citedArticleID="1049520" data-doi="10.1038/35087601"><li><a href="http://dx.doi.org/10.1038/35087601" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Practising+orientation+identification+improves+orientation+coding+in+V1+neurons." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Practising+orientation+identification+improves+orientation+coding+in+V1+neurons.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">3.
              </span><a name="pone.0001532-Recanzone1" id="pone.0001532-Recanzone1"></a>Recanzone GH, Schreiner CE, Merzenich MM (1993) Plasticity in the frequency representation of primary auditory cortex following discrimination training in adult owl monkeys. J Neurosci  13: 87–103.  <ul class="find" data-citedArticleID="1049516"><li><a href="http://www.crossref.org/guestquery/?auth2=&amp;atitle2=Plasticity+in+the+frequency+representation+of+primary+auditory+cortex+following+discrimination+training+in+adult+owl+monkeys.&amp;auth=&amp;atitle=Plasticity+in+the+frequency+representation+of+primary+auditory+cortex+following+discrimination+training+in+adult+owl+monkeys." target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Plasticity+in+the+frequency+representation+of+primary+auditory+cortex+following+discrimination+training+in+adult+owl+monkeys." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Plasticity+in+the+frequency+representation+of+primary+auditory+cortex+following+discrimination+training+in+adult+owl+monkeys.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">4.
              </span><a name="pone.0001532-Dinse1" id="pone.0001532-Dinse1"></a>Dinse HR, Ragert P, Pleger B, Schwenkreis P, Tegenthoff M (2003) Pharmacological modulation of perceptual learning and associated cortical reorganization. Science  301: 91–94.  <ul class="find" data-citedArticleID="1049496" data-doi="10.1126/science.1085423"><li><a href="http://dx.doi.org/10.1126/science.1085423" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Pharmacological+modulation+of+perceptual+learning+and+associated+cortical+reorganization." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Pharmacological+modulation+of+perceptual+learning+and+associated+cortical+reorganization.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">5.
              </span><a name="pone.0001532-Amedi1" id="pone.0001532-Amedi1"></a>Amedi A, Malach , Hendler T, Peled S, Zohary E (2001) Visuo-haptic object-related activation in the ventral visual pathway. Nature Neuroscience  4: 324–330.  <ul class="find" data-citedArticleID="1049484" data-doi="10.1038/85201"><li><a href="http://dx.doi.org/10.1038/85201" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Visuo-haptic+object-related+activation+in+the+ventral+visual+pathway." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Visuo-haptic+object-related+activation+in+the+ventral+visual+pathway.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">6.
              </span><a name="pone.0001532-Calvert1" id="pone.0001532-Calvert1"></a>Calvert GA, Bullmore ET, Brammer MJ, Campbell R, Williams SC, et al.  (1997) Activation of auditory cortex during silent lipreading. Science  276: 593–596.  <ul class="find" data-citedArticleID="1049492" data-doi="10.1126/science.276.5312.593"><li><a href="http://dx.doi.org/10.1126/science.276.5312.593" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Activation+of+auditory+cortex+during+silent+lipreading." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Activation+of+auditory+cortex+during+silent+lipreading.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">7.
              </span><a name="pone.0001532-Foxe1" id="pone.0001532-Foxe1"></a>Foxe JJ, Morocz IA, Murray MM, Higgins BA, Javitt DC, et al.  (2000) Multisensory auditory-somatosensory interactions in early cortical processing revealed by high-density electrical mapping. Cognitive Brain Research  10: 77–83.  <ul class="find" data-citedArticleID="1049500" data-doi="10.1016/s0926-6410(00)00024-0"><li><a href="http://dx.doi.org/10.1016/s0926-6410(00)00024-0" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Multisensory+auditory-somatosensory+interactions+in+early+cortical+processing+revealed+by+high-density+electrical+mapping." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Multisensory+auditory-somatosensory+interactions+in+early+cortical+processing+revealed+by+high-density+electrical+mapping.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">8.
              </span><a name="pone.0001532-Shams1" id="pone.0001532-Shams1"></a>Shams L, Iwaki S, Chawla A, Bhattacharya J (2005) Early Modulation of visual cortex by sound: An MEG study. Neuroscience Letters  378: 76–81.  <ul class="find" data-citedArticleID="1049528" data-doi="10.1016/j.neulet.2004.12.035"><li><a href="http://dx.doi.org/10.1016/j.neulet.2004.12.035" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Early+Modulation+of+visual+cortex+by+sound%3A+An+MEG+study." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Early+Modulation+of+visual+cortex+by+sound%3A+An+MEG+study.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">9.
              </span><a name="pone.0001532-Watkins1" id="pone.0001532-Watkins1"></a>Watkins S, Shams L, Tanaka S, Haynes JD, Rees G (2006) Sound alters activity in human V1 in association with illusory visual perception. Neuroimage  31: 1247–1256.  <ul class="find" data-citedArticleID="1049532" data-doi="10.1016/j.neuroimage.2006.01.016"><li><a href="http://dx.doi.org/10.1016/j.neuroimage.2006.01.016" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Sound+alters+activity+in+human+V1+in+association+with+illusory+visual+perception." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Sound+alters+activity+in+human+V1+in+association+with+illusory+visual+perception.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">10.
              </span><a name="pone.0001532-Zangaladze1" id="pone.0001532-Zangaladze1"></a>Zangaladze A, Epstein CM, Grafton ST, Sathian K (1999) Involvement of visual cortex in tactile discrimination of orientation. Nature  401: 587–590.  <ul class="find" data-citedArticleID="1049534" data-doi="10.1038/44139"><li><a href="http://dx.doi.org/10.1038/44139" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Involvement+of+visual+cortex+in+tactile+discrimination+of+orientation." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Involvement+of+visual+cortex+in+tactile+discrimination+of+orientation.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">11.
              </span><a name="pone.0001532-Ghazanfar1" id="pone.0001532-Ghazanfar1"></a>Ghazanfar AA, Schroeder CE (2006) Is neocortex essentially multisensory? Trends in Cognitive Science  10: 278–85.  <ul class="find" data-citedArticleID="1049502" data-doi="10.1016/j.tics.2006.04.008"><li><a href="http://dx.doi.org/10.1016/j.tics.2006.04.008" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Is+neocortex+essentially+multisensory%3F" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Is+neocortex+essentially+multisensory%3F%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">12.
              </span><a name="pone.0001532-Bahrick1" id="pone.0001532-Bahrick1"></a>Bahrick LE, Flom R, Lickliter R (2002) Intersensory redundancy facilitates discrimination of tempo in 3-month-old infants. Developmental Psychobiology  41: 352–63.  <ul class="find" data-citedArticleID="1049486" data-doi="10.1002/dev.10049"><li><a href="http://dx.doi.org/10.1002/dev.10049" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Intersensory+redundancy+facilitates+discrimination+of+tempo+in+3-month-old+infants." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Intersensory+redundancy+facilitates+discrimination+of+tempo+in+3-month-old+infants.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">13.
              </span><a name="pone.0001532-Carlsen1" id="pone.0001532-Carlsen1"></a>Carlsen R, Lickliter R (1999) Augmented prenatal tactile and vestibular stimulation alters postnatal auditory and visual responsiveness in bobwhite quail chicks. Dev Psychobiol  35: 215–225.  <ul class="find" data-citedArticleID="1049494" data-doi="10.1002/(sici)1098-2302(199911)35:3<215::aid-dev6&gt;3.3.co;2-f"><li><a href="http://dx.doi.org/10.1002/(sici)1098-2302(199911)35:3<215::aid-dev6&gt;3.3.co;2-f" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Augmented+prenatal+tactile+and+vestibular+stimulation+alters+postnatal+auditory+and+visual+responsiveness+in+bobwhite+quail+chicks." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Augmented+prenatal+tactile+and+vestibular+stimulation+alters+postnatal+auditory+and+visual+responsiveness+in+bobwhite+quail+chicks.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">14.
              </span><a name="pone.0001532-Shimojo1" id="pone.0001532-Shimojo1"></a>Shimojo S, Shams L (2001) Sensory modalities are not separate modalities: plasticity and interactions. Current Opinion in Neurobiology  11: 505–509.  <ul class="find" data-citedArticleID="1049530" data-doi="10.1016/s0959-4388(00)00241-5"><li><a href="http://dx.doi.org/10.1016/s0959-4388(00)00241-5" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Sensory+modalities+are+not+separate+modalities%3A+plasticity+and+interactions." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Sensory+modalities+are+not+separate+modalities%3A+plasticity+and+interactions.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">15.
              </span><a name="pone.0001532-Seitz1" id="pone.0001532-Seitz1"></a>Seitz A, Kim R, Shams L (2006) Sound Facilitates Visual Learning. Curr Biol  16: 1422–1427.  <ul class="find" data-citedArticleID="1049522" data-doi="10.1016/j.cub.2006.05.048"><li><a href="http://dx.doi.org/10.1016/j.cub.2006.05.048" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Sound+Facilitates+Visual+Learning." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Sound+Facilitates+Visual+Learning.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">16.
              </span><a name="pone.0001532-Seitz2" id="pone.0001532-Seitz2"></a>Seitz AR, Yamagishi N, Werner B, Goda N, Kawato M, et al.  (2005) Task-specific disruption of perceptual learning. Proc Natl Acad Sci U S A  102: 14895–14900.  <ul class="find" data-citedArticleID="1049524" data-doi="10.1073/pnas.0505765102"><li><a href="http://dx.doi.org/10.1073/pnas.0505765102" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Task-specific+disruption+of+perceptual+learning." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Task-specific+disruption+of+perceptual+learning.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">17.
              </span><a name="pone.0001532-Britten1" id="pone.0001532-Britten1"></a>Britten KH, Shadlen MN, Newsome WT, Movshon JA (1992) The analysis of visual motion: a comparison of neuronal and psychophysical performance. J Neurosci  12: 4745–4765.  <ul class="find" data-citedArticleID="1049490"><li><a href="http://www.crossref.org/guestquery/?auth2=&amp;atitle2=The+analysis+of+visual+motion%3A+a+comparison+of+neuronal+and+psychophysical+performance.&amp;auth=&amp;atitle=The+analysis+of+visual+motion%3A+a+comparison+of+neuronal+and+psychophysical+performance." target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=The+analysis+of+visual+motion%3A+a+comparison+of+neuronal+and+psychophysical+performance." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22The+analysis+of+visual+motion%3A+a+comparison+of+neuronal+and+psychophysical+performance.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">18.
              </span><a name="pone.0001532-Ball1" id="pone.0001532-Ball1"></a>Ball K, Sekuler R (1981) Adaptive processing of visual motion. J Exp Psychol Hum Percept Perform  7: 780–794.  <ul class="find" data-citedArticleID="1049488"><li><a href="http://www.crossref.org/guestquery/?auth2=&amp;atitle2=Adaptive+processing+of+visual+motion.&amp;auth=&amp;atitle=Adaptive+processing+of+visual+motion." target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Adaptive+processing+of+visual+motion." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Adaptive+processing+of+visual+motion.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">19.
              </span><a name="pone.0001532-Loftus1" id="pone.0001532-Loftus1"></a>Loftus GR, Masson ME (1994) Using confidence intervals in within-subject designs. Psychonomic Bulletin &amp; Review  1: 476–490.  <ul class="find" data-citedArticleID="1049510" data-doi="10.3758/bf03210951"><li><a href="http://dx.doi.org/10.3758/bf03210951" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Using+confidence+intervals+in+within-subject+designs." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Using+confidence+intervals+in+within-subject+designs.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">20.
              </span><a name="pone.0001532-Heeger1" id="pone.0001532-Heeger1"></a>Heeger DJ, Boynton GM, Demb JB, Seidemann E, Newsome WT (1999) Motion opponency in visual cortex. Journal of Neuroscience  19: 7162–74.  <ul class="find" data-citedArticleID="1049504"><li><a href="http://www.crossref.org/guestquery/?auth2=&amp;atitle2=Motion+opponency+in+visual+cortex.&amp;auth=&amp;atitle=Motion+opponency+in+visual+cortex." target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Motion+opponency+in+visual+cortex." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Motion+opponency+in+visual+cortex.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">21.
              </span><a name="pone.0001532-Qian1" id="pone.0001532-Qian1"></a>Qian N, Andersen RA, Adelson EH (1994) Transparent motion perception as detection of unbalanced motion signals. I. Psychophysics. Journal of Neuroscience  14: 7357–66.  <ul class="find" data-citedArticleID="1049514"><li><a href="http://www.crossref.org/guestquery/?auth2=&amp;atitle2=Transparent+motion+perception+as+detection+of+unbalanced+motion+signals.+I.+Psychophysics.&amp;auth=&amp;atitle=Transparent+motion+perception+as+detection+of+unbalanced+motion+signals.+I.+Psychophysics." target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Transparent+motion+perception+as+detection+of+unbalanced+motion+signals.+I.+Psychophysics." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Transparent+motion+perception+as+detection+of+unbalanced+motion+signals.+I.+Psychophysics.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">22.
              </span><a name="pone.0001532-Levinson1" id="pone.0001532-Levinson1"></a>Levinson E, Sekuler R (1975) Inhibition and disinhibition of direction-specific mechanisms in human vision. Nature  254: 692–4.  <ul class="find" data-citedArticleID="1049506" data-doi="10.1038/254692a0"><li><a href="http://dx.doi.org/10.1038/254692a0" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Inhibition+and+disinhibition+of+direction-specific+mechanisms+in+human+vision." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Inhibition+and+disinhibition+of+direction-specific+mechanisms+in+human+vision.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">23.
              </span><a name="pone.0001532-Moore1" id="pone.0001532-Moore1"></a>Moore C, Nelson A (2006) Is MT a multi-modal information processing region?   Paper presented at the International Multisensory Research Forum, Dublin, Ireland June 18–21.  <ul class="find-nolinks"></ul></li><li><span class="label">24.
              </span><a name="pone.0001532-Seitz3" id="pone.0001532-Seitz3"></a>Seitz A, Dinse H (2007) A Common Framework for Perceptual Learning. Current Opinion of Neurobiology  17(2): 148–153.  <ul class="find" data-citedArticleID="1049526" data-doi="10.1016/j.conb.2007.02.004"><li><a href="http://dx.doi.org/10.1016/j.conb.2007.02.004" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=A+Common+Framework+for+Perceptual+Learning." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22A+Common+Framework+for+Perceptual+Learning.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">25.
              </span><a name="pone.0001532-Falchier1" id="pone.0001532-Falchier1"></a>Falchier A, Clavagnier S, Barone P, Kennedy H (2002) Anatomical evidence of multimodal integration in primate striate cortex. Journal of Neuroscience  22: 5749–5759.  <ul class="find" data-citedArticleID="1049498"><li><a href="http://www.crossref.org/guestquery/?auth2=&amp;atitle2=Anatomical+evidence+of+multimodal+integration+in+primate+striate+cortex.&amp;auth=&amp;atitle=Anatomical+evidence+of+multimodal+integration+in+primate+striate+cortex." target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Anatomical+evidence+of+multimodal+integration+in+primate+striate+cortex." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Anatomical+evidence+of+multimodal+integration+in+primate+striate+cortex.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">26.
              </span><a name="pone.0001532-Rockland1" id="pone.0001532-Rockland1"></a>Rockland KS, Ojima H (2003) Multisensory convergence in calcarine visual areas in macaque monkey. International Journal of Psychophysiology  50: 19–26.  <ul class="find" data-citedArticleID="1049518" data-doi="10.1016/s0167-8760(03)00121-1"><li><a href="http://dx.doi.org/10.1016/s0167-8760(03)00121-1" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Multisensory+convergence+in+calcarine+visual+areas+in+macaque+monkey." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Multisensory+convergence+in+calcarine+visual+areas+in+macaque+monkey.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li></ol></div>

  </div>

      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001532.XML" value="50145"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001532.PDF" value="350509"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001532.g001.PNG_L" value="623294"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001532.g001.PNG_M" value="109548"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001532.g001.PNG_S" value="10394"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001532.g001.TIF" value="1072720"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001532.g001.PNG_I" value="36793"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001532.g002.PNG_L" value="77810"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001532.g002.PNG_M" value="69924"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001532.g002.PNG_S" value="5712"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001532.g002.TIF" value="150848"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001532.g002.PNG_I" value="27069"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001532.g003.PNG_L" value="116821"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001532.g003.PNG_M" value="45798"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001532.g003.PNG_S" value="6426"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001532.g003.TIF" value="200792"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001532.g003.PNG_I" value="16157"/>

</div>
<div class="sidebar">

  <div class="article-actions cf">
      <div class="download">
        <span class="btn"><a href="/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0001532&amp;representation=PDF" title="Download" target="_blank">Download PDF</a></span>
      </div>
      <div class="btn-reveal dropdown">
        <div class="dropdown-icon">
          <span class="btn">&nbsp;</span>
        </div>

        <div class="content">
          <ul class="bullet">
            <li><a href="/article/citationList.action?articleURI=info%3Adoi%2F10.1371%2Fjournal.pone.0001532" title="Download citations">Citation</a></li>
            <li><a href="/article/fetchObjectAttachment.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0001532&amp;representation=XML" title="Download article XML">XML</a></li>
          </ul>
        </div>
      </div> <!-- end btn-reveal dropdown-->


    <div class="btn-reveal flt-l">
        <span class="btn">Print</span>
        <div class="content">
            <ul class="bullet">
                <li id="print-article"><a href="#" onclick="if(typeof(_gaq) != 'undefined'){ _gaq.push(['_trackEvent','Article', 'Print', 'Click']); } window.print(); return false;" title="Print Article">Print article</a></li>
                <li>
                  <a href="https://www.odysseypress.com/onlinehost/reprint_order.php?type=A&page=0&journal=7&doi=10.1371/journal.pone.0001532&volume=&issue=&title=Benefits of Stimulus Congruency for Multisensory Facilitation of Visual Learning&author_name=Robyn%20S.%20Kim%2C%20Aaron%20R.%20Seitz%2C%20Ladan%20Shams&start_page=1&end_page=5" title="Odyssey Press">EzReprint</a>
                </li>
            </ul>
        </div>
    </div>

    <div class="btn-reveal flt-r">
        <span class="btn">Share</span>
        <div class="content">
            <ul class="social">
                <li><a href="http://www.reddit.com/submit?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0001532" target="_blank" title="Submit to Reddit"><img src="/images/icon.reddit.16.png" width="16" height="16" alt="Reddit">Reddit</a></li>

                <li><a href="https://plus.google.com/share?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0001532" target="_blank" title="Share on Google+"><img src="/images/icon.gplus.16.png" width="16" height="16" alt="Google+">Google+</a></li>

                <li><a href="http://www.stumbleupon.com/submit?url=http%3A%2F%2Fwww.plosone.org%2Farticle%2Finfo%253Adoi%252F10.1371%252Fjournal.pone.0001532" target="_blank" title="Add to StumbleUpon"><img src="/images/icon.stumble.16.png" width="16" height="16" alt="StumbleUpon">StumbleUpon</a></li>

                <li><a href="http://www.facebook.com/share.php?u=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0001532&amp;t=Benefits%20of%20Stimulus%20Congruency%20for%20Multisensory%20Facilitation%20of%20Visual%20Learning" target="_blank" title="Share on Facebook"><img src="/images/icon.fb.16.png" width="16" height="16" alt="Facebook">Facebook</a></li>

                <li><a href="http://www.linkedin.com/shareArticle?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0001532&title=Benefits%20of%20Stimulus%20Congruency%20for%20Multisensory%20Facilitation%20of%20Visual%20Learning&summary=Checkout%20this%20article%20I%20found%20at%20PLOS" target="_blank" title="Add to LinkedIn"><img src="/images/icon.linkedin.16.png" width="16" height="16" alt="Mendeley">LinkedIn</a></li>

                <li><a href="http://www.citeulike.org/posturl?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0001532&amp;title=Benefits%20of%20Stimulus%20Congruency%20for%20Multisensory%20Facilitation%20of%20Visual%20Learning" target="_blank" title="Add to CiteULike"><img src="/images/icon.cul.16.png" width="16" height="16" alt="CiteULike">CiteULike</a></li>

                <li><a href="http://www.mendeley.com/import/?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0001532" target="_blank" title="Add to Mendeley"><img src="/images/icon.mendeley.16.png" width="16" height="16" alt="Mendeley">Mendeley</a></li>

                <li><a href="https://www.pubchase.com/library?add_aid=10.1371%2Fjournal.pone.0001532&amp;source=plos" target="_blank" title="Add to PubChase"><img src="/images/icon.pc.16.png" width="16" height="16" alt="PubChase">PubChase</a></li>


                <script type="text/javascript">
                    // replace tweet with one that's pre-shortened to 140 chars
                    function truncateTweetText() {
                        var twtTitle = 'Benefits of Stimulus Congruency for Multisensory Facilitation of Visual Learning';
                        var twtUrl = 'http://dx.plos.org/10.1371/journal.pone.0001532';
                        // all URLs posted to twitter get auto-shortened to 20 chars.
                        var maxLength = 140 - (20 + 1);
                        // truncate the title to include space for twtTag and ellipsis (here, 10 = tag length + space + ellipsis)
                        if (twtTitle.length > maxLength) { twtTitle = twtTitle.substr(0, (maxLength - 10)) + '...'; }
                        // set the href to use the shortened tweet
                        $('#twitter-share-link').prop('href', 'http://twitter.com/intent/tweet?text=' + encodeURIComponent('#PLOSONE: ' + twtTitle + ' ' + twtUrl));
                    }
                </script>
                <li><a href="http://twitter.com/intent/tweet?text=#PLOSONE%3A%20Benefits%20of%20Stimulus%20Congruency%20for%20Multisensory%20Facilitation%20of%20Visual%20Learning http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0001532" onclick="truncateTweetText();" target="_blank" title="Share on Twitter" id="twitter-share-link"><img src="/images/icon.twtr.16.png" width="16" height="16" alt="Twitter">Twitter</a></li>

                <li><a href="/article/email/info%3Adoi%2F10.1371%2Fjournal.pone.0001532" title="Email this article"><img src="/images/icon.email.16.png" width="16" height="16" alt="Email">Email</a></li>
            </ul>
        </div>
    </div><!--end btn-reveal flt-r-->
</div><!-- end article-actions-->

<!-- begin Crossmark -->

<a id="open-crossmark" href="#" style="margin-top: -28px; display:block"><img style="border: 0; display: none;
 padding: 10px 0 18px 0;"  id="crossmark-icon" src="/images/logo-crossmark-bw.png" /></a>
<div id="crossmark-dialog" style="display: none;" title="">
    <!-- the external CrossMark data is loaded inside this iframe -->
    <iframe id="crossmark-dialog-frame" frameborder="0"></iframe>
</div>

<!-- end crossmark -->


<div class="block" id="subject-area-sidebar-block">
    <div class="header">
        <h3>Subject Areas</h3><div title="More information" id="subject-area-sidebar-block-help-icon"><img align="right"
                                                                                                           alt="info" src="/images/button_info.png"/><div id="subject-area-sidebar-block-help"><img align="right"
                                                                                                                                                                                                    src="/images/button_info.png"/><p>
        <b>We want your feedback.</b> Do these subject areas make sense for this article? If not, click the flag
        next to the incorrect subject area and we will review it. Thanks for your help!
    </p></div></div>
    </div>


    <ul id="subject-area-sidebar-list">



















          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Acoustic+signals%22" title="Search for articles in the subject area:'Acoustic signals'"><div class="flagText">Acoustic signals</div></a>
              <div data-categoryid="20989" data-articleid="25890"
                   data-categoryname="Acoustic signals"
                   class="flagImage" title="Flag 'Acoustic signals' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Cognition%22" title="Search for articles in the subject area:'Cognition'"><div class="flagText">Cognition</div></a>
              <div data-categoryid="17733" data-articleid="25890"
                   data-categoryname="Cognition"
                   class="flagImage" title="Flag 'Cognition' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Cognitive+science%22" title="Search for articles in the subject area:'Cognitive science'"><div class="flagText">Cognitive science</div></a>
              <div data-categoryid="17271" data-articleid="25890"
                   data-categoryname="Cognitive science"
                   class="flagImage" title="Flag 'Cognitive science' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Human+learning%22" title="Search for articles in the subject area:'Human learning'"><div class="flagText">Human learning</div></a>
              <div data-categoryid="34727" data-articleid="25890"
                   data-categoryname="Human learning"
                   class="flagImage" title="Flag 'Human learning' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Human+performance%22" title="Search for articles in the subject area:'Human performance'"><div class="flagText">Human performance</div></a>
              <div data-categoryid="18145" data-articleid="25890"
                   data-categoryname="Human performance"
                   class="flagImage" title="Flag 'Human performance' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Learning%22" title="Search for articles in the subject area:'Learning'"><div class="flagText">Learning</div></a>
              <div data-categoryid="20059" data-articleid="25890"
                   data-categoryname="Learning"
                   class="flagImage" title="Flag 'Learning' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Sensory+perception%22" title="Search for articles in the subject area:'Sensory perception'"><div class="flagText">Sensory perception</div></a>
              <div data-categoryid="46099" data-articleid="25890"
                   data-categoryname="Sensory perception"
                   class="flagImage" title="Flag 'Sensory perception' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Vision%22" title="Search for articles in the subject area:'Vision'"><div class="flagText">Vision</div></a>
              <div data-categoryid="32965" data-articleid="25890"
                   data-categoryname="Vision"
                   class="flagImage" title="Flag 'Vision' as inappropriate"></div>
          </li>
    </ul>
</div>

<div class="ad">
    <div class="title">Advertisement</div>






  <iframe id='a0852f54' name='a0852f54'
    src='http://ads.plos.org/www/delivery/afr.php?zoneid=381&amp;cb=1247'
    frameborder='0' scrolling='no' width='160' height='600'>
    <a href='http://ads.plos.org/www/delivery/ck.php?n=a0852f54&amp;cb=2071'
      target='_top'><img src='http://ads.plos.org/www/delivery/avw.php?zoneid=381&amp;cb=2721&amp;n=a0852f54'
      border='0' alt=''/>
    </a>
  </iframe>



</div>

<div id="twitter-alm-timeline" class="twitter-alm-timeline"></div>


</div><!-- sidebar -->
    </div>
  </div>
</div>
<script src="http://wl.figshare.com/static/p_widget.js" type="text/javascript"></script><div id="pageftr">
  <div class="ftr-cols cf">
    <div class="col col-1">
      <img src="/images/logo-plos-footer.png" alt="PLOS Logo" class="logo" />
      <p><a href="/static/releaseNotes">Ambra 2.9.16</a> Managed Colocation provided <br />by <a href="http://www.isc.org/">Internet Systems Consortium</a>.<p>
      <div class="nav nav-aux">
        <a href="/static/privacy">Privacy Policy</a> |
        <a href="/static/terms">Terms of Use</a> |
        <a href="http://www.plos.org/advertise/">Advertise</a> |
        <a href="http://www.plos.org/about/media-inquiries/">Media Inquiries</a>
      </div>
    </div>
    <div class="col col-2">
      <p><a href="http://www.plos.org/publications/journals/">Publications</a></p>
      <div class="nav">
        <ul>
          <li><a href="http://www.plosbiology.org">PLOS Biology</a></li>
          <li><a href="http://www.plosmedicine.org">PLOS Medicine</a></li>
          <li><a href="http://www.ploscompbiol.org">PLOS Computational Biology</a></li>
          <li><a href="http://currents.plos.org">PLOS Currents</a></li>
          <li><a href="http://www.plosgenetics.org">PLOS Genetics</a></li>
          <li><a href="http://www.plospathogens.org">PLOS Pathogens</a></li>
          <li><a href="http://www.plosone.org">PLOS ONE</a></li>
          <li><a href="http://www.plosntds.org">PLOS Neglected Tropical Diseases</a></li>
        </ul>
      </div>
    </div>
    <div class="col col-3">
      <div class="nav">
        <p><a href="http://www.plos.org">plos.org</a></p>
        <p><a href="http://blogs.plos.org">Blogs</a></p>
        <p><a href="http://www.ploscollections.org">Collections</a></p>
        <p><a href="/feedback/new">Send us feedback</a></p>

        <p>California (US) corporation #C2354500, based in San Francisco</p>
      </div>
    </div>
  </div>
</div><!-- pageftr -->

</div><!-- end page-wrap, this div is in header.ftl -->
<script type="text/javascript" src="/javascript/jquery-1.8.1-min.js?v=Tm7VCOzZz3lE03ghpkS6SWkHbyI"></script>
<script type="text/javascript" src="/javascript/ga-min.js?v=lNQ4gt8QcPDatjsdOFl_FGpPhLY"></script>
<script type="text/javascript" src="/javascript/jquery.hoverIntent-min.js?v=mRiGNYY9cIXxVb8u0K_MdW7hHnc"></script>
<script type="text/javascript" src="/javascript/jquery.placeholder-min.js?v=21Pn56Ur9h1N4K4VZDa0nqI3Pxo"></script>
<script type="text/javascript" src="/javascript/jquery.jsonp-2.4.0-min.js?v=lqTpzoHfSq3I5Ygo01qq5WankEo"></script>
<script type="text/javascript" src="/javascript/jquery-ui-1.9.2.custom-min.js?v=raSSlfNO0YsV5uUpAKmTB9n5VTc"></script>
<script type="text/javascript" src="/javascript/jquery.tooltip-min.js?v=cw+6Smh+mdryIA25xvqIvHMrnZM"></script>
<script type="text/javascript" src="/javascript/jquery.uniform-min.js?v=kYUAnX6W2W_2fK3RIuQ2m_YFG9U"></script>
<script type="text/javascript" src="/javascript/jquery.pjax-min.js?v=939kLBjL5_YKbx71T1RHjYaD4l8"></script>
<script type="text/javascript" src="/javascript/imagesloaded-min.js?v=XeuAp8Gc3mvQUo+wZCSF8ttPwvw"></script>
<script type="text/javascript" src="/javascript/figviewer-min.js?v=yPUa0sUQ_iHkI+IRv2i9bjyZJFo"></script>
<script type="text/javascript" src="/javascript/global-min.js?v=0Q3PwjeaWtXYDnqIsQvnL_ou0qs"></script>
<script type="text/javascript" src="/javascript/jquery.touchswipe-min.js?v=huaek_e6HqTduvCNAN91dJolTyw"></script>
<script type="text/javascript" src="/javascript/jquery.base64-min.js?v=VwV1zeVqKZj5FCAdlK0q5NRxbBg"></script>
<script type="text/javascript" src="/javascript/alm-min.js?v=Y5gm6B0b4Kx2YHNObNrgEeBgXlY"></script>
<script type="text/javascript" src="/javascript/taxonomy-browser-min.js?v=vBVMuDMYkGJCXIUxLe35GoyiJNw"></script>
<script type="text/javascript" src="/javascript/jquery.filterize-min.js?v=j0ZKVnHyk2nhFy8eIuNJkp7xaM0"></script>
<script type="text/javascript" src="/javascript/plosone-min.js?v=TK4H4arL_XBSwwJq+K1N3kqYfAI"></script>
<script type="text/javascript" src="/javascript/twitter-min.js?v=xKgcxLsQFXy+at1ao1NVke8nFlM"></script>
<script type="text/javascript" src="/javascript/crossmark.1.4-min.js?v=3FO4k0SjwTaGNnKGNSqthar1080"></script>
<script type="text/javascript">
  var _sf_async_config={uid:16579,domain:"plosone.org"};
  (function(){
    function loadChartbeat() {
      window._sf_endpt=(new Date()).getTime();
      var e = document.createElement('script');
      e.setAttribute('language', 'javascript');
      e.setAttribute('type', 'text/javascript');
      e.setAttribute('src',
          (("https:" == document.location.protocol) ? "https://a248.e.akamai.net/chartbeat.download.akamai.com/102508/" : "http://static.chartbeat.com/") +
              "js/chartbeat.js");
      document.body.appendChild(e);
    }
    var oldonload = window.onload;
    window.onload = (typeof window.onload != 'function') ?
        loadChartbeat : function() { oldonload(); loadChartbeat(); };
  })();
</script>
<!-- <script type="application/javascript" src="http://crossmark.crossref.org/javascripts/v1.3/crossmark.min.js"></script> -->

</body>
</html>
