

 



<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"
      xmlns:foaf="http://xmlns.com/foaf/0.1/"
      xmlns:dc="http://purl.org/dc/terms/"
      xmlns:doi="http://dx.doi.org/"
      xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
      xmlns:xsd="http://www.w3.org/2001/XMLSchema-datatypes#"
      lang="en" xml:lang="en"
      itemscope itemtype="http://schema.org/Article"
      class="no-js">
<head prefix="og: http://ogp.me/ns#">
  <title>PLOS ONE: Are There Multiple Visual Short-Term Memory Stores?</title>


<link rel="stylesheet" type="text/css"  href="/css/global-min.css?v=izteQ6tu7kgsJZW_xmrYizvKiHM" />


    <!--[if lte IE 7]>
<link rel="stylesheet" type="text/css"  href="/css/lte_ie7-min.css?v=3bykQUyQmReeuobVyPozcJ9LxRc" />
    <![endif]-->


<link rel="stylesheet" type="text/css"  href="/css/jquery-ui-min.css?v=eXDHTEJM0lIAmDe5k0I0Ad4nxNo" />


<link rel="stylesheet" type="text/css"  href="/css/journal.css?v=T7ZVxJfgk9jNxLAJ2qHz1vZpgYU" />


<link rel="stylesheet" type="text/css" media="print" href="/css/print-min.css?v=T5lb0B3q6EXBsuDluc5V5w+AkRc" />


  <link rel="stylesheet" href="http://f.fontdeck.com/s/css/js/www.plosone.org/24557.css" type="text/css"/>

  <!--chartbeat -->
  <script type="text/javascript">var _sf_startpt = (new Date()).getTime()</script>
  <script>document.documentElement.className += ' js';</script>

  <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <meta http-equiv="X-UA-Compatible" content="IE=EmulateIE7; IE=EmulateIE9"/>
  <meta name="description" content="PLOS ONE: an inclusive, peer-reviewed, open-access resource from the PUBLIC LIBRARY OF SCIENCE. Reports of well-performed scientific studies from all disciplines freely available to the whole world."/>
  <meta name="keywords" content="PLOS, Public Library of Science, Open Access, Open-Access, Science, Medicine, Biology, Research, Peer-review, Inclusive, Interdisciplinary, Ante-disciplinary, Physics, Chemistry, Engineering"/>
  <meta name="almHost" content="http://alm.plos.org/api/v3/articles"/>
  <meta name="searchHost" content="http://api.plos.org/search" />
  <meta name="termsHost" content="http://api.plos.org/terms" />
  <meta name="solrApiKey" content="plos"/>
  <meta name="almAPIKey" content="3pezRBRXdyzYW6ztfwft" />
  <meta name="currentJournal" content="PLoSONE" />
  <meta name="almRequestBatchSize" content="" />

  <meta name="citation_publisher" content="Public Library of Science"/>
  <meta name="citation_doi" content="10.1371/journal.pone.0001699"/>
  <meta name="dc.identifier" content="10.1371/journal.pone.0001699" />

    <meta name="citation_title" content="Are There Multiple Visual Short-Term Memory Stores?"/>
    <meta itemprop="name" content="Are There Multiple Visual Short-Term Memory Stores?"/>

      <meta name="citation_author" content="Ilja G. Sligte"/>
            <meta name="citation_author_institution" content="Cognitive Neuroscience Group, Department of Psychology, University of Amsterdam, Amsterdam, The Netherlands"/>
      <meta name="citation_author" content="H. Steven Scholte"/>
            <meta name="citation_author_institution" content="Cognitive Neuroscience Group, Department of Psychology, University of Amsterdam, Amsterdam, The Netherlands"/>
      <meta name="citation_author" content="Victor A. F. Lamme"/>
            <meta name="citation_author_institution" content="Cognitive Neuroscience Group, Department of Psychology, University of Amsterdam, Amsterdam, The Netherlands"/>
            <meta name="citation_author_institution" content="Netherlands Institute for Neuroscience, Royal Netherlands Academy of Arts and Sciences (KNAW), Amsterdam, The Netherlands"/>

    <meta name="citation_date" content="2008/2/27"/>

  <meta name="citation_pdf_url" content="http://dx.plos.org/10.1371/journal.pone.0001699.pdf" />

      <meta name="citation_journal_title" content="PLOS ONE" />
    <meta name="citation_firstpage" content="e1699"/>
    <meta name="citation_issue" content="2"/>
    <meta name="citation_volume" content="3"/>
    <meta name="citation_issn" content="1932-6203"/>

    <meta name="citation_journal_abbrev" content="PLoS ONE" />

      <meta name="citation_reference" content="citation_title=Information integration across saccadic eye movements.; citation_author=DE Irwin; citation_journal_title=Cognit Psychol; citation_volume=23; citation_number=1; citation_pages=420-456; citation_date=1991; " />
      <meta name="citation_reference" content="citation_title=Integrating information across saccadic eye movements.; citation_author=DE Irwin; citation_journal_title=Curr Dir in Psychol Sci; citation_volume=5; citation_number=2; citation_pages=94-100; citation_date=1996; " />
      <meta name="citation_reference" content="citation_title=The capacity of visual working memory for features and conjunctions.; citation_author=SJ Luck; citation_author=EK Vogel; citation_journal_title=Nature; citation_volume=390; citation_number=3; citation_pages=279-281; citation_date=1997; " />
      <meta name="citation_reference" content="citation_title=Delayed matching-to-sample performance as a measure of human visuospatial working memory.; citation_author=WV Parr; citation_journal_title=B Psychonomic Soc; citation_volume=30; citation_number=4; citation_pages=369-372; citation_date=1992; " />
      <meta name="citation_reference" content="citation_title=Familiarity and visual change detection.; citation_author=H Pashler; citation_journal_title=Percept Psychophys; citation_volume=44; citation_number=5; citation_pages=369-378; citation_date=1988; " />
      <meta name="citation_reference" content="citation_title=On the distinction between sensory storage and short-term visual memory.; citation_author=WA Phillips; citation_journal_title=Percept Psychophys; citation_volume=16; citation_number=6; citation_pages=283-290; citation_date=1974; " />
      <meta name="citation_reference" content="citation_title=Storage of features, conjunctions and objects in visual working memory.; citation_author=EK Vogel; citation_author=GF Woodman; citation_author=SJ Luck; citation_journal_title=J Exp Psychol Hum Percept Perform; citation_volume=27; citation_number=7; citation_pages=92-114; citation_date=2001; " />
      <meta name="citation_reference" content="citation_title=The capacity of visual short-term memory is set both by visual information load and by number of objects.; citation_author=GA Alvarez; citation_author=P Cavanagh; citation_journal_title=Psychol Sci; citation_volume=15; citation_number=8; citation_pages=106-111; citation_date=2004; " />
      <meta name="citation_reference" content="citation_title=Visual working memory for simple and complex visual stimuli.; citation_author=HY Eng; citation_author=D Chen; citation_author=Y Jiang; citation_journal_title=Psychon Bull Rev; citation_volume=12; citation_number=9; citation_pages=1127-1133; citation_date=2005; " />
      <meta name="citation_reference" content="citation_title=Visual memory needs categories.; citation_author=H Olsson; citation_author=L Poom; citation_journal_title=Proc Natl Acad Sci U S A; citation_volume=102; citation_number=10; citation_pages=8776-8780; citation_date=2005; " />
      <meta name="citation_reference" content="citation_title=Perceptual grouping in change detection.; citation_author=Y Jiang; citation_author=MM Chun; citation_author=IR Olson; citation_journal_title=Percept Psychophys; citation_volume=66; citation_number=11; citation_pages=446-453; citation_date=2004; " />
      <meta name="citation_reference" content="citation_title=Organization of visual short-term memory.; citation_author=Y Jiang; citation_author=IR Olson; citation_author=MM Chun; citation_journal_title=J Exp Psychol Learn Mem Cogn; citation_volume=26; citation_number=12; citation_pages=683-702; citation_date=2000; " />
      <meta name="citation_reference" content="citation_title=The information available in brief visual presentations.; citation_author=G Sperling; citation_journal_title=Psychol monogr; citation_volume=74; citation_number=13; citation_pages=1-29; citation_date=1960; " />
      <meta name="citation_reference" content="citation_title=Orienting attention to locations in internal representations.; citation_author=IC Griffin; citation_author=AC Nobre; citation_journal_title=J Cogn Neurosci; citation_volume=15; citation_number=14; citation_pages=1176-1194; citation_date=2003; " />
      <meta name="citation_reference" content="citation_title=Large capacity storage of integrated objects before change blindness.; citation_author=R Landman; citation_author=H Spekreijse; citation_author=VAF Lamme; citation_journal_title=Vision Res; citation_volume=43; citation_number=15; citation_pages=149-164; citation_date=2003; " />
      <meta name="citation_reference" content="citation_title=Directing spatial attention in mental representations: Interactions between attentional orienting and working-memory load.; citation_author=J Lepsien; citation_author=IC Griffin; citation_author=JT Devlin; citation_author=AC Nobre; citation_journal_title=Neuroimage; citation_volume=26; citation_number=16; citation_pages=733-743; citation_date=2005; " />
      <meta name="citation_reference" content="citation_title=Attentional modulation of object representations in working memory.; citation_author=J Lepsien; citation_author=AC Nobre; citation_journal_title=Cereb Cortex; citation_volume=17; citation_number=17; citation_pages=2072-2083; citation_date=2007; " />
      <meta name="citation_reference" content="citation_title=Distributing versus focusing attention in visual short-term memory; citation_author=T Makovski; citation_author=YV Jiang; citation_journal_title=Psychonomic Bulletin & Review; citation_volume=14; citation_number=18; citation_pages=1072-1078; citation_date=2007; " />
      <meta name="citation_reference" content="citation_title=Orienting attention in visual working memory reduces interference from memory probes.; citation_author=T Makovski; citation_author=R Sussman; citation_author=YV Jiang; citation_journal_title=Journal of Experimental Psychology: Learning, Memory, & Cognition; citation_number=19; citation_date=In press; " />
      <meta name="citation_reference" content="citation_title=Attention effects during visual short-term memory maintenance: Protection or prioritization?; citation_author=MJLS Matsukura; citation_author=SP Vecera; citation_journal_title=Percept Psychophys; citation_volume=69; citation_number=20; citation_pages=1422-1434; citation_date=2007; " />
      <meta name="citation_reference" content="citation_title=The magical number 4 in short-term memory: a reconsideration of mental storage capacity.; citation_author=N Cowan; citation_journal_title=Behav Brain Sci; citation_volume=24; citation_number=21; citation_pages=87-114; citation_date=2001; " />
      <meta name="citation_reference" content="citation_title=Iconic storage: the role of rods.; citation_author=EH Adelson; citation_journal_title=Science; citation_volume=201; citation_number=22; citation_pages=544-546; citation_date=1978; " />
      <meta name="citation_reference" content="citation_title=Two neural correlates of consciousness.; citation_author=N Block; citation_journal_title=Trends Cogn Sci; citation_volume=9; citation_number=23; citation_pages=46-52; citation_date=2005; " />
      <meta name="citation_reference" content="citation_title=Why visual attention and awareness are different.; citation_author=VA Lamme; citation_journal_title=Trends Cogn Sci; citation_volume=7; citation_number=24; citation_pages=12-18; citation_date=2003; " />
      <meta name="citation_reference" content="citation_title=Towards a true neural stance on consciousness.; citation_author=VA Lamme; citation_journal_title=Trends Cogn Sci; citation_volume=10; citation_number=25; citation_pages=494-501; citation_date=2006; " />
      <meta name="citation_reference" content="citation_title=Iconic memory and visible persistence.; citation_author=M Coltheart; citation_journal_title=Percept Psychophys; citation_volume=27; citation_number=26; citation_pages=183-228; citation_date=1980; " />
      <meta name="citation_reference" content="citation_title=J Exp Psychol; citation_author=AO Dick; citation_volume=82; citation_number=27; citation_pages=279-284; citation_date=1969; " />
      <meta name="citation_reference" content="citation_title=Loss of spatial and identity information following a tachistoscopic exposure.; citation_author=VM Townsend; citation_journal_title=J Exp Psychol; citation_volume=98; citation_number=28; citation_pages=113-118; citation_date=1973; " />
      <meta name="citation_reference" content="citation_title=Identification, localization, and “iconic memory”: an evaluation of the bar-probe task.; citation_author=DJ Mewhort; citation_author=AJ Campbell; citation_author=FM Marchetti; citation_author=JI Campbell; citation_journal_title=Mem Cognit; citation_volume=9; citation_number=29; citation_pages=50-67; citation_date=1981; " />
      <meta name="citation_reference" content="citation_title=Spatiotopic and retinotopic components of iconic memory.; citation_author=K McRae; citation_author=BE Butler; citation_author=SJ Popiel; citation_journal_title=Psychol Res; citation_volume=49; citation_number=30; citation_pages=221-227; citation_date=1987; " />
      <meta name="citation_reference" content="citation_title=The distinct modes of vision offered by feedforward and recurrent processing.; citation_author=VA Lamme; citation_author=PR Roelfsema; citation_journal_title=Trends Neurosci; citation_volume=23; citation_number=31; citation_pages=571-579; citation_date=2000; " />
      <meta name="citation_reference" content="citation_title=Masking interrupts figure-ground signals in V1.; citation_author=VA Lamme; citation_author=K Zipser; citation_author=H Spekreijse; citation_journal_title=J Cogn Neurosci; citation_volume=14; citation_number=32; citation_pages=1044-1053; citation_date=2002; " />
      <meta name="citation_reference" content="citation_title=Repression of unconscious information by conscious processing: evidence from affective blindsight induced by transcranial magnetic stimulation.; citation_author=J Jolij; citation_author=VA Lamme; citation_journal_title=Proc Natl Acad Sci U S A; citation_volume=102; citation_number=33; citation_pages=10747-10751; citation_date=2005; " />
      <meta name="citation_reference" content="citation_title=Fast backprojections from the motion to the primary visual area necessary for visual awareness.; citation_author=A Pascual-Leone; citation_author=V Walsh; citation_journal_title=Science; citation_volume=292; citation_number=34; citation_pages=510-512; citation_date=2001; " />
      <meta name="citation_reference" content="citation_title=Cortical feedback improves discrimination between figure and background by V1, V2 and V3 neurons.; citation_author=JM Hupe; citation_author=AC James; citation_author=BR Payne; citation_author=SG Lomber; citation_author=P Girard; citation_journal_title=Nature; citation_volume=394; citation_number=35; citation_pages=784-787; citation_date=1998; " />
      <meta name="citation_reference" content="citation_title=Figure-ground activity in primary visual cortex is suppressed by anesthesia.; citation_author=VA Lamme; citation_author=K Zipser; citation_author=H Spekreijse; citation_journal_title=Proc Natl Acad Sci U S A; citation_volume=95; citation_number=36; citation_pages=3263-3268; citation_date=1998; " />
      <meta name="citation_reference" content="citation_title=Two distinct modes of sensory processing observed in monkey primary visual cortex (V1).; citation_author=H Super; citation_author=H Spekreijse; citation_author=VA Lamme; citation_journal_title=Nat Neurosci; citation_volume=4; citation_number=37; citation_pages=304-310; citation_date=2001; " />
      <meta name="citation_reference" content="citation_title=Conscious, preconscious, and subliminal processing: a testable taxonomy.; citation_author=S Dehaene; citation_author=JP Changeux; citation_author=L Naccache; citation_author=J Sackur; citation_author=C Sergent; citation_journal_title=Trends Cogn Sci; citation_volume=10; citation_number=38; citation_pages=204-211; citation_date=2006; " />
      <meta name="citation_reference" content="citation_title=The role of figure-ground segregation in change blindness.; citation_author=R Landman; citation_author=H Spekreijse; citation_author=VA Lamme; citation_journal_title=Psychon Bull Rev; citation_volume=11; citation_number=39; citation_pages=254-261; citation_date=2004; " />

  <link rel="canonical" href="http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0001699" />

    <meta name="twitter:card" content="summary"/>
    <meta name="twitter:site" content="@plosone"/>
    <meta name="twitter:title" content="Are There Multiple Visual Short-Term Memory Stores?"/>
    <meta name="twitter:description" content="BackgroundClassic work on visual short-term memory (VSTM) suggests that people store a limited amount of items for subsequent report. However, when human observers are cued to shift attention to one item in VSTM during retention, it seems as if there is a much larger representation, which keeps additional items in a more fragile VSTM store. Thus far, it is not clear whether the capacity of this fragile VSTM store indeed exceeds the traditional capacity limits of VSTM. The current experiments address this issue and explore the capacity, stability, and duration of fragile VSTM representations.Methodology/Principal FindingsWe presented cues in a change-detection task either just after off-set of the memory array (iconic-cue), 1,000 ms after off-set of the memory array (retro-cue) or after on-set of the probe array (post-cue). We observed three stages in visual information processing 1) iconic memory with unlimited capacity, 2) a four seconds lasting fragile VSTM store with a capacity that is at least a factor of two higher than 3) the robust and capacity-limited form of VSTM. Iconic memory seemed to depend on the strength of the positive after-image resulting from the memory display and was virtually absent under conditions of isoluminance or when intervening light masks were presented. This suggests that iconic memory is driven by prolonged retinal activation beyond stimulus duration. Fragile VSTM representations were not affected by light masks, but were completely overwritten by irrelevant pattern masks that spatially overlapped the memory array.Conclusions/SignificanceWe find that immediately after a stimulus has disappeared from view, subjects can still access information from iconic memory because they can see an after-image of the display. After that period, human observers can still access a substantial, but somewhat more limited amount of information from a high-capacity, but fragile VSTM that is overwritten when new items are presented to the eyes. What is left after that is the traditional VSTM store, with a limit of about four objects. We conclude that human observers store more sustained representations than is evident from standard change detection tasks and that these representations can be accessed at will."/>
      <meta name="twitter:image" content="http://dx.plos.org/10.1371/journal.pone.0001699.g008"/>

  <meta property="og:title" content="Are There Multiple Visual Short-Term Memory Stores?" />
  <meta property="og:type" content="article" />
  <meta property="og:url" content="http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0001699" />

 <!--end articleInfoX-->

  <link rel="pingback" href="http://www.plosone.org/pingback" />


  <link rel="shortcut icon" href="/images/favicon.ico" type="image/x-icon"/>
  <link rel="home" title="home" href="/"/>
  <link rel="alternate" type="application/rss+xml"
        title="PLOS ONE: New Articles"
        href="http://www.plosone.org/article/feed"/>
</head>
<body>

  <div id="page-wrap">
    <div id="topbanner" class="cf">

<!-- Div for the ad at the top of journal home page-->
<div class="center">
  <div class="title">Advertisement</div>
  <iframe id='a3ac9da4' name='a3ac9da4'
    src='http://ads.plos.org/www/delivery/afr.php?zoneid=345&amp;cb=8248'
    frameborder='0' scrolling='no' width='730' height='90'>
    <a href='http://ads.plos.org/www/delivery/ck.php?n=a3ac9da4&amp;cb=8255'
      target='_top'><img src='http://ads.plos.org/www/delivery/avw.php?zoneid=345&amp;cb=2513&amp;n=a3ac9da4'
      border='0' alt=''/>
    </a>
  </iframe>
</div>    </div>

    <div id="pagehdr-wrap">
      <div id="pagehdr">
        <div id="user" class="nav">
          <ul>
            <li><a href="http://www.plos.org">plos.org</a></li>
            <li><a href="https://register.plos.org/ambra-registration/register.action">create account</a></li>
            <li class="btn-style"><a
              href="/user/secure/secureRedirect.action?goTo=%2Farticle%2FfetchArticle.action%3FarticleURI%3Dinfo%253Adoi%252F10.1371%252Fjournal.pone.0001699">sign in</a>
            </li>
          </ul>
        </div>
        <div class="logo">
          <a href="/"><img src="/images/logo.png" alt="PLOS ONE"></a>
        </div>

<div id="nav-main" class="nav">
  <ul>
        <li id="mn-01"><a href="/taxonomy" class="areas-link">Subject Areas</a></li>
    <li id="mn-02"><a href="javascript:void(0);">For Authors</a>
      <div class="submenu" style="width: 540px; margin-left: -300px;">
        <div class="block">
          <div class="submit-script">
            <h3>Submit your Manuscript</h3>
            <ul>
              <li>Fair, rigorous peer review</li>
              <li>Broad scope and wide reach</li>
            </ul>
            <a href="/static/submissionInstructions" class="btn">get started</a>
          </div>
        </div>
        <div class="menu">
          <ul>
            <li><a href="/static/publish">Why Publish with PLOS ONE</a></li>
            <li><a href="/static/publication">Publication Criteria</a></li>
            <li><a href="/static/editorial">Editorial Policies</a></li>
            <li><a href="/static/guidelines">Preparing A Manuscript</a></li>
            <li><a href="/static/figureGuidelines">Figure and Table Guidelines</a></li>
          <li><a href="/static/supportingInformation">Supporting Information Guidelines</a></li>
            <li><a href="/static/submissionInstructions">Submitting a Manuscript</a></li>
          </ul>
        </div>
      </div>
    </li>

    <li id="mn-03"><a href="javascript:void(0);">About Us</a>
      <div class="submenu" style="width:248px; margin-left:-30px;">
        <div class="menu">
          <ul>
            <li><a href="/static/information">Journal Information</a></li>
            <li><a href="/static/edboard">Editorial Board</a></li>
            <li><a href="/static/reviewerGuidelines">Reviewer Guidelines</a></li>
            <li><a href="/static/almInfo">Article-Level Metrics</a></li>
            <li><a href="/static/license">Open-Access License</a></li>
            <li><a href="/static/downloads">Media Downloads</a></li>
            <li><a href="/static/commentGuidelines">Guidelines for Comments</a></li>
            <li><a href="/static/corrections">Corrections</a></li>
            <li><a href="/static/help">Help Using this Site</a></li>
            <li><a href="/static/contact">Contact Us</a></li>
          </ul>
        </div>
      </div>
    </li>
  </ul>
<div id="db">
  <form name="searchForm" action="/search/simple?noSearchFlag=true&amp;query=&amp;articleURI=info%3Adoi%2F10.1371%2Fjournal.pone.0001699" method="get" >
<input type="hidden" name="from" value="globalSimpleSearch" id="from"/><input type="hidden" name="filterJournals" value="PLoSONE" id="filterJournals"/>    <fieldset>
      <legend>Search</legend>
      <label for="search">Search</label>
      <div class="wrap">
        <input id="search" type="text" name="query" placeholder="Search">
        <input type="image" alt="SEARCH" src="/images/icon.search.gif">
      </div>
    </fieldset>
  </form>
    <a id="advSearch" href="/search/advanced?noSearchFlag=true&amp;query=&amp;articleURI=info%3Adoi%2F10.1371%2Fjournal.pone.0001699&filterJournals=PLoSONE">advanced search</a>
</div></div>

      </div>
      <!-- pagehdr-->
    </div>
    <!-- pagehdr-wrap -->

  <!--body and html tags gets closed in global_footer.ftl-->
<script type="text/javascript" src="/javascript/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<div id="pagebdy-wrap">
  <div id="pagebdy">

    <div id="article-block" class="cf">

<div class="article-meta cf">
  <ul id="almSignPost" style="display: none;"></ul>
  <div class="article-type">
    <span class="type oa">Open Access</span>
      <span class="type pr">Peer-Reviewed</span>
  </div>
</div>

<div class="header" id="hdr-article">

<div class="article-kicker">
      <span id="article-type-heading">
        Research Article
      </span>
</div>  <h1 property="dc:title" datatype="" rel="dc:type" href="http://purl.org/dc/dcmitype/Text">
    Are There Multiple Visual Short-Term Memory Stores?
  </h1>

  <ul class="authors">
      <li>


        <span rel="dc:creator" class="author">
          <span class="person" property="foaf:name" typeof="foaf:Person">
            Ilja G. Sligte
              <span class="corresponding">mail</span>, 
          </span>
        </span>

          <div class="author_meta">
            <div class="author_inner">


              
              <p><span class="email">*</span>To whom correspondence should be addressed. E-mail: <a href="mailto:I.G.Sligte@uva.nl">I.G.Sligte@uva.nl</a></p>

                <p>Affiliation:
                  Cognitive Neuroscience Group, Department of Psychology, University of Amsterdam, Amsterdam, The Netherlands
                </p>


              <span class="close">X</span>

            </div>
          </div>
      </li>
      <li>


        <span rel="dc:creator" class="author">
          <span class="person" property="foaf:name" typeof="foaf:Person">
            H. Steven Scholte, 
          </span>
        </span>

          <div class="author_meta">
            <div class="author_inner">


              
              

                <p>Affiliation:
                  Cognitive Neuroscience Group, Department of Psychology, University of Amsterdam, Amsterdam, The Netherlands
                </p>


              <span class="close">X</span>

            </div>
          </div>
      </li>
      <li>


        <span rel="dc:creator" class="author">
          <span class="person" property="foaf:name" typeof="foaf:Person">
            Victor A. F. Lamme
          </span>
        </span>

          <div class="author_meta">
            <div class="author_inner">


              
              

                <p>Affiliations:
                  Cognitive Neuroscience Group, Department of Psychology, University of Amsterdam, Amsterdam, The Netherlands, 
                  Netherlands Institute for Neuroscience, Royal Netherlands Academy of Arts and Sciences (KNAW), Amsterdam, The Netherlands
                </p>


              <span class="close">X</span>

            </div>
          </div>
      </li>
  </ul>
  <ul class="date-doi-line">
    <li>Published: February 27, 2008</li>
    <li>DOI: 10.1371/journal.pone.0001699</li>
  </ul>


</div><!--end header-->
<div class="main cf" id="pjax-container">
  

<div class="nav items-5" id="nav-article">
  <ul>
  <li>
        <span class="active" name="article">Article</span>
  </li>
  <li>
      <a href="/article/authors/info%3Adoi%2F10.1371%2Fjournal.pone.0001699" name="authors">About the Authors</a>
  </li>
  <li>
      <a href="/article/metrics/info%3Adoi%2F10.1371%2Fjournal.pone.0001699" name="metrics">Metrics</a>
  </li>
  <li>
      <a href="/article/comments/info%3Adoi%2F10.1371%2Fjournal.pone.0001699" name="comments">Comments</a>
  </li>
  <li>
      <a href="/article/related/info%3Adoi%2F10.1371%2Fjournal.pone.0001699" name="related">Related Content</a>
  </li>
  </ul>
</div>

<script type="text/javascript">
  var selected_tab = "article";
</script>
  <div id="figure-thmbs" class="carousel cf">
    <div class="wrapper">
      <div class="slider">
              <div class="item">
                <a href="#pone-0001699-g001" data-doi="info:doi/10.1371/journal.pone.0001699" data-uri="info:doi/10.1371/journal.pone.0001699.g001" title="Figure 1">
                  <span class="thmb-wrap">
                    <img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001699.g001&representation=PNG_I" alt="">
                  </span>
                </a>
              </div>
              <div class="item">
                <a href="#pone-0001699-g002" data-doi="info:doi/10.1371/journal.pone.0001699" data-uri="info:doi/10.1371/journal.pone.0001699.g002" title="Figure 2">
                  <span class="thmb-wrap">
                    <img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001699.g002&representation=PNG_I" alt="">
                  </span>
                </a>
              </div>
              <div class="item">
                <a href="#pone-0001699-g003" data-doi="info:doi/10.1371/journal.pone.0001699" data-uri="info:doi/10.1371/journal.pone.0001699.g003" title="Figure 3">
                  <span class="thmb-wrap">
                    <img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001699.g003&representation=PNG_I" alt="">
                  </span>
                </a>
              </div>
              <div class="item">
                <a href="#pone-0001699-g004" data-doi="info:doi/10.1371/journal.pone.0001699" data-uri="info:doi/10.1371/journal.pone.0001699.g004" title="Figure 4">
                  <span class="thmb-wrap">
                    <img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001699.g004&representation=PNG_I" alt="">
                  </span>
                </a>
              </div>
              <div class="item">
                <a href="#pone-0001699-g005" data-doi="info:doi/10.1371/journal.pone.0001699" data-uri="info:doi/10.1371/journal.pone.0001699.g005" title="Figure 5">
                  <span class="thmb-wrap">
                    <img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001699.g005&representation=PNG_I" alt="">
                  </span>
                </a>
              </div>
              <div class="item">
                <a href="#pone-0001699-g006" data-doi="info:doi/10.1371/journal.pone.0001699" data-uri="info:doi/10.1371/journal.pone.0001699.g006" title="Figure 6">
                  <span class="thmb-wrap">
                    <img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001699.g006&representation=PNG_I" alt="">
                  </span>
                </a>
              </div>
              <div class="item">
                <a href="#pone-0001699-g007" data-doi="info:doi/10.1371/journal.pone.0001699" data-uri="info:doi/10.1371/journal.pone.0001699.g007" title="Figure 7">
                  <span class="thmb-wrap">
                    <img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001699.g007&representation=PNG_I" alt="">
                  </span>
                </a>
              </div>
              <div class="item">
                <a href="#pone-0001699-g008" data-doi="info:doi/10.1371/journal.pone.0001699" data-uri="info:doi/10.1371/journal.pone.0001699.g008" title="Figure 8">
                  <span class="thmb-wrap">
                    <img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001699.g008&representation=PNG_I" alt="">
                  </span>
                </a>
              </div>
      </div>
    </div>
  </div>

  <div class="nav-col">
    <div class="nav" id="nav-article-page">
      <ul>
        <li class="nav-col-comments"><a href="/article/comments/info%3Adoi%2F10.1371%2Fjournal.pone.0001699">Reader Comments (1)</a></li>
          <li id="nav-figures"><a data-doi="info:doi/10.1371/journal.pone.0001699" >Figures</a></li>
      </ul>
    </div>
  </div>

  <div class="article">







<div class="abstract"><a id="abstract0" name="abstract0" toc="abstract0" title="Abstract"></a><h2>Abstract</h2>
<h3>Background</h3>
<a id="article1.front1.article-meta1.abstract1.sec1.p1" name="article1.front1.article-meta1.abstract1.sec1.p1"></a><p>Classic work on visual short-term memory (VSTM) suggests that people store a limited amount of items for subsequent report. However, when human observers are cued to shift attention to one item in VSTM during retention, it seems as if there is a much larger representation, which keeps additional items in a more fragile VSTM store. Thus far, it is not clear whether the capacity of this fragile VSTM store indeed exceeds the traditional capacity limits of VSTM. The current experiments address this issue and explore the capacity, stability, and duration of fragile VSTM representations.</p>


<h3>Methodology/Principal Findings</h3>
<a id="article1.front1.article-meta1.abstract1.sec2.p1" name="article1.front1.article-meta1.abstract1.sec2.p1"></a><p>We presented cues in a <em>change-detection task</em> either just after off-set of the memory array (<em>iconic-cue</em>), 1,000 ms after off-set of the memory array (<em>retro-cue</em>) or after on-set of the probe array (<em>post-cue</em>). We observed three stages in visual information processing 1) iconic memory with unlimited capacity, 2) a four seconds lasting fragile VSTM store with a capacity that is at least a factor of two higher than 3) the robust and capacity-limited form of VSTM. Iconic memory seemed to depend on the strength of the positive after-image resulting from the memory display and was virtually absent under conditions of isoluminance or when intervening light masks were presented. This suggests that iconic memory is driven by prolonged retinal activation beyond stimulus duration. Fragile VSTM representations were not affected by light masks, but were completely overwritten by irrelevant pattern masks that spatially overlapped the memory array.</p>


<h3>Conclusions/Significance</h3>
<a id="article1.front1.article-meta1.abstract1.sec3.p1" name="article1.front1.article-meta1.abstract1.sec3.p1"></a><p>We find that immediately after a stimulus has disappeared from view, subjects can still access information from iconic memory because they can see an after-image of the display. After that period, human observers can still access a substantial, but somewhat more limited amount of information from a high-capacity, but fragile VSTM that is overwritten when new items are presented to the eyes. What is left after that is the traditional VSTM store, with a limit of about four objects. We conclude that human observers store more sustained representations than is evident from standard change detection tasks and that these representations can be accessed at will.</p>

</div>


<div class="articleinfo"><p><strong>Citation: </strong>Sligte IG, Scholte HS, Lamme VAF (2008) Are There Multiple Visual Short-Term Memory Stores? PLoS ONE 3(2):
          e1699.
            doi:10.1371/journal.pone.0001699</p><p><strong>Academic Editor: </strong>Sheng He, University of Minnesota, United States of America</p><p><strong>Received:</strong> November 5, 2007; <strong>Accepted:</strong> January 31, 2008; <strong>Published:</strong> February 27, 2008</p><p><strong>Copyright:</strong> © 2008 Sligte et al. This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p><p><strong>Funding: </strong>The authors have no support or funding to report.</p><p><strong>Competing interests:</strong> The authors have declared that no competing interests exist.</p></div>





<div id="section1" class="section"><a id="s1" name="s1" toc="s1" title="Introduction"></a><h3>Introduction</h3><a id="article1.body1.sec1.p1" name="article1.body1.sec1.p1"></a><p>Humans are constantly interacting with a complex and ever-changing environment. Selectively orienting our attention to specific parts of the external world seems to be essential to efficiently process all available information. Although we tend to believe that we perceive everything around us, the visual <em>change-detection task</em> strikingly demonstrates that this is not the case. In the typical <em>change-detection task</em>, observers are shown a multi-item memory array (or a complex natural scene containing many items) and they are asked to remember as much individual items as possible. A short while after disappearance of the memory array, a probe array appears and subjects report whether the probe array is identical to the memory array or not. Observers are generally good at this task when they have to remember four items or less, but performance deteriorates rapidly when more than four items are shown in the memory array. A well-accepted explanation for this result is that people can store a maximum of about four integrated objects in visual short-term memory (VSTM) <a href="#pone.0001699-Irwin1">[1]</a>–<a href="#pone.0001699-Vogel1">[7]</a>, although the exact capacity seems to depend on stimulus complexity <a href="#pone.0001699-Alvarez1">[8]</a>–<a href="#pone.0001699-Olsson1">[10]</a> and the organization of objects in the memory array <a href="#pone.0001699-Jiang1">[11]</a>, <a href="#pone.0001699-Jiang2">[12]</a>.</p>
<a id="article1.body1.sec1.p2" name="article1.body1.sec1.p2"></a><p>Recently, several authors have begun to question whether indeed mental representation are limited to the four objects stored in VSTM. They probed for additional representations by introducing cues during the retention interval of a <em>change-detection task</em> that retrospectively indicate which item has to be attended (a so-called <em>retro-cue</em>). This is very similar to the way iconic memory is measured <a href="#pone.0001699-Sperling1">[13]</a>, only now the <em>retro-cue</em> is provided well beyond the time domain in which iconic memory can exert its influence. All experiments so far <a href="#pone.0001699-Griffin1">[14]</a>–<a href="#pone.0001699-Matsukura1">[20]</a> have reported an increase in performance when a <em>retro-cue</em> is provided compared to when no cue or a cue during the probe array is provided (a so-called <em>post-cue</em>). This suggests that VSTM has an additional capacity that is however overwritten as soon as a second array (i.e. the probe array) is shown.</p>
<a id="article1.body1.sec1.p3" name="article1.body1.sec1.p3"></a><p>One can ask what happens to a VSTM representation when it is cued retrospectively. It seems that a <em>retro-cue</em> protects a fragile VSTM representation from interference with new information (such as the probe array), regardless of whether this new information is irrelevant <a href="#pone.0001699-Makovski1">[18]</a> or task-relevant, <a href="#pone.0001699-Makovski2">[19]</a>, <a href="#pone.0001699-Matsukura1">[20]</a>. It does so by recruiting the same fron toparietal network (responsible for selective attention) as when a cue is shown before the presentation of an image <a href="#pone.0001699-Griffin1">[14]</a>, <a href="#pone.0001699-Lepsien1">[16]</a>, <a href="#pone.0001699-Lepsien2">[17]</a>, resulting in enhanced activity of the representation in object-specific cortex <a href="#pone.0001699-Lepsien2">[17]</a>. Most likely, this enhancement in object-specific cortex protects relatively fragile VSTM representations against overwriting. So, contrary to dogmatic views of VSTM as a robust and capacity-limited store that is able to retain information as long as subjects concentrate on the task at hand, VSTM seems to exhibit gradations of robustness, depending on the amount of attention that is allocated to it.</p>
<a id="article1.body1.sec1.p4" name="article1.body1.sec1.p4"></a><p>However, whether the capacity of VSTM indeed surpasses the “magical number 4” is still controversial <a href="#pone.0001699-Cowan1">[21]</a> . All the findings referred to above are limited since the number of items presented in the memory array cannot be presumed to have really exceeded the capacity limits of VSTM, except maybe for the experiment of Landman <a href="#pone.0001699-Landman1">[15]</a>. Yet, a commonly heard objection against the high-capacity results of Landman is that oriented rectangles were used in the paradigm and subjects could have grouped these objects to form fewer compound figures (‘chunking’), hence the high capacity measure. Therefore, the current experiments further explore the capacity, stability and lifetime of fragile VSTM representations. In addition, we address whether ‘chunking’ of simple oriented rectangles into fewer compound figures can explain the high capacity of fragile VSTM.</p>
</div>

<div id="section2" class="section"><a id="s2" name="s2" toc="s2" title="Results"></a><h3>Results</h3>
<h4>Representational limits in VSTM</h4>
<a id="article1.body1.sec2.sec1.p1" name="article1.body1.sec2.sec1.p1"></a><p>The goal of this experiment was to produce estimates of the representational limits in VSTM and we varied set size of the memory array up to 32 figures accordingly. We used simple oriented rectangles, as the capacity of VSTM tends to decrease with stimulus complexity (see below). In the basic design, subjects were asked to detect changes between a briefly presented memory array (<a href="#pone-0001699-g001"><strong>Fig. 1a/b</strong></a>) and a subsequently delivered probe array. There was a change in 50% of cases, and we cued the location of the potentially changing item at different moments after presentation of the memory array. This cue (<a href="#pone-0001699-g001"><strong>Fig. 1c</strong></a>) was provided either 10 ms after off-set of the memory array (<em>iconic-cue</em><strong>; </strong><a href="#pone-0001699-g001"><strong>Fig. 1d</strong></a>), 1,000 ms after off-set memory array, but before on-set of the probe array (<em>retro-cue</em><strong>; </strong><a href="#pone-0001699-g001"><strong>Fig. 1e</strong></a>) or 100 ms after on-set of the probe array (<em>post-cue</em><strong>; </strong><a href="#pone-0001699-g001"><strong>Fig. 1f</strong></a>). To prevent that subjects used a strategy of grouping similar items together, we rotated all other items by 90 degrees between memory and probe array. When subjects did use strategy of grouping, this should lead to the general percept of change on each trial, and a corresponding decrease in performance. In addition, we manipulated strength of the positive after-image by using either white items on a black background, or red items on an isoluminant gray background (<a href="#pone-0001699-g001"><strong>Fig. 1a/b</strong></a>). By using white rectangles on a black background, we recruited both rod and cone systems, and by using red rectangles on a grey background of the same luminance, we recruited the isolated cone system <a href="#pone.0001699-Adelson1">[22]</a>.</p>
<div class="figure" id="pone-0001699-g001"><div class="img"><a name="pone-0001699-g001" title="Click for larger image " href="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001699.g001&amp;representation=PNG_M" data-doi="info:doi/10.1371/journal.pone.0001699" data-uri="info:doi/10.1371/journal.pone.0001699.g001"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001699.g001&amp;representation=PNG_I" alt="thumbnail" class="thumbnail"></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001699.g001/powerpoint">
                    PPT
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001699.g001/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001699.g001/largerimage">
                    PNG
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001699.g001/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pone.0001699.g001.PNG_L"></span>)
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001699.g001/originalimage">
                    TIFF
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001699.g001/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pone.0001699.g001.TIF"></span>)
                </a></li></ul></div><p><strong>Figure 1.  <span>Design <em>Representational limits in VSTM.</em></span></strong></p><a id="article1.body1.sec2.sec1.fig1.caption1.p1" name="article1.body1.sec2.sec1.fig1.caption1.p1"></a><p>A. High contrast black-white stimulus producing strong after-image B. Isoluminant red-grey stimulus producing weak after-image C. Cue display; background is black in high-contrast condition and grey in isoluminant condition D. Iconic-cue condition measuring iconic memory E. Retro-cue condition measuring fragile VSTM F. Post-cue condition measuring robust VSTM</p>
<span>doi:10.1371/journal.pone.0001699.g001</span></div><a id="article1.body1.sec2.sec1.p2" name="article1.body1.sec2.sec1.p2"></a><p>It is well known that rod receptors integrate information over relatively long periods of time and will continue to respond for some period of time after off-set of a stimulus, whereas cone receptors respond to stimulation with very brief bursts of activation. In effect, by selectively recruiting the rod system, we induced a strong (<a href="#pone-0001699-g001"><strong>Fig. 1a</strong></a>) or a weak (<a href="#pone-0001699-g001"><strong>Fig. 1b</strong></a>), positive after-image. Still, the visibility of a strong, positive after-image would not last more than a few hundred milliseconds, and would thus only influence results when an <em>iconic-cue</em> is delivered (we measured phosphor persistence of our monitor to preclude that experimental effects were due to persistence of the display instead of persistence in the visual system, see <a href="#s4"><strong>Materials and Methods</strong></a>).</p>
<a id="article1.body1.sec2.sec1.p3" name="article1.body1.sec2.sec1.p3"></a><p>When <em>iconic-cues</em> (<a href="#pone-0001699-g001"><strong>Fig. 1d</strong></a>) were delivered, subjects performed nearly perfectly regardless of set size when stimuli producing strong after-images were presented. However, performance was significantly worse when stimuli producing weak after-images were presented [F(1,9) = 22.36, p = .001] (<a href="#pone-0001699-g002"><strong>Fig. 2a</strong></a>). On the other hand, when <em>retro-cues</em> (<a href="#pone-0001699-g001"><strong>Fig. 1e</strong></a>) or <em>post-cues</em> (<a href="#pone-0001699-g001"><strong>Fig. 1f</strong></a>) were delivered, no differences due to the strength of after-images were found [F(1,9) = .25, p = .63]. Still, we observed that subjects could report much more items when <em>retro-cues</em> were provided (<a href="#pone-0001699-g002"><strong>Fig. 2b</strong></a>) instead of <em>post-cues</em> (<a href="#pone-0001699-g002"><strong>Fig. 2c</strong></a>) [F(3, 7) = 38.45, p&lt;.001].</p>
<div class="figure" id="pone-0001699-g002"><div class="img"><a name="pone-0001699-g002" title="Click for larger image " href="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001699.g002&amp;representation=PNG_M" data-doi="info:doi/10.1371/journal.pone.0001699" data-uri="info:doi/10.1371/journal.pone.0001699.g002"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001699.g002&amp;representation=PNG_I" alt="thumbnail" class="thumbnail"></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001699.g002/powerpoint">
                    PPT
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001699.g002/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001699.g002/largerimage">
                    PNG
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001699.g002/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pone.0001699.g002.PNG_L"></span>)
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001699.g002/originalimage">
                    TIFF
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001699.g002/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pone.0001699.g002.TIF"></span>)
                </a></li></ul></div><p><strong>Figure 2.  <span>Results <em>Representational limits in VSTM.</em></span></strong></p><a id="article1.body1.sec2.sec1.fig2.caption1.p1" name="article1.body1.sec2.sec1.fig2.caption1.p1"></a><p>A. Capacity of iconic memory depends on strength of after-image; unlimited for strong after-images and lower, but still high for weak after-images. B. Capacity of fragile VSTM not dependent on strength of after-image; capacity high for both kinds of stimuli. C. Capacity of robust VSTM not dependent on strength of after-image; capacity more or less limited to about 4 figures. Data are plotted as mean Cowan's K+SEM.</p>
<span>doi:10.1371/journal.pone.0001699.g002</span></div><a id="article1.body1.sec2.sec1.p4" name="article1.body1.sec2.sec1.p4"></a><p>Apparently, subjects can retain and report large amounts of information up to 1,000 ms after stimulus off-set, and this is not due to a retinal afterimage producing iconic memory. However, upon arrival of the next image we see ‘overwriting’ of this large capacity store and subjects can only access objects that are represented in VSTM in a robust way. Surprisingly, we found that positive after-images made up the majority of the iconic memory effect, but after-images do not seem to boost performance 1,000 ms after image off-set or after onset of a new image. Based on these results, we can say that there is evidence for two high-capacity stages in visual information processing: 1) iconic memory that is highly dependent on after-images of the shown image and does not seem to be limited in capacity (at least up to 32 objects), and 2) a fragile form of VSTM that at least exceeds a capacity of 10 objects on top of robust VSTM.</p>


<h4>Stability of VSTM representations</h4>
<a id="article1.body1.sec2.sec2.p1" name="article1.body1.sec2.sec2.p1"></a><p>We explored the stability of iconic memory and fragile VSTM representations. This was done by displaying masks before the attention-directing cue was presented. Subjects were informed of these irrelevant mask displays, and they were instructed to ignore them. The mask display was either: 1) a uniform display of light (<a href="#pone-0001699-g003"><strong>Fig. 3b</strong></a>) in the same color as the previously shown objects in the memory arrray (<a href="#pone-0001699-g003"><strong>Fig. 3a</strong></a>), or 2) a pattern mask that was identical to the previously shown memory array with respect to the location of all items, only orientation of individual items was randomly re-assigned (<a href="#pone-0001699-g003"><strong>Fig. 3c</strong></a>). Again, in this experiment, all non-cued items were rotated between memory and probe array to prevent grouping.</p>
<div class="figure" id="pone-0001699-g003"><div class="img"><a name="pone-0001699-g003" title="Click for larger image " href="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001699.g003&amp;representation=PNG_M" data-doi="info:doi/10.1371/journal.pone.0001699" data-uri="info:doi/10.1371/journal.pone.0001699.g003"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001699.g003&amp;representation=PNG_I" alt="thumbnail" class="thumbnail"></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001699.g003/powerpoint">
                    PPT
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001699.g003/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001699.g003/largerimage">
                    PNG
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001699.g003/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pone.0001699.g003.PNG_L"></span>)
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001699.g003/originalimage">
                    TIFF
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001699.g003/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pone.0001699.g003.TIF"></span>)
                </a></li></ul></div><p><strong>Figure 3.  <span>Design <em>Stability of VSTM representations.</em></span></strong></p><a id="article1.body1.sec2.sec2.fig1.caption1.p1" name="article1.body1.sec2.sec2.fig1.caption1.p1"></a><p>A. Memory arrays. B. Light masks. C. Pattern masks; objects are at the same location as memory array, orientations are randomized. D. Iconic-cue condition with or without preceding 10-ms light mask. E. Retro-cue condition with or without preceding 250-ms light or 250-ms pattern mask. F. Post-cue condition.</p>
<span>doi:10.1371/journal.pone.0001699.g003</span></div><a id="article1.body1.sec2.sec2.p2" name="article1.body1.sec2.sec2.p2"></a><p>The presentation of a light mask before the <em>iconic-cue</em> wiped out differences in capacity due to the strength of the after-image [F(1,9) = 18,99, p = .002] (<a href="#pone-0001699-g004"><strong>Fig. 4a</strong></a>). Conversely, this manipulation did not affect performance in the <em>retro-cue</em> condition [F(1,9) = .17, p = .69] (<a href="#pone-0001699-g004"><strong>Fig. 4b</strong></a>). Yet, the appearance of a pattern mask before the <em>retro-cue</em> did result in a dramatic performance deterioration [F(1,9) = 24.39, p = .001] such that no performance difference with the <em>post-cue</em> condition was observed [F(1,9) = .00, p = 1.00] (<a href="#pone-0001699-g004"><strong>Fig. 4b</strong></a>).</p>
<div class="figure" id="pone-0001699-g004"><div class="img"><a name="pone-0001699-g004" title="Click for larger image " href="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001699.g004&amp;representation=PNG_M" data-doi="info:doi/10.1371/journal.pone.0001699" data-uri="info:doi/10.1371/journal.pone.0001699.g004"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001699.g004&amp;representation=PNG_I" alt="thumbnail" class="thumbnail"></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001699.g004/powerpoint">
                    PPT
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001699.g004/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001699.g004/largerimage">
                    PNG
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001699.g004/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pone.0001699.g004.PNG_L"></span>)
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001699.g004/originalimage">
                    TIFF
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001699.g004/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pone.0001699.g004.TIF"></span>)
                </a></li></ul></div><p><strong>Figure 4.  <span>Results <em>Stability of VSTM representations.</em></span></strong></p><a id="article1.body1.sec2.sec2.fig2.caption1.p1" name="article1.body1.sec2.sec2.fig2.caption1.p1"></a><p>A. Iconic memory is overwritten by presenting a light mask. B. Fragile VSTM is not influenced by the presence of a light mask, but a pattern mask erases fragile VSTM representations leading to drop-off in performance to robust VSTM levels. Data are plotted as mean Cowan's K+SEM.</p>
<span>doi:10.1371/journal.pone.0001699.g004</span></div><a id="article1.body1.sec2.sec2.p3" name="article1.body1.sec2.sec2.p3"></a><p>The fact that iconic memory can be wiped out by a non-informational flash of light suggests that this type of memory is pre-categorical in nature and must be driven by persistent activation in the retina. Conversely, the same light mask did not influence <em>retro-cue</em> aided VSTM performance. Only when new, but irrelevant oriented rectangles were presented, did we observe that <em>retro-cues</em> could no longer aid standard VSTM performance. We suggest that iconic memory is driven by persistent retinal activation beyond stimulus duration, while persistent activation in visual and temporal cortex (without additional input of the retina) is responsible for maintenance of fragile VSTM representations (see also <a href="#pone.0001699-Lepsien2">[17]</a>).</p>


<h4>Influence of perceptual organization on change detection</h4>
<a id="article1.body1.sec2.sec3.p1" name="article1.body1.sec2.sec3.p1"></a><p>A potential problem with our results thus far, showing higher capacity representations upon the <em>retro-cue</em> compared to the <em>post-cue,</em> is that our measure to counter chunking (rotating all irrelevant items between memory and probe arrays) could have introduced a difference in capacity by itself. It was recently shown that the orientation of lines is automatically coded in a memory representation <a href="#pone.0001699-Jiang1">[11]</a>, <a href="#pone.0001699-Jiang2">[12]</a>. When the context of the item to report is changed (as in our case), retrieval is impaired. Only when attention is directed to the relevant item before the change occurs, it was observed that context changes did not affect performance. This alternative explanation could account for the higher performance in our <em>iconic-cue</em> and <em>retro-cue</em> conditions and a reduced performance in our <em>post-cue</em> condition (although, if VSTM is indeed limited to four items, this difference should have revealed it self as a lower-than-four capacity in the <em>post-cue</em> condition, not so much as a higher-than-four capacity in the <em>retro-cue</em> condition).</p>
<a id="article1.body1.sec2.sec3.p2" name="article1.body1.sec2.sec3.p2"></a><p>Here, we manipulated perceptual organization between memory and probe array to control for this alternative explanation; perceptual organization was either identical between memory and probe array (context+; <a href="#pone-0001699-g005"><strong>Fig. 5a</strong></a>), absent since only the cued item was shown (context0; <a href="#pone-0001699-g005"><strong>Fig. 5b</strong></a>) or disrupted since all non-cued items were changed between memory and probe array (context−; <a href="#pone-0001699-g005"><strong>Fig. 5c</strong></a>). We only measured the <em>retro-cue</em> and <em>post-cue</em> conditions in this experiment.</p>
<div class="figure" id="pone-0001699-g005"><div class="img"><a name="pone-0001699-g005" title="Click for larger image " href="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001699.g005&amp;representation=PNG_M" data-doi="info:doi/10.1371/journal.pone.0001699" data-uri="info:doi/10.1371/journal.pone.0001699.g005"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001699.g005&amp;representation=PNG_I" alt="thumbnail" class="thumbnail"></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001699.g005/powerpoint">
                    PPT
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001699.g005/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001699.g005/largerimage">
                    PNG
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001699.g005/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pone.0001699.g005.PNG_L"></span>)
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001699.g005/originalimage">
                    TIFF
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001699.g005/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pone.0001699.g005.TIF"></span>)
                </a></li></ul></div><p><strong>Figure 5.  <span><em>Influence of perceptual organization on change detection.</em></span></strong></p><a id="article1.body1.sec2.sec3.fig1.caption1.p1" name="article1.body1.sec2.sec3.fig1.caption1.p1"></a><p>A. Identical perceptual organization between memory and probe array. B. Perceptual organization is absent in probe array. C. Perceptual organization is disrupted between memory and probe array. D. A change in perceptual organization between memory and probe array does not influence the capacity of fragile VSTM, but it slightly reduces capacity of robust VSTM. Data are plotted as mean Cowan's K+SEM.</p>
<span>doi:10.1371/journal.pone.0001699.g005</span></div><a id="article1.body1.sec2.sec3.p3" name="article1.body1.sec2.sec3.p3"></a><p>A change in perceptual organization clearly influenced task performance in the <em>post-cue</em> condition (F(2,38) = 12.75, p&lt;.001), but not in the <em>retro-cue</em> condition (F(2, 38) = 1.181, p = .177) (<a href="#pone-0001699-g005"><strong>Fig. 5d</strong></a>). When we compared performance in the <em>retro-cue</em> condition with performance in the <em>post-cue</em> condition, the effect size increased as contextual information decreased (<strong>context+:</strong> <em>d</em> = 2.31; <strong>context0:</strong> <em>d</em> = 2.88; <strong>context−:</strong> <em>d</em> = 2.77). Thus, the difference in performance between the <em>post-cue</em> and the <em>retro-cue</em> conditions in the previous experiments was inflated by about 20 percent when the perceptual organization of the probe array is disrupted.</p>
<a id="article1.body1.sec2.sec3.p4" name="article1.body1.sec2.sec3.p4"></a><p>A change in the perceptual organization of the probe array reduces performance on a change detection task when attention is divided among multiple items as in our post-cue condition, but not when attention is focused on a single item (even when focusing of attention occurs retrospectively). Altogether, we conclude that differential use of context slightly inflates the capacity difference between the retro-cue and post-cue conditions, but the majority of the difference cannot be explained by this factor.</p>


<h4>Capacity of VSTM for complex objects</h4>
<a id="article1.body1.sec2.sec4.p1" name="article1.body1.sec2.sec4.p1"></a><p>In the previous control experiment, we assessed that differential grouping effects could explain about 20 percent of the difference in performance between <em>retro-cue</em> and <em>post-cue</em> conditions in Experiments 1 and 2. In the present experiment, more complex stimuli (either eight alphanumeric or eight horoscope characters; <a href="#pone-0001699-g006"><strong>Fig. 6a/b</strong></a>) were employed in a similar design to the previous experiments. These kinds of stimuli cannot be (easily) ‘chunked’. If performance in the <em>retro-cue</em> condition is about twice the performance in the <em>post-cue</em> condition for complex stimuli (as it was for simple stimuli in the previous experiments), this would yield additional evidence that the effects of ‘chunking’ in the previous experiments are minor.</p>
<div class="figure" id="pone-0001699-g006"><div class="img"><a name="pone-0001699-g006" title="Click for larger image " href="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001699.g006&amp;representation=PNG_M" data-doi="info:doi/10.1371/journal.pone.0001699" data-uri="info:doi/10.1371/journal.pone.0001699.g006"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001699.g006&amp;representation=PNG_I" alt="thumbnail" class="thumbnail"></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001699.g006/powerpoint">
                    PPT
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001699.g006/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001699.g006/largerimage">
                    PNG
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001699.g006/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pone.0001699.g006.PNG_L"></span>)
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001699.g006/originalimage">
                    TIFF
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001699.g006/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pone.0001699.g006.TIF"></span>)
                </a></li></ul></div><p><strong>Figure 6.  <span><em>Capacity of VSTM for complex objects.</em></span></strong></p><a id="article1.body1.sec2.sec4.fig1.caption1.p1" name="article1.body1.sec2.sec4.fig1.caption1.p1"></a><p>A. Memory array with alphanumeric stimuli. B. Memory array with horoscope stimuli. C. Cue display. D. Retro-cue condition measuring fragile VSTM E. Post-cue condition measuring robust VSTM. F. Capacity of fragile VSTM is about twice the capacity of robust VSTM regardless of stimulus complexity. Rectangle data are adopted from Exp. 1 with set size 8. Data are plotted as mean Cowan's K+SEM.</p>
<span>doi:10.1371/journal.pone.0001699.g006</span></div><a id="article1.body1.sec2.sec4.p2" name="article1.body1.sec2.sec4.p2"></a><p>We found superior performance for the <em>retro-cue</em> condition (<a href="#pone-0001699-g006"><strong>Fig. 6d</strong></a>) compared to the <em>post-cue</em> condition (<a href="#pone-0001699-g006"><strong>Fig. 6e</strong></a>) for both alphanumeric [t(9) = 7.09, p&lt;.001] and horoscope characters [t(9) = 5.38, p&lt;.001] (<a href="#pone-0001699-g006"><strong>Fig. 6f</strong></a>). For clarity, we have plotted the results of Experiment 1 with set size 8 in the same figure.</p>
<a id="article1.body1.sec2.sec4.p3" name="article1.body1.sec2.sec4.p3"></a><p>The capacity of fragile VSTM is always about twice the capacity of robust VSTM regardless of object type and complexity (as it was in Experiment 1 at set size 8). Capacity of both fragile and robust VSTM decreases as object complexity increases, which can be expected from previous experiments <a href="#pone.0001699-Alvarez1">[8]</a>–<a href="#pone.0001699-Olsson1">[10]</a>. We conclude, based on this experiment and the previous control experiment, that the high estimates for fragile VSTM capacity in Experiment 1 and 2 cannot be explained by differential ‘chunking’ mechanisms between conditions.</p>


<h4>Lifetime of VSTM representations</h4>
<a id="article1.body1.sec2.sec5.p1" name="article1.body1.sec2.sec5.p1"></a><p>We (previous sections) and others <a href="#pone.0001699-Griffin1">[14]</a>–<a href="#pone.0001699-Matsukura1">[20]</a> observed that performance in the <em>retro-cue</em> condition did not drop to the performance observed in the <em>post-cue</em> condition even after 1,000 ms after display off-set. Here, we increased cue latencies up to 5.5 s after display off-set (<a href="#pone-0001699-g007"><strong>fig. 7c/d</strong></a>) to find when <em>retro-cue</em> performance drops to the level of <em>post-cue</em> performance.</p>
<div class="figure" id="pone-0001699-g007"><div class="img"><a name="pone-0001699-g007" title="Click for larger image " href="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001699.g007&amp;representation=PNG_M" data-doi="info:doi/10.1371/journal.pone.0001699" data-uri="info:doi/10.1371/journal.pone.0001699.g007"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001699.g007&amp;representation=PNG_I" alt="thumbnail" class="thumbnail"></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001699.g007/powerpoint">
                    PPT
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001699.g007/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001699.g007/largerimage">
                    PNG
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001699.g007/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pone.0001699.g007.PNG_L"></span>)
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001699.g007/originalimage">
                    TIFF
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001699.g007/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pone.0001699.g007.TIF"></span>)
                </a></li></ul></div><p><strong>Figure 7.  <span>Lifetime of VSTM representations.</span></strong></p><a id="article1.body1.sec2.sec5.fig1.caption1.p1" name="article1.body1.sec2.sec5.fig1.caption1.p1"></a><p>A. Memory array. B. Cue display. C. Retro-cue condition measuring fragile VSTM with variable blank interval of 1000, 2500, 4000 or 5500 ms until cueing. D. Post-cue condition measuring robust VSTM with variable blank interval of 900, 2400, 3900 or 5400 ms until cueing. E. High-capacity, fragile VSTM decays linearly over time, whereas limited-capacity, robust VSTM is more or less durable. Data are plotted as mean Cowan's K+SEM.</p>
<span>doi:10.1371/journal.pone.0001699.g007</span></div><a id="article1.body1.sec2.sec5.p2" name="article1.body1.sec2.sec5.p2"></a><p>The high-capacity <em>retro-cue</em> performance decayed over time [F(1,19) = 102.61, p&lt;.001], and the limited-capacity <em>post-cue</em> performance was stable until four seconds after stimulus off-set [F(1,19) = .23, p = .64] (<a href="#pone-0001699-g007"><strong>Fig. 7e</strong></a>). Contrary to our expectations, we observed a drop in <em>post-cue</em> performance at the longest cue latency [F(3,17) = 8.524, p &lt;.01]. Performance was significantly higher at all cue latencies when a <em>retro-cue</em> was shown compared to when a <em>post-cue</em> was shown (smallest t-value [t(19) = 7.92, p&lt;.001]).</p>
<a id="article1.body1.sec2.sec5.p3" name="article1.body1.sec2.sec5.p3"></a><p>Fragile VSTM representations seem to exist for four seconds after stimulus off-set, at least. We are not sure if we can uniformly interpret the superior performance at the longest cue latency as evidence for the existence of fragile VSTM representations. At this cue latency, we see a drop-off in capacity for robust VSTM, possibly due to problems maintaining concentration. If subjects could maintain concentration for these long intervals, equal capacities for both stores might have been found. Thus, we conclude that fragile VSTM representations exist for a minimum of four seconds after stimulus off-set on top of robust VSTM.</p>

</div>

<div id="section3" class="section"><a id="s3" name="s3" toc="s3" title="Discussion"></a><h3>Discussion</h3><a id="article1.body1.sec3.p1" name="article1.body1.sec3.p1"></a><p>Traditional work on visual short-term memory (VSTM) suggests that we can be aware of four visual objects only <a href="#pone.0001699-Irwin1">[1]</a>–<a href="#pone.0001699-Vogel1">[7]</a>. Does this suggest that we build up a limited internal picture of the world? Or can it be that visual scenes are more fully represented on a neural level, but not completely transferred to a reportable stage <a href="#pone.0001699-Block1">[23]</a>–<a href="#pone.0001699-Lamme2">[25]</a>? To answer this question, we used a <em>change detection task</em> in which attention-directing cues are incorporated. These cues retrospectively indicate which item has to be attended. We found that human observers can represent and access more objects than they can keep in traditional visual short-term memory (VSTM) up to four seconds after disappearance of the visual scene. Moreover, this high representational capacity is not due to iconic memory and seems to depend on the complexity of the observed objects.</p>

<h4>Three stages in visual information processing</h4>
<a id="article1.body1.sec3.sec1.p1" name="article1.body1.sec3.sec1.p1"></a><p>By manipulating after-images and masks, we observed three stages in visual information processing; 1) iconic memory with unlimited capacity, 2) a long-lasting, but fragile form of VSTM with a capacity that is at least a factor 2 higher than the 3) robust form of VSTM that is clearly capacity-limited. Surprisingly, iconic memory representations seemed to depend on positive after-images of the previously shown image. When after-images were weak or when after-images were overwritten by flashes of light, iconic memory was found to be almost non-existent suggesting that it is primarily driven by persistent retinal activation beyond stimulus duration. The fragile form of VSTM was unaffected by the delivery of a light mask, but was completely overwritten to the level of robust VSTM by an irrelevant mask containing similar objects as the memory array. The capacity of both the fragile and the robust form of VSTM seemed to depend on stimulus complexity, which can be expected when we compare these results to previous findings <a href="#pone.0001699-Alvarez1">[8]</a>–<a href="#pone.0001699-Olsson1">[10]</a>.</p>


<h4>Chunking effects do not explain high-capacity measures of fragile VSTM</h4>
<a id="article1.body1.sec3.sec2.p1" name="article1.body1.sec3.sec2.p1"></a><p>As far as we know, this paper and the paper of Landman <a href="#pone.0001699-Landman1">[15]</a> are the first to show the existence of a high-capacity, but fragile VSTM store on top of robust VSTM. A commonly heard objection against the high-capacity results of Landman (and thus against our results) is that oriented rectangles were used in the paradigm and subjects could have grouped these objects to form fewer compound figures (‘chunking’), resulting in an apparent high capacity. Indeed, it was recently shown that grouping of these kinds of stimuli may occur automatically <a href="#pone.0001699-Jiang1">[11]</a>, and this principle could reduce our set sizes to some smaller number. However, we are firm that this cannot explain the high-capacity results.</p>
<a id="article1.body1.sec3.sec2.p2" name="article1.body1.sec3.sec2.p2"></a><p>First, chunking in itself would not account for the difference in capacity that is found between <em>retro-</em> and <em>post-cue</em> conditions. If subjects would chunk items, this would increase the capacity of both fragile and durable VSTM. We found a capacity of about four objects for durable VSTM (the post-cue condition), which is well in accordance with traditional estimates. Second, to counter chunking in the current experiments we rotated all irrelevant items between the memory and probe arrays. Employing a strategy of chunking in this case would be fully detrimental to performance in the post-cue condition, as a ‘change’ to the compound figure would always be detected, regardless of whether the cued item changed or not.</p>
<a id="article1.body1.sec3.sec2.p3" name="article1.body1.sec3.sec2.p3"></a><p>Of course it can be argued that our measure to counter chunking (rotating all irrelevant items between memory and probe arrays) could have introduced a difference in capacity between retro- and post-cue by itself (see <a href="#s2">results</a> section of experiment 3); it has been shown that when the context of the item to report is changed (as in our case), retrieval is impaired <a href="#pone.0001699-Jiang1">[11]</a>. Only when attention is focused on one item, context changes do not affect performance. This could account for a reduced performance in our <em>post-cue</em> condition compared to the <em>iconic-</em> and <em>retro-cue</em> conditions. We performed an additional experiment in which we manipulated perceptual organization between memory and probe array to test this alternative explanation. We found that differential use of context slightly inflates the capacity difference between the retro-cue and post-cue conditions, but the majority of the difference cannot be explained by this factor.</p>
<a id="article1.body1.sec3.sec2.p4" name="article1.body1.sec3.sec2.p4"></a><p>Finally, in Experiment 4 we employed complex stimuli that cannot be (easily) chunked. The capacity of fragile VSTM still was about twice the capacity of robust VSTM (as it was for oriented rectangles in Experiment 1). Altogether, it seems unlikely that the high-capacity findings found here and in the paper of Landman are due to grouping mechanisms.</p>


<h4>Can we equate fragile VSTM to a form of iconic memory?</h4>
<a id="article1.body1.sec3.sec3.p1" name="article1.body1.sec3.sec3.p1"></a><p>We make a tri-partite division between iconic memory, fragile VSTM, and durable VSTM. However, these results can also be explained by pleading for a dissociation of iconic memory in a <em>retinal</em> and a <em>cortical</em> icon (and traditional, capacity limited VSTM). This interpretation resembles earlier theoretical claims of Coltheart <a href="#pone.0001699-Coltheart1">[26]</a> that iconic memory might consist of both 1) a visible persistent component (alike our finding that iconic memory resembles a positive after-image) and, 2) an informational persistent component (akin to our finding of additional information in the <em>retro-cue</em> condition compared to the <em>post-cue</em> condition). There are two arguments that prevent us from drawing this conclusion. First, our <em>retro-cues</em> were presented well beyond the time period in which iconic memory effects are traditionally found. In addition, a recent study <a href="#pone.0001699-Makovski2">[19]</a> found that items in fragile VSTM are not stored in a retinotopic way, but in a spatiotopic way. On the premise that iconic memory is a retinotopic phenomenon, it seems hard to reconcile this property with iconic memory, but not with VSTM. Still, these arguments can be quelled based on the available literature.</p>
<a id="article1.body1.sec3.sec3.p2" name="article1.body1.sec3.sec3.p2"></a><p>In traditional iconic memory paradigms, items are only shown once and here items are shown twice: once during encoding and once during report. It is well known that errors in iconic memory are location errors and not intrusion errors, suggesting that the location of items is lost over time and not the identity of the objects <a href="#pone.0001699-Dick1">[27]</a>–<a href="#pone.0001699-Mewhort1">[29]</a>. Our paradigm very much reduces spatial uncertainties (by showing items twice at the same location) and we can, therefore, presume that it could capture iconic effects for a longer period. Also, some evidence exists <a href="#pone.0001699-McRae1">[30]</a> that iconic memory might consist of a fast, retinotopic buffer followed by a relatively slow, spatiotopic buffer in which the spatial relations among visual information is represented. However, the effects found in that experiment were small, and other authors have not found these effects. Altogether, it remains speculative whether we can equate fragile VSTM to a form of VSTM. Yet, this approach is interesting since it relates to a current controversy in conscious vision (see next section).</p>


<h4>A fleeting form of visual awareness without direct report</h4>
<a id="article1.body1.sec3.sec4.p1" name="article1.body1.sec3.sec4.p1"></a><p>Neurophysiologic findings suggest that we can discern two modes of visual processing; the feed forward sweep (FFS), and recurrent processing (RP) <a href="#pone.0001699-Lamme3">[31]</a>. By selectively disrupting RP, but leaving FFS intact it is observed that visual awareness never arises. This was shown by backward masking <a href="#pone.0001699-Lamme4">[32]</a>, by applying transcranial magnetic stimulation (TMS) to V1 <a href="#pone.0001699-Jolij1">[33]</a>, <a href="#pone.0001699-PascualLeone1">[34]</a>, and by inactivating higher visual areas <a href="#pone.0001699-Hupe1">[35]</a>, <a href="#pone.0001699-Lamme5">[36]</a>. Even when there are sudden lapses in awareness, it is observed that RP is absent, whereas FFS is intact <a href="#pone.0001699-Super1">[37]</a>.</p>
<a id="article1.body1.sec3.sec4.p2" name="article1.body1.sec3.sec4.p2"></a><p>While RP thus seems to be necessary for conscious perception, current controversy hinges on the question whether RP is sufficient for conscious perception <a href="#pone.0001699-Block1">[23]</a>–<a href="#pone.0001699-Lamme2">[25]</a>, or that consciousness only occurs in the case of widespread RP, which includes areas necessary for cognitive access and control, such as the prefrontal cortex <a href="#pone.0001699-Dehaene1">[38]</a>. What happens when RP is limited to the visual and temporal cortex? Do we have conscious experience without report or no experience at all?</p>
<a id="article1.body1.sec3.sec4.p3" name="article1.body1.sec3.sec4.p3"></a><p>Experiments on iconic memory and fragile VSTM are interesting exactly because of this controversy. Just as robust VSTM forms a window on reportable and directly accessible conscious percepts, iconic memory and fragile VSTM could form a window on ‘perception without immediate cognitive access’. Only when attention is re-directed to the right location, representations can presumably ‘jump’ over the report threshold.</p>
<a id="article1.body1.sec3.sec4.p4" name="article1.body1.sec3.sec4.p4"></a><p>There are two key issues which need to be addressed. The first would be to establish the perceptual rather than unconscious nature of these kinds of representations, and the evidence for this is growing. Previous experiments showed that objects in fragile VSTM are processed up to the level of figure-ground organization <a href="#pone.0001699-Landman2">[39]</a>, and that features are perceptually bound into coherent object representations <a href="#pone.0001699-Landman1">[15]</a>. The second issue is to establish a link between fragile VSTM (and iconic memory) and recurrent cortical processing. Our current results provide some evidence for this latter issue, by showing evidence for a long-lasting, i.e. reverberating nature. Still, neurophysiologic measures will have to confirm this link.</p>

</div>

<div id="section4" class="section"><a id="s4" name="s4" toc="s4" title="Materials and Methods"></a><h3>Materials and Methods</h3>
<h4>Participants</h4>
<a id="article1.body1.sec4.sec1.p1" name="article1.body1.sec4.sec1.p1"></a><p>Ten right-handed young adults (5 females) participated in Experiment 1, 2 and 4; 40 right-handed young adults (25 females) in Experiment 3; and 20 right-handed young adults (16 females) in Experiment 5. All subjects had normal or corrected-to-normal vision and no colour deficiencies and they participated as part of a study course or for financial compensation. All subjects gave their written informed consent to participate in either one experiment. All experiments were approved by the local ethics committee of the department of Psychology of the University of Amsterdam.</p>


<h4>Equipment</h4>
<a id="article1.body1.sec4.sec2.p1" name="article1.body1.sec4.sec2.p1"></a><p>All experiments were done on a 19 inch LG CRT-display (type FB915BP) at a refresh rate of 100 Hz. We measured phosphor persistence of the display using a photo-cell placed at the centre of the screen. Presented data are averages of 100 trials of single frames of pure white light (87.66 cd/m<sup>2</sup>) (see <a href="#pone-0001699-g008"><strong>Figure 8</strong></a>). Each single-frame presentation was followed by a 200-ms blank period. It was observed that the phosphors returned to baseline activity approximately 6.4 ms after their peak amplitude. For all experiments, we used Presentation version 9.7 (NeuroBehavioral Systems, Inc.) to display our stimulation on the monitor.</p>
<div class="figure" id="pone-0001699-g008"><div class="img"><a name="pone-0001699-g008" title="Click for larger image " href="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001699.g008&amp;representation=PNG_M" data-doi="info:doi/10.1371/journal.pone.0001699" data-uri="info:doi/10.1371/journal.pone.0001699.g008"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001699.g008&amp;representation=PNG_I" alt="thumbnail" class="thumbnail"></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001699.g008/powerpoint">
                    PPT
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001699.g008/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001699.g008/largerimage">
                    PNG
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001699.g008/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pone.0001699.g008.PNG_L"></span>)
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001699.g008/originalimage">
                    TIFF
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001699.g008/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pone.0001699.g008.TIF"></span>)
                </a></li></ul></div><p><strong>Figure 8.  <span>Phosphor persistence of CRT monitor.</span></strong></p><a id="article1.body1.sec4.sec2.fig1.caption1.p1" name="article1.body1.sec4.sec2.fig1.caption1.p1"></a><p>Phosphor persistence lasts approximately 6.4 ms after peak amplitude. Data are averages of 100 trials of single frame presentations of pure white light (87.66 cd/m<sup>2</sup>) presented at a refresh rate of 100 Hz. Each single frame presentation of light was followed by a 200-ms blank period.</p>
<span>doi:10.1371/journal.pone.0001699.g008</span></div>

<h4>Experimental paradigm</h4>
<h5>Experiment 1.</h5><a id="article1.body1.sec4.sec3.sec1.p1" name="article1.body1.sec4.sec3.sec1.p1"></a><p>Stimulus displays consisted of grids of 36 locations that were each 2°×2° in size; total grid size 12°×12°. The centre four grid locations were always empty. Each display consisted of 4, 8, 16, or 32 rectangles with either a horizontal or a vertical orientation. Individual rectangles were 1.56°×0.39° in size and presented randomly at the centre of the grid locations–except for the 32 figures condition in which all locations were filled. All displays were composed of either pure white rectangles (87.66 cd/m<sup>2</sup>) on a pure black background (0.01 cd/m<sup>2</sup>; <a href="#pone-0001699-g001"><strong>Fig. 1a</strong></a>)-or of red rectangles on an isoluminant gray background (both 13.52 cd/m<sup>2</sup>; <a href="#pone-0001699-g001"><strong>Fig. 1b</strong></a>). Cues were composed of four white triangles–each 0.09°×0.09° in size-placed at the edges of a grid location (<a href="#pone-0001699-g001"><strong>Fig. 1c</strong></a>). Subjects were seated 100 cm from a 19-inch display, which spanned 20.4 by 15.4 degrees of visual angle.</p>
<a id="article1.body1.sec4.sec3.sec1.p2" name="article1.body1.sec4.sec3.sec1.p2"></a><p>On each trial, we showed a 250-ms memory array containing 4 to 32 oriented rectangles. We instructed the subjects to remember as many oriented rectangles of the memory array as possible. On each trial, one rectangle was cued to indicate which item to report. After some delay, a probe array was shown and subjects were asked to indicate by button press whether the cued item had the same or different (50–50) orientation as the one shown in the memory array. Probe arrays were present until subjects made a response. All non-cued items were rotated by 90° to prevent subjects to use a strategy of encoding items in chunks. Cues were introduced at different latencies during the trial; either 10 ms after off-set of the memory array (<em>iconic-cue</em>; <a href="#pone-0001699-g001"><strong>Fig. 1d</strong></a>), 1,000 ms after off-set of the memory array (<em>retro-cue</em>; <a href="#pone-0001699-g001"><strong>Fig. 1e</strong></a>), or 100 ms after on-set of the probe array (<em>post-cue</em>; <a href="#pone-0001699-g001"><strong>Fig. 1f</strong></a>). The interval between memory and probe array was 2000 ms for the <em>iconic-cue</em> and <em>retro-cue</em> conditions and 900 ms for the <em>post-cue</em> conditions. In effect, the <em>retro-cues</em> and <em>post-cues</em> were given at the same latency after memory array off-set ruling out differences in capacity due to a differential interval in which subjects had to remember all objects. Cue conditions of particular set sizes were presented in separate blocks of 64 trials each.</p>

<h5>Experiment 2.</h5><a id="article1.body1.sec4.sec3.sec2.p1" name="article1.body1.sec4.sec3.sec2.p1"></a><p>Here, rectangles could have one of four possible orientations; horizontal, vertical, 45° to the vertical, and 135° to the vertical (<a href="#pone-0001699-g003"><strong>Fig. 3a</strong></a>). Also, masks were introduced. Light masks were composed of uniform full-screen displays in red (13.52 cd/m<sup>2</sup>) or white (87.66 cd/m<sup>2</sup>) (<a href="#pone-0001699-g003"><strong>Fig. 3b</strong></a>). Pattern masks were identical to the shown memory arrays with all elements at the same location, only orientation of individual rectangles was randomly re-assigned (<a href="#pone-0001699-g003"><strong>Fig. 3c</strong></a>). We introduced a 10-ms light mask before the <em>iconic-cue</em> (<a href="#pone-0001699-g003"><strong>Fig. 3d</strong></a>), a 250-ms light mask before the <em>retro-cue</em> or a 250-ms pattern mask before the <em>retro-cue</em> (<a href="#pone-0001699-g003"><strong>Fig. 3e</strong></a>). Subjects were informed of the presence of mask displays and were instructed to ignore them. All other details were identical to Experiment 1.</p>

<h5>Experiment 3.</h5><a id="article1.body1.sec4.sec3.sec3.p1" name="article1.body1.sec4.sec3.sec3.p1"></a><p>Displays consisted of eight rectangles–spanning 1.56°×0.39°-placed radially at 4 degrees of visual angle around the fixation point. The exact location of each rectangle was randomly jittered by half degree of visual angle towards the centre or the periphery. Rectangles could have one of four possible orientations; horizontal, vertical, 45° to the vertical, and 135° to the vertical. Only white rectangles (87.66 cd/m<sup>2</sup>) on a black background (0.01 cd/m<sup>2</sup>) were used. Cues consisted of a 3-pixel thick line which was at one end close (~0.7°) to fixation and at the other end close (average ~1.2°) to the critical item. We manipulated perceptual organization between memory and probe array; perceptual organization was either identical between memory and probe array (context+; <a href="#pone-0001699-g005"><strong>Fig. 5a</strong></a>), absent since only the cued item was shown (context0; <a href="#pone-0001699-g005"><strong>Fig. 5b</strong></a>) or disrupted since all non-cued items were changed between memory and probe array (context−; <a href="#pone-0001699-g005"><strong>Fig. 5c</strong></a>). We only measured the <em>retro-cue</em> and <em>post-cue</em> conditions in this experiment. All other details were identical to Experiment 1.</p>

<h5>Experiment 4.</h5><a id="article1.body1.sec4.sec3.sec4.p1" name="article1.body1.sec4.sec3.sec4.p1"></a><p>Displays consisted of either eight different alphanumerical symbols from a pool of 18 items (B,D,F,G,H,J,K,L,M,1,2,3,4,5,6,7,8,9) or eight different astrological symbols from a pool of 11 items (we excluded the symbol scorpio since it is very similar to the symbol virgo) placed radially at 4 degrees of visual angle around the fixation point (<a href="#pone-0001699-g006"><strong>Fig. 6a/b</strong></a>). All symbols were presented at font size 64 in white (87.66 cd/m<sup>2</sup>) on a black background (0.01 cd/m<sup>2</sup>). Only the <em>retro-cue</em> condition (<a href="#pone-0001699-g006"><strong>Fig. 6d</strong></a>) and the <em>post-cue</em> condition (<a href="#pone-0001699-g006"><strong>Fig. 6e</strong></a>) were presented. All non-cued items were not changed between memory and probe array. All other details were identical to Experiment 3.</p>

<h5>Experiment 5.</h5><a id="article1.body1.sec4.sec3.sec5.p1" name="article1.body1.sec4.sec3.sec5.p1"></a><p>Displays consisted of eight rectangles–spanning 1.56°×0.39°-placed radially at 4 degrees of visual angle around the fixation point (<a href="#pone-0001699-g007"><strong>Fig. 7a</strong></a>). We presented only the <em>retro-cue</em> and <em>post-cue</em> conditions, and we varied the blank interval between memory and probe array between 1,000 and 5,500 ms for <em>retro-cue</em> conditions (<a href="#pone-0001699-g007"><strong>Fig. 7c</strong></a>) and between 900 and 5,400 ms for <em>post-cue</em> conditions (<a href="#pone-0001699-g007"><strong>Fig. 7d</strong></a>) in steps of 1,500 ms. All non-cued items were not changed between memory and probe array. All other details were identical to Experiment 3.</p>



<h4>Procedure</h4>
<h5>Experiment 1 &amp; 2.</h5><a id="article1.body1.sec4.sec4.sec1.p1" name="article1.body1.sec4.sec4.sec1.p1"></a><p>Subjects were heavily trained on the task in a separate 3-hour session before entering the experimental sessions. In the practice session, all conditions at all set sizes were practiced at least once for the high-contrast stimuli, and subjects were allowed to practice blocks more than once when they indicated that they could have attained higher performance. Results obtained from subjects during the training sessions were qualitatively similar in the sense that iconic memory capacity was always much higher than working memory capacity. Training increased capacity for all conditions up to the ceiling levels reported in the results section. After the practice session, subjects participated in the experimental session with high-contrast stimuli first and subsequently in the session with isoluminant stimuli; this procedure was counterbalanced over subjects. Subjects were instructed to maintain fixation throughout the entire experiment, and they were encouraged to indicate changes only if they were certain that a change had occurred. The experiment was done in a darkened room to increase the strength of the after-images <a href="#pone.0001699-Adelson1">[22]</a>.</p>

<h5>Experiment 3.</h5><a id="article1.body1.sec4.sec4.sec2.p1" name="article1.body1.sec4.sec4.sec2.p1"></a><p>Subjects were either assigned to the <em>retro-cue</em> condition first or to the <em>post-cue</em> condition first in a counterbalanced fashion. All different perceptual organizations (context+, context0, context−) were presented randomly intermixed within a block consisting of 48 trials. After doing a block of one condition (f.i. the retro-cue condition), subjects did a block of the other condition. This sequence was repeated five times, and the first block of each condition was discarded in the analysis since it functioned as a training block. Thus, subjects performed a total of 192 trials in each condition. All other details were identical to Experiment 1.</p>

<h5>Experiment 4.</h5><a id="article1.body1.sec4.sec4.sec3.p1" name="article1.body1.sec4.sec4.sec3.p1"></a><p>Subjects were either assigned to the <em>retro-cue</em> condition first or to the <em>post-cue</em> condition first in a counterbalanced fashion. Alphanumerical versions were always performed first followed by the horoscope versions. This sequence was repeated three times, resulting in three sessions of 48 trials for each condition. The first block of each condition was discarded in the analysis since it functioned as a training block. All other details were identical to Experiment 1.</p>

<h5>Experiment 5.</h5><a id="article1.body1.sec4.sec4.sec4.p1" name="article1.body1.sec4.sec4.sec4.p1"></a><p>Subjects practiced the <em>retro-cue</em> condition with an ISI of 1,000 ms and the <em>post-cue</em> condition with an ISI of 900 ms for 48 trials each. Subsequently, they entered the experimental condition in which they performed 48 trials on each condition. All conditions were randomly intermixed throughout the entire experiment. All other details were identical to Experiment 1.</p>



<h4>Data analysis</h4>
<a id="article1.body1.sec4.sec5.p1" name="article1.body1.sec4.sec5.p1"></a><p>We computed memory capacity measures using a formula developed by Cowan <a href="#pone.0001699-Cowan1">[21]</a>. The formula is <em>K</em> = (hit rate–0.5+correct rejection rate–0.5)*<em>N</em>, and gives an estimate of the representational capacity and corrects for guessing trials. All statistical analyses were performed with repeated measures ANOVAS. In some instances, we tested specific differences with paired t-tests.</p>

</div>





<div><a id="ack" name="ack" toc="ack" title="Acknowledgments"></a><h3>Acknowledgments</h3>
<a id="article1.back1.ack1.p1" name="article1.back1.ack1.p1"></a><p>We wish to thank Ralf Cornelissen, Sanne Huijsmans, Ellen de Jong and Taketo Pang for conducting several experiments and Olympia Colizoli for proof-reading the article.</p>
</div><div class="contributions"><a id="authcontrib" name="authcontrib" toc="authcontrib" title="Author Contributions"></a><h3>Author Contributions</h3><p>Conceived and designed the experiments: VL IS HS. Performed the experiments: IS. Analyzed the data: IS. Wrote the paper: VL IS.</p></div><div><a id="references" name="references" toc="references" title="References"></a><h3>References</h3><ol class="references"><li><span class="label">1.
              </span><a name="pone.0001699-Irwin1" id="pone.0001699-Irwin1"></a>Irwin DE (1991) Information integration across saccadic eye movements. Cognit Psychol  23: 420–456.  <ul class="find" data-citedArticleID="1064970" data-doi="10.1016/0010-0285(91)90015-g"><li><a href="http://dx.doi.org/10.1016/0010-0285(91)90015-g" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Information+integration+across+saccadic+eye+movements." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Information+integration+across+saccadic+eye+movements.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">2.
              </span><a name="pone.0001699-Irwin2" id="pone.0001699-Irwin2"></a>Irwin DE (1996) Integrating information across saccadic eye movements. Curr Dir in Psychol Sci  5: 94–100.  <ul class="find" data-citedArticleID="1064972" data-doi="10.1111/1467-8721.ep10772833"><li><a href="http://dx.doi.org/10.1111/1467-8721.ep10772833" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Integrating+information+across+saccadic+eye+movements." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Integrating+information+across+saccadic+eye+movements.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">3.
              </span><a name="pone.0001699-Luck1" id="pone.0001699-Luck1"></a>Luck SJ, Vogel EK (1997) The capacity of visual working memory for features and conjunctions. Nature  390: 279–281.  <ul class="find" data-citedArticleID="1064998"><li><a href="http://www.crossref.org/guestquery/?auth2=&amp;atitle2=The+capacity+of+visual+working+memory+for+features+and+conjunctions.&amp;auth=&amp;atitle=The+capacity+of+visual+working+memory+for+features+and+conjunctions." target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=The+capacity+of+visual+working+memory+for+features+and+conjunctions." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22The+capacity+of+visual+working+memory+for+features+and+conjunctions.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">4.
              </span><a name="pone.0001699-Parr1" id="pone.0001699-Parr1"></a>Parr WV (1992) Delayed matching-to-sample performance as a measure of human visuospatial working memory. B Psychonomic Soc  30: 369–372.  <ul class="find" data-citedArticleID="1065012" data-doi="10.3758/bf03334092"><li><a href="http://dx.doi.org/10.3758/bf03334092" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Delayed+matching-to-sample+performance+as+a+measure+of+human+visuospatial+working+memory." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Delayed+matching-to-sample+performance+as+a+measure+of+human+visuospatial+working+memory.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">5.
              </span><a name="pone.0001699-Pashler1" id="pone.0001699-Pashler1"></a>Pashler H (1988) Familiarity and visual change detection. Percept Psychophys  44: 369–378.  <ul class="find" data-citedArticleID="1065016" data-doi="10.3758/bf03210419"><li><a href="http://dx.doi.org/10.3758/bf03210419" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Familiarity+and+visual+change+detection." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Familiarity+and+visual+change+detection.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">6.
              </span><a name="pone.0001699-Phillips1" id="pone.0001699-Phillips1"></a>Phillips WA (1974) On the distinction between sensory storage and short-term visual memory. Percept Psychophys  16: 283–290.  <ul class="find" data-citedArticleID="1065018" data-doi="10.3758/bf03203943"><li><a href="http://dx.doi.org/10.3758/bf03203943" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=On+the+distinction+between+sensory+storage+and+short-term+visual+memory." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22On+the+distinction+between+sensory+storage+and+short-term+visual+memory.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">7.
              </span><a name="pone.0001699-Vogel1" id="pone.0001699-Vogel1"></a>Vogel EK, Woodman GF, Luck SJ (2001) Storage of features, conjunctions and objects in visual working memory. J Exp Psychol Hum Percept Perform  27: 92–114.  <ul class="find" data-citedArticleID="1065026" data-doi="10.1037//0096-1523.27.1.92"><li><a href="http://dx.doi.org/10.1037//0096-1523.27.1.92" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Storage+of+features%2C+conjunctions+and+objects+in+visual+working+memory." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Storage+of+features%2C+conjunctions+and+objects+in+visual+working+memory.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">8.
              </span><a name="pone.0001699-Alvarez1" id="pone.0001699-Alvarez1"></a>Alvarez GA, Cavanagh P (2004) The capacity of visual short-term memory is set both by visual information load and by number of objects. Psychol Sci  15: 106–111.  <ul class="find" data-citedArticleID="1064952" data-doi="10.1111/j.0963-7214.2004.01502006.x"><li><a href="http://dx.doi.org/10.1111/j.0963-7214.2004.01502006.x" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=The+capacity+of+visual+short-term+memory+is+set+both+by+visual+information+load+and+by+number+of+objects." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22The+capacity+of+visual+short-term+memory+is+set+both+by+visual+information+load+and+by+number+of+objects.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">9.
              </span><a name="pone.0001699-Eng1" id="pone.0001699-Eng1"></a>Eng HY, Chen D, Jiang Y (2005) Visual working memory for simple and complex visual stimuli. Psychon Bull Rev  12: 1127–1133.  <ul class="find" data-citedArticleID="1064964" data-doi="10.3758/bf03206454"><li><a href="http://dx.doi.org/10.3758/bf03206454" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Visual+working+memory+for+simple+and+complex+visual+stimuli." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Visual+working+memory+for+simple+and+complex+visual+stimuli.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">10.
              </span><a name="pone.0001699-Olsson1" id="pone.0001699-Olsson1"></a>Olsson H, Poom L (2005) Visual memory needs categories. Proc Natl Acad Sci U S A  102: 8776–8780.  <ul class="find" data-citedArticleID="1065010" data-doi="10.1073/pnas.0500810102"><li><a href="http://dx.doi.org/10.1073/pnas.0500810102" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Visual+memory+needs+categories." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Visual+memory+needs+categories.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">11.
              </span><a name="pone.0001699-Jiang1" id="pone.0001699-Jiang1"></a>Jiang Y, Chun MM, Olson IR (2004) Perceptual grouping in change detection. Percept Psychophys  66: 446–453.  <ul class="find" data-citedArticleID="1064974" data-doi="10.3758/bf03194892"><li><a href="http://dx.doi.org/10.3758/bf03194892" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Perceptual+grouping+in+change+detection." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Perceptual+grouping+in+change+detection.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">12.
              </span><a name="pone.0001699-Jiang2" id="pone.0001699-Jiang2"></a>Jiang Y, Olson IR, Chun MM (2000) Organization of visual short-term memory. J Exp Psychol Learn Mem Cogn  26: 683–702.  <ul class="find" data-citedArticleID="1064976" data-doi="10.1037//0278-7393.26.3.683"><li><a href="http://dx.doi.org/10.1037//0278-7393.26.3.683" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Organization+of+visual+short-term+memory." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Organization+of+visual+short-term+memory.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">13.
              </span><a name="pone.0001699-Sperling1" id="pone.0001699-Sperling1"></a>Sperling G (1960) The information available in brief visual presentations. Psychol monogr  74: 1–29.  <ul class="find" data-citedArticleID="1065020" data-doi="10.1037/h0093759"><li><a href="http://dx.doi.org/10.1037/h0093759" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=The+information+available+in+brief+visual+presentations." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22The+information+available+in+brief+visual+presentations.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">14.
              </span><a name="pone.0001699-Griffin1" id="pone.0001699-Griffin1"></a>Griffin IC, Nobre AC (2003) Orienting attention to locations in internal representations. J Cogn Neurosci  15: 1176–1194.  <ul class="find" data-citedArticleID="1064966" data-doi="10.1162/089892903322598139"><li><a href="http://dx.doi.org/10.1162/089892903322598139" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Orienting+attention+to+locations+in+internal+representations." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Orienting+attention+to+locations+in+internal+representations.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">15.
              </span><a name="pone.0001699-Landman1" id="pone.0001699-Landman1"></a>Landman R, Spekreijse H, Lamme VAF (2003) Large capacity storage of integrated objects before change blindness. Vision Res  43: 149–164.  <ul class="find" data-citedArticleID="1064990" data-doi="10.1016/s0042-6989(02)00402-9"><li><a href="http://dx.doi.org/10.1016/s0042-6989(02)00402-9" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Large+capacity+storage+of+integrated+objects+before+change+blindness." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Large+capacity+storage+of+integrated+objects+before+change+blindness.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">16.
              </span><a name="pone.0001699-Lepsien1" id="pone.0001699-Lepsien1"></a>Lepsien J, Griffin IC, Devlin JT, Nobre AC (2005) Directing spatial attention in mental representations: Interactions between attentional orienting and working-memory load. Neuroimage  26: 733–743.  <ul class="find" data-citedArticleID="1064994" data-doi="10.1016/j.neuroimage.2005.02.026"><li><a href="http://dx.doi.org/10.1016/j.neuroimage.2005.02.026" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Directing+spatial+attention+in+mental+representations%3A+Interactions+between+attentional+orienting+and+working-memory+load." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Directing+spatial+attention+in+mental+representations%3A+Interactions+between+attentional+orienting+and+working-memory+load.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">17.
              </span><a name="pone.0001699-Lepsien2" id="pone.0001699-Lepsien2"></a>Lepsien J, Nobre AC (2007) Attentional modulation of object representations in working memory. Cereb Cortex  17: 2072–2083.  <ul class="find" data-citedArticleID="1064996" data-doi="10.1093/cercor/bhl116"><li><a href="http://dx.doi.org/10.1093/cercor/bhl116" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Attentional+modulation+of+object+representations+in+working+memory." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Attentional+modulation+of+object+representations+in+working+memory.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">18.
              </span><a name="pone.0001699-Makovski1" id="pone.0001699-Makovski1"></a>Makovski T, Jiang YV (2007) Distributing versus focusing attention in visual short-term memory Psychonomic Bulletin &amp; Review  14: 1072–1078.  <ul class="find" data-citedArticleID="1065000" data-doi="10.3758/bf03193093"><li><a href="http://dx.doi.org/10.3758/bf03193093" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Distributing+versus+focusing+attention+in+visual+short-term+memory" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Distributing+versus+focusing+attention+in+visual+short-term+memory%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">19.
              </span><a name="pone.0001699-Makovski2" id="pone.0001699-Makovski2"></a>Makovski T, Sussman R, Jiang YV (In press) Orienting attention in visual working memory reduces interference from memory probes. Journal of Experimental Psychology: Learning, Memory, &amp; Cognition.   <ul class="find" data-citedArticleID="1065002" data-doi="10.1037/0278-7393.34.2.369"><li><a href="http://dx.doi.org/10.1037/0278-7393.34.2.369" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Orienting+attention+in+visual+working+memory+reduces+interference+from+memory+probes." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Orienting+attention+in+visual+working+memory+reduces+interference+from+memory+probes.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">20.
              </span><a name="pone.0001699-Matsukura1" id="pone.0001699-Matsukura1"></a>Matsukura MJLS, Vecera SP (2007) Attention effects during visual short-term memory maintenance: Protection or prioritization? Percept Psychophys  69: 1422–1434.  <ul class="find" data-citedArticleID="1065004" data-doi="10.3758/bf03192957"><li><a href="http://dx.doi.org/10.3758/bf03192957" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Attention+effects+during+visual+short-term+memory+maintenance%3A+Protection+or+prioritization%3F" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Attention+effects+during+visual+short-term+memory+maintenance%3A+Protection+or+prioritization%3F%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">21.
              </span><a name="pone.0001699-Cowan1" id="pone.0001699-Cowan1"></a>Cowan N (2001) The magical number 4 in short-term memory: a reconsideration of mental storage capacity. Behav Brain Sci  24: 87–114.  discussion 114–185.  <ul class="find" data-citedArticleID="1064958" data-doi="10.1017/s0140525x01003922"><li><a href="http://dx.doi.org/10.1017/s0140525x01003922" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=The+magical+number+4+in+short-term+memory%3A+a+reconsideration+of+mental+storage+capacity." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22The+magical+number+4+in+short-term+memory%3A+a+reconsideration+of+mental+storage+capacity.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">22.
              </span><a name="pone.0001699-Adelson1" id="pone.0001699-Adelson1"></a>Adelson EH (1978) Iconic storage: the role of rods. Science  201: 544–546.  <ul class="find" data-citedArticleID="1064950" data-doi="10.1126/science.663675"><li><a href="http://dx.doi.org/10.1126/science.663675" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Iconic+storage%3A+the+role+of+rods." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Iconic+storage%3A+the+role+of+rods.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">23.
              </span><a name="pone.0001699-Block1" id="pone.0001699-Block1"></a>Block N (2005) Two neural correlates of consciousness. Trends Cogn Sci  9: 46–52.  <ul class="find" data-citedArticleID="1064954" data-doi="10.1016/j.tics.2004.12.006"><li><a href="http://dx.doi.org/10.1016/j.tics.2004.12.006" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Two+neural+correlates+of+consciousness." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Two+neural+correlates+of+consciousness.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">24.
              </span><a name="pone.0001699-Lamme1" id="pone.0001699-Lamme1"></a>Lamme VA (2003) Why visual attention and awareness are different. Trends Cogn Sci  7: 12–18.  <ul class="find" data-citedArticleID="1064980" data-doi="10.1016/s1364-6613(02)00013-x"><li><a href="http://dx.doi.org/10.1016/s1364-6613(02)00013-x" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Why+visual+attention+and+awareness+are+different." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Why+visual+attention+and+awareness+are+different.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">25.
              </span><a name="pone.0001699-Lamme2" id="pone.0001699-Lamme2"></a>Lamme VA (2006) Towards a true neural stance on consciousness. Trends Cogn Sci  10: 494–501.  <ul class="find" data-citedArticleID="1064982" data-doi="10.1016/j.tics.2006.09.001"><li><a href="http://dx.doi.org/10.1016/j.tics.2006.09.001" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Towards+a+true+neural+stance+on+consciousness." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Towards+a+true+neural+stance+on+consciousness.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">26.
              </span><a name="pone.0001699-Coltheart1" id="pone.0001699-Coltheart1"></a>Coltheart M (1980) Iconic memory and visible persistence. Percept Psychophys  27: 183–228.  <ul class="find" data-citedArticleID="1064956" data-doi="10.3758/bf03204258"><li><a href="http://dx.doi.org/10.3758/bf03204258" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Iconic+memory+and+visible+persistence." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Iconic+memory+and+visible+persistence.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">27.
              </span><a name="pone.0001699-Dick1" id="pone.0001699-Dick1"></a>Dick AO (1969) J Exp Psychol 82: 279–284.  <ul class="find" data-citedArticleID="1064962"><li><a href="http://www.crossref.org/guestquery/?auth2=&amp;atitle2=J+Exp+Psychol&amp;auth=&amp;atitle=J+Exp+Psychol" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=J+Exp+Psychol" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22J+Exp+Psychol%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">28.
              </span><a name="pone.0001699-Townsend1" id="pone.0001699-Townsend1"></a>Townsend VM (1973) Loss of spatial and identity information following a tachistoscopic exposure. J Exp Psychol  98: 113–118.  <ul class="find" data-citedArticleID="1065024" data-doi="10.1037/h0034309"><li><a href="http://dx.doi.org/10.1037/h0034309" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Loss+of+spatial+and+identity+information+following+a+tachistoscopic+exposure." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Loss+of+spatial+and+identity+information+following+a+tachistoscopic+exposure.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">29.
              </span><a name="pone.0001699-Mewhort1" id="pone.0001699-Mewhort1"></a>Mewhort DJ, Campbell AJ, Marchetti FM, Campbell JI (1981) Identification, localization, and “iconic memory”: an evaluation of the bar-probe task. Mem Cognit  9: 50–67.  <ul class="find" data-citedArticleID="1065008" data-doi="10.3758/bf03196951"><li><a href="http://dx.doi.org/10.3758/bf03196951" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Identification%2C+localization%2C+and+%E2%80%9Ciconic+memory%E2%80%9D%3A+an+evaluation+of+the+bar-probe+task." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Identification%2C+localization%2C+and+%E2%80%9Ciconic+memory%E2%80%9D%3A+an+evaluation+of+the+bar-probe+task.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">30.
              </span><a name="pone.0001699-McRae1" id="pone.0001699-McRae1"></a>McRae K, Butler BE, Popiel SJ (1987) Spatiotopic and retinotopic components of iconic memory. Psychol Res  49: 221–227.  <ul class="find" data-citedArticleID="1065006" data-doi="10.1007/bf00309030"><li><a href="http://dx.doi.org/10.1007/bf00309030" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Spatiotopic+and+retinotopic+components+of+iconic+memory." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Spatiotopic+and+retinotopic+components+of+iconic+memory.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">31.
              </span><a name="pone.0001699-Lamme3" id="pone.0001699-Lamme3"></a>Lamme VA, Roelfsema PR (2000) The distinct modes of vision offered by feedforward and recurrent processing. Trends Neurosci  23: 571–579.  <ul class="find" data-citedArticleID="1064984" data-doi="10.1016/s0166-2236(00)01657-x"><li><a href="http://dx.doi.org/10.1016/s0166-2236(00)01657-x" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=The+distinct+modes+of+vision+offered+by+feedforward+and+recurrent+processing." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22The+distinct+modes+of+vision+offered+by+feedforward+and+recurrent+processing.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">32.
              </span><a name="pone.0001699-Lamme4" id="pone.0001699-Lamme4"></a>Lamme VA, Zipser K, Spekreijse H (2002) Masking interrupts figure-ground signals in V1. J Cogn Neurosci  14: 1044–1053.  <ul class="find" data-citedArticleID="1064986" data-doi="10.1162/089892902320474490"><li><a href="http://dx.doi.org/10.1162/089892902320474490" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Masking+interrupts+figure-ground+signals+in+V1." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Masking+interrupts+figure-ground+signals+in+V1.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">33.
              </span><a name="pone.0001699-Jolij1" id="pone.0001699-Jolij1"></a>Jolij J, Lamme VA (2005) Repression of unconscious information by conscious processing: evidence from affective blindsight induced by transcranial magnetic stimulation. Proc Natl Acad Sci U S A  102: 10747–10751.  <ul class="find" data-citedArticleID="1064978" data-doi="10.1073/pnas.0500834102"><li><a href="http://dx.doi.org/10.1073/pnas.0500834102" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Repression+of+unconscious+information+by+conscious+processing%3A+evidence+from+affective+blindsight+induced+by+transcranial+magnetic+stimulation." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Repression+of+unconscious+information+by+conscious+processing%3A+evidence+from+affective+blindsight+induced+by+transcranial+magnetic+stimulation.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">34.
              </span><a name="pone.0001699-PascualLeone1" id="pone.0001699-PascualLeone1"></a>Pascual-Leone A, Walsh V (2001) Fast backprojections from the motion to the primary visual area necessary for visual awareness. Science  292: 510–512.  <ul class="find" data-citedArticleID="1065014" data-doi="10.1126/science.1057099"><li><a href="http://dx.doi.org/10.1126/science.1057099" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Fast+backprojections+from+the+motion+to+the+primary+visual+area+necessary+for+visual+awareness." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Fast+backprojections+from+the+motion+to+the+primary+visual+area+necessary+for+visual+awareness.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">35.
              </span><a name="pone.0001699-Hupe1" id="pone.0001699-Hupe1"></a>Hupe JM, James AC, Payne BR, Lomber SG, Girard P, et al.  (1998) Cortical feedback improves discrimination between figure and background by V1, V2 and V3 neurons. Nature  394: 784–787.  <ul class="find" data-citedArticleID="1064968"><li><a href="http://www.crossref.org/guestquery/?auth2=&amp;atitle2=Cortical+feedback+improves+discrimination+between+figure+and+background+by+V1%2C+V2+and+V3+neurons.&amp;auth=&amp;atitle=Cortical+feedback+improves+discrimination+between+figure+and+background+by+V1%2C+V2+and+V3+neurons." target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Cortical+feedback+improves+discrimination+between+figure+and+background+by+V1%2C+V2+and+V3+neurons." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Cortical+feedback+improves+discrimination+between+figure+and+background+by+V1%2C+V2+and+V3+neurons.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">36.
              </span><a name="pone.0001699-Lamme5" id="pone.0001699-Lamme5"></a>Lamme VA, Zipser K, Spekreijse H (1998) Figure-ground activity in primary visual cortex is suppressed by anesthesia. Proc Natl Acad Sci U S A  95: 3263–3268.  <ul class="find" data-citedArticleID="1064988" data-doi="10.1073/pnas.95.6.3263"><li><a href="http://dx.doi.org/10.1073/pnas.95.6.3263" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Figure-ground+activity+in+primary+visual+cortex+is+suppressed+by+anesthesia." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Figure-ground+activity+in+primary+visual+cortex+is+suppressed+by+anesthesia.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">37.
              </span><a name="pone.0001699-Super1" id="pone.0001699-Super1"></a>Super H, Spekreijse H, Lamme VA (2001) Two distinct modes of sensory processing observed in monkey primary visual cortex (V1). Nat Neurosci  4: 304–310.  <ul class="find" data-citedArticleID="1065022"><li><a href="http://www.crossref.org/guestquery/?auth2=&amp;atitle2=Two+distinct+modes+of+sensory+processing+observed+in+monkey+primary+visual+cortex+%28V1%29.&amp;auth=&amp;atitle=Two+distinct+modes+of+sensory+processing+observed+in+monkey+primary+visual+cortex+%28V1%29." target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Two+distinct+modes+of+sensory+processing+observed+in+monkey+primary+visual+cortex+%28V1%29." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Two+distinct+modes+of+sensory+processing+observed+in+monkey+primary+visual+cortex+%28V1%29.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">38.
              </span><a name="pone.0001699-Dehaene1" id="pone.0001699-Dehaene1"></a>Dehaene S, Changeux JP, Naccache L, Sackur J, Sergent C (2006) Conscious, preconscious, and subliminal processing: a testable taxonomy. Trends Cogn Sci  10: 204–211.  <ul class="find" data-citedArticleID="1064960" data-doi="10.1016/j.tics.2006.03.007"><li><a href="http://dx.doi.org/10.1016/j.tics.2006.03.007" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Conscious%2C+preconscious%2C+and+subliminal+processing%3A+a+testable+taxonomy." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Conscious%2C+preconscious%2C+and+subliminal+processing%3A+a+testable+taxonomy.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">39.
              </span><a name="pone.0001699-Landman2" id="pone.0001699-Landman2"></a>Landman R, Spekreijse H, Lamme VA (2004) The role of figure-ground segregation in change blindness. Psychon Bull Rev  11: 254–261.  <ul class="find" data-citedArticleID="1064992" data-doi="10.3758/bf03196567"><li><a href="http://dx.doi.org/10.3758/bf03196567" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=The+role+of+figure-ground+segregation+in+change+blindness." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22The+role+of+figure-ground+segregation+in+change+blindness.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li></ol></div>

  </div>

      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001699.XML" value="83985"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001699.PDF" value="362663"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001699.g001.PNG_L" value="301748"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001699.g001.PNG_M" value="53798"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001699.g001.PNG_S" value="6244"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001699.g001.TIF" value="637546"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001699.g001.PNG_I" value="19738"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001699.g002.PNG_L" value="387514"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001699.g002.PNG_M" value="57078"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001699.g002.PNG_S" value="6951"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001699.g002.TIF" value="900392"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001699.g002.PNG_I" value="21208"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001699.g003.PNG_L" value="407315"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001699.g003.PNG_M" value="70229"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001699.g003.PNG_S" value="8076"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001699.g003.TIF" value="875888"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001699.g003.PNG_I" value="25116"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001699.g004.PNG_L" value="210985"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001699.g004.PNG_M" value="44260"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001699.g004.PNG_S" value="6319"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001699.g004.TIF" value="652684"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001699.g004.PNG_I" value="16278"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001699.g005.PNG_L" value="209873"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001699.g005.PNG_M" value="50821"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001699.g005.PNG_S" value="7529"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001699.g005.TIF" value="539234"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001699.g005.PNG_I" value="18091"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001699.g006.PNG_L" value="57373"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001699.g006.PNG_M" value="57641"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001699.g006.PNG_S" value="7478"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001699.g006.TIF" value="414208"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001699.g006.PNG_I" value="20221"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001699.g007.PNG_L" value="337135"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001699.g007.PNG_M" value="61991"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001699.g007.PNG_S" value="9923"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001699.g007.TIF" value="714096"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001699.g007.PNG_I" value="21258"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001699.g008.PNG_L" value="158530"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001699.g008.PNG_M" value="84932"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001699.g008.PNG_S" value="10285"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001699.g008.TIF" value="409654"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001699.g008.PNG_I" value="30091"/>

</div>
<div class="sidebar">

  <div class="article-actions cf">
      <div class="download">
        <span class="btn"><a href="/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0001699&amp;representation=PDF" title="Download" target="_blank">Download PDF</a></span>
      </div>
      <div class="btn-reveal dropdown">
        <div class="dropdown-icon">
          <span class="btn">&nbsp;</span>
        </div>

        <div class="content">
          <ul class="bullet">
            <li><a href="/article/citationList.action?articleURI=info%3Adoi%2F10.1371%2Fjournal.pone.0001699" title="Download citations">Citation</a></li>
            <li><a href="/article/fetchObjectAttachment.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0001699&amp;representation=XML" title="Download article XML">XML</a></li>
          </ul>
        </div>
      </div> <!-- end btn-reveal dropdown-->


    <div class="btn-reveal flt-l">
        <span class="btn">Print</span>
        <div class="content">
            <ul class="bullet">
                <li id="print-article"><a href="#" onclick="if(typeof(_gaq) != 'undefined'){ _gaq.push(['_trackEvent','Article', 'Print', 'Click']); } window.print(); return false;" title="Print Article">Print article</a></li>
                <li>
                  <a href="https://www.odysseypress.com/onlinehost/reprint_order.php?type=A&page=0&journal=7&doi=10.1371/journal.pone.0001699&volume=&issue=&title=Are There Multiple Visual Short-Term Memory Stores?&author_name=Ilja%20G.%20Sligte%2C%20H.%20Steven%20Scholte%2C%20Victor%20A.%20F.%20Lamme&start_page=1&end_page=9" title="Odyssey Press">EzReprint</a>
                </li>
            </ul>
        </div>
    </div>

    <div class="btn-reveal flt-r">
        <span class="btn">Share</span>
        <div class="content">
            <ul class="social">
                <li><a href="http://www.reddit.com/submit?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0001699" target="_blank" title="Submit to Reddit"><img src="/images/icon.reddit.16.png" width="16" height="16" alt="Reddit">Reddit</a></li>

                <li><a href="https://plus.google.com/share?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0001699" target="_blank" title="Share on Google+"><img src="/images/icon.gplus.16.png" width="16" height="16" alt="Google+">Google+</a></li>

                <li><a href="http://www.stumbleupon.com/submit?url=http%3A%2F%2Fwww.plosone.org%2Farticle%2Finfo%253Adoi%252F10.1371%252Fjournal.pone.0001699" target="_blank" title="Add to StumbleUpon"><img src="/images/icon.stumble.16.png" width="16" height="16" alt="StumbleUpon">StumbleUpon</a></li>

                <li><a href="http://www.facebook.com/share.php?u=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0001699&amp;t=Are%20There%20Multiple%20Visual%20Short-Term%20Memory%20Stores%3F" target="_blank" title="Share on Facebook"><img src="/images/icon.fb.16.png" width="16" height="16" alt="Facebook">Facebook</a></li>

                <li><a href="http://www.linkedin.com/shareArticle?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0001699&title=Are%20There%20Multiple%20Visual%20Short-Term%20Memory%20Stores%3F&summary=Checkout%20this%20article%20I%20found%20at%20PLOS" target="_blank" title="Add to LinkedIn"><img src="/images/icon.linkedin.16.png" width="16" height="16" alt="Mendeley">LinkedIn</a></li>

                <li><a href="http://www.citeulike.org/posturl?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0001699&amp;title=Are%20There%20Multiple%20Visual%20Short-Term%20Memory%20Stores%3F" target="_blank" title="Add to CiteULike"><img src="/images/icon.cul.16.png" width="16" height="16" alt="CiteULike">CiteULike</a></li>

                <li><a href="http://www.mendeley.com/import/?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0001699" target="_blank" title="Add to Mendeley"><img src="/images/icon.mendeley.16.png" width="16" height="16" alt="Mendeley">Mendeley</a></li>

                <li><a href="https://www.pubchase.com/library?add_aid=10.1371%2Fjournal.pone.0001699&amp;source=plos" target="_blank" title="Add to PubChase"><img src="/images/icon.pc.16.png" width="16" height="16" alt="PubChase">PubChase</a></li>


                <script type="text/javascript">
                    // replace tweet with one that's pre-shortened to 140 chars
                    function truncateTweetText() {
                        var twtTitle = 'Are There Multiple Visual Short-Term Memory Stores?';
                        var twtUrl = 'http://dx.plos.org/10.1371/journal.pone.0001699';
                        // all URLs posted to twitter get auto-shortened to 20 chars.
                        var maxLength = 140 - (20 + 1);
                        // truncate the title to include space for twtTag and ellipsis (here, 10 = tag length + space + ellipsis)
                        if (twtTitle.length > maxLength) { twtTitle = twtTitle.substr(0, (maxLength - 10)) + '...'; }
                        // set the href to use the shortened tweet
                        $('#twitter-share-link').prop('href', 'http://twitter.com/intent/tweet?text=' + encodeURIComponent('#PLOSONE: ' + twtTitle + ' ' + twtUrl));
                    }
                </script>
                <li><a href="http://twitter.com/intent/tweet?text=#PLOSONE%3A%20Are%20There%20Multiple%20Visual%20Short-Term%20Memory%20Stores%3F http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0001699" onclick="truncateTweetText();" target="_blank" title="Share on Twitter" id="twitter-share-link"><img src="/images/icon.twtr.16.png" width="16" height="16" alt="Twitter">Twitter</a></li>

                <li><a href="/article/email/info%3Adoi%2F10.1371%2Fjournal.pone.0001699" title="Email this article"><img src="/images/icon.email.16.png" width="16" height="16" alt="Email">Email</a></li>
            </ul>
        </div>
    </div><!--end btn-reveal flt-r-->
</div><!-- end article-actions-->

<!-- begin Crossmark -->

<a id="open-crossmark" href="#" style="margin-top: -28px; display:block"><img style="border: 0; display: none;
 padding: 10px 0 18px 0;"  id="crossmark-icon" src="/images/logo-crossmark-bw.png" /></a>
<div id="crossmark-dialog" style="display: none;" title="">
    <!-- the external CrossMark data is loaded inside this iframe -->
    <iframe id="crossmark-dialog-frame" frameborder="0"></iframe>
</div>

<!-- end crossmark -->


<div class="block" id="subject-area-sidebar-block">
    <div class="header">
        <h3>Subject Areas</h3><div title="More information" id="subject-area-sidebar-block-help-icon"><img align="right"
                                                                                                           alt="info" src="/images/button_info.png"/><div id="subject-area-sidebar-block-help"><img align="right"
                                                                                                                                                                                                    src="/images/button_info.png"/><p>
        <b>We want your feedback.</b> Do these subject areas make sense for this article? If not, click the flag
        next to the incorrect subject area and we will review it. Thanks for your help!
    </p></div></div>
    </div>


    <ul id="subject-area-sidebar-list">
















          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Access+to+consciousness%22" title="Search for articles in the subject area:'Access to consciousness'"><div class="flagText">Access to consciousness</div></a>
              <div data-categoryid="35567" data-articleid="26224"
                   data-categoryname="Access to consciousness"
                   class="flagImage" title="Flag 'Access to consciousness' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Attention%22" title="Search for articles in the subject area:'Attention'"><div class="flagText">Attention</div></a>
              <div data-categoryid="33353" data-articleid="26224"
                   data-categoryname="Attention"
                   class="flagImage" title="Flag 'Attention' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Change+detection%22" title="Search for articles in the subject area:'Change detection'"><div class="flagText">Change detection</div></a>
              <div data-categoryid="33441" data-articleid="26224"
                   data-categoryname="Change detection"
                   class="flagImage" title="Flag 'Change detection' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Human+performance%22" title="Search for articles in the subject area:'Human performance'"><div class="flagText">Human performance</div></a>
              <div data-categoryid="18145" data-articleid="26224"
                   data-categoryname="Human performance"
                   class="flagImage" title="Flag 'Human performance' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Information+processing%22" title="Search for articles in the subject area:'Information processing'"><div class="flagText">Information processing</div></a>
              <div data-categoryid="46329" data-articleid="26224"
                   data-categoryname="Information processing"
                   class="flagImage" title="Flag 'Information processing' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Memory%22" title="Search for articles in the subject area:'Memory'"><div class="flagText">Memory</div></a>
              <div data-categoryid="31849" data-articleid="26224"
                   data-categoryname="Memory"
                   class="flagImage" title="Flag 'Memory' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Vision%22" title="Search for articles in the subject area:'Vision'"><div class="flagText">Vision</div></a>
              <div data-categoryid="32965" data-articleid="26224"
                   data-categoryname="Vision"
                   class="flagImage" title="Flag 'Vision' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Young+adults%22" title="Search for articles in the subject area:'Young adults'"><div class="flagText">Young adults</div></a>
              <div data-categoryid="40981" data-articleid="26224"
                   data-categoryname="Young adults"
                   class="flagImage" title="Flag 'Young adults' as inappropriate"></div>
          </li>
    </ul>
</div>

<div class="ad">
    <div class="title">Advertisement</div>






  <iframe id='a0852f54' name='a0852f54'
    src='http://ads.plos.org/www/delivery/afr.php?zoneid=381&amp;cb=7880'
    frameborder='0' scrolling='no' width='160' height='600'>
    <a href='http://ads.plos.org/www/delivery/ck.php?n=a0852f54&amp;cb=2255'
      target='_top'><img src='http://ads.plos.org/www/delivery/avw.php?zoneid=381&amp;cb=3294&amp;n=a0852f54'
      border='0' alt=''/>
    </a>
  </iframe>



</div>

<div id="twitter-alm-timeline" class="twitter-alm-timeline"></div>

<div class="block sidebar-comments">
    <div class="header">
        <h3>Comments</h3>
    </div>
      <p><a href="/annotation/listThread.action?root=6283">Referee comments: Referee 1</a><br>Posted by PLoS_ONE_Group</p>
</div>

</div><!-- sidebar -->
    </div>
  </div>
</div>
<script src="http://wl.figshare.com/static/p_widget.js" type="text/javascript"></script><div id="pageftr">
  <div class="ftr-cols cf">
    <div class="col col-1">
      <img src="/images/logo-plos-footer.png" alt="PLOS Logo" class="logo" />
      <p><a href="/static/releaseNotes">Ambra 2.9.16</a> Managed Colocation provided <br />by <a href="http://www.isc.org/">Internet Systems Consortium</a>.<p>
      <div class="nav nav-aux">
        <a href="/static/privacy">Privacy Policy</a> |
        <a href="/static/terms">Terms of Use</a> |
        <a href="http://www.plos.org/advertise/">Advertise</a> |
        <a href="http://www.plos.org/about/media-inquiries/">Media Inquiries</a>
      </div>
    </div>
    <div class="col col-2">
      <p><a href="http://www.plos.org/publications/journals/">Publications</a></p>
      <div class="nav">
        <ul>
          <li><a href="http://www.plosbiology.org">PLOS Biology</a></li>
          <li><a href="http://www.plosmedicine.org">PLOS Medicine</a></li>
          <li><a href="http://www.ploscompbiol.org">PLOS Computational Biology</a></li>
          <li><a href="http://currents.plos.org">PLOS Currents</a></li>
          <li><a href="http://www.plosgenetics.org">PLOS Genetics</a></li>
          <li><a href="http://www.plospathogens.org">PLOS Pathogens</a></li>
          <li><a href="http://www.plosone.org">PLOS ONE</a></li>
          <li><a href="http://www.plosntds.org">PLOS Neglected Tropical Diseases</a></li>
        </ul>
      </div>
    </div>
    <div class="col col-3">
      <div class="nav">
        <p><a href="http://www.plos.org">plos.org</a></p>
        <p><a href="http://blogs.plos.org">Blogs</a></p>
        <p><a href="http://www.ploscollections.org">Collections</a></p>
        <p><a href="/feedback/new">Send us feedback</a></p>

        <p>California (US) corporation #C2354500, based in San Francisco</p>
      </div>
    </div>
  </div>
</div><!-- pageftr -->

</div><!-- end page-wrap, this div is in header.ftl -->
<script type="text/javascript" src="/javascript/jquery-1.8.1-min.js?v=Tm7VCOzZz3lE03ghpkS6SWkHbyI"></script>
<script type="text/javascript" src="/javascript/ga-min.js?v=lNQ4gt8QcPDatjsdOFl_FGpPhLY"></script>
<script type="text/javascript" src="/javascript/jquery.hoverIntent-min.js?v=mRiGNYY9cIXxVb8u0K_MdW7hHnc"></script>
<script type="text/javascript" src="/javascript/jquery.placeholder-min.js?v=21Pn56Ur9h1N4K4VZDa0nqI3Pxo"></script>
<script type="text/javascript" src="/javascript/jquery.jsonp-2.4.0-min.js?v=lqTpzoHfSq3I5Ygo01qq5WankEo"></script>
<script type="text/javascript" src="/javascript/jquery-ui-1.9.2.custom-min.js?v=raSSlfNO0YsV5uUpAKmTB9n5VTc"></script>
<script type="text/javascript" src="/javascript/jquery.tooltip-min.js?v=cw+6Smh+mdryIA25xvqIvHMrnZM"></script>
<script type="text/javascript" src="/javascript/jquery.uniform-min.js?v=kYUAnX6W2W_2fK3RIuQ2m_YFG9U"></script>
<script type="text/javascript" src="/javascript/jquery.pjax-min.js?v=939kLBjL5_YKbx71T1RHjYaD4l8"></script>
<script type="text/javascript" src="/javascript/imagesloaded-min.js?v=XeuAp8Gc3mvQUo+wZCSF8ttPwvw"></script>
<script type="text/javascript" src="/javascript/figviewer-min.js?v=yPUa0sUQ_iHkI+IRv2i9bjyZJFo"></script>
<script type="text/javascript" src="/javascript/global-min.js?v=0Q3PwjeaWtXYDnqIsQvnL_ou0qs"></script>
<script type="text/javascript" src="/javascript/jquery.touchswipe-min.js?v=huaek_e6HqTduvCNAN91dJolTyw"></script>
<script type="text/javascript" src="/javascript/jquery.base64-min.js?v=VwV1zeVqKZj5FCAdlK0q5NRxbBg"></script>
<script type="text/javascript" src="/javascript/alm-min.js?v=Y5gm6B0b4Kx2YHNObNrgEeBgXlY"></script>
<script type="text/javascript" src="/javascript/taxonomy-browser-min.js?v=vBVMuDMYkGJCXIUxLe35GoyiJNw"></script>
<script type="text/javascript" src="/javascript/jquery.filterize-min.js?v=j0ZKVnHyk2nhFy8eIuNJkp7xaM0"></script>
<script type="text/javascript" src="/javascript/plosone-min.js?v=TK4H4arL_XBSwwJq+K1N3kqYfAI"></script>
<script type="text/javascript" src="/javascript/twitter-min.js?v=xKgcxLsQFXy+at1ao1NVke8nFlM"></script>
<script type="text/javascript" src="/javascript/crossmark.1.4-min.js?v=3FO4k0SjwTaGNnKGNSqthar1080"></script>
<script type="text/javascript">
  var _sf_async_config={uid:16579,domain:"plosone.org"};
  (function(){
    function loadChartbeat() {
      window._sf_endpt=(new Date()).getTime();
      var e = document.createElement('script');
      e.setAttribute('language', 'javascript');
      e.setAttribute('type', 'text/javascript');
      e.setAttribute('src',
          (("https:" == document.location.protocol) ? "https://a248.e.akamai.net/chartbeat.download.akamai.com/102508/" : "http://static.chartbeat.com/") +
              "js/chartbeat.js");
      document.body.appendChild(e);
    }
    var oldonload = window.onload;
    window.onload = (typeof window.onload != 'function') ?
        loadChartbeat : function() { oldonload(); loadChartbeat(); };
  })();
</script>
<!-- <script type="application/javascript" src="http://crossmark.crossref.org/javascripts/v1.3/crossmark.min.js"></script> -->

</body>
</html>
