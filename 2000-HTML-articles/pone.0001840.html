

 



<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"
      xmlns:foaf="http://xmlns.com/foaf/0.1/"
      xmlns:dc="http://purl.org/dc/terms/"
      xmlns:doi="http://dx.doi.org/"
      xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
      xmlns:xsd="http://www.w3.org/2001/XMLSchema-datatypes#"
      lang="en" xml:lang="en"
      itemscope itemtype="http://schema.org/Article"
      class="no-js">
<head prefix="og: http://ogp.me/ns#">
  <title>PLOS ONE: Seeing ‘Where’ through the Ears: Effects of Learning-by-Doing and Long-Term Sensory Deprivation on Localization Based on Image-to-Sound Substitution</title>


<link rel="stylesheet" type="text/css"  href="/css/global-min.css?v=izteQ6tu7kgsJZW_xmrYizvKiHM" />


    <!--[if lte IE 7]>
<link rel="stylesheet" type="text/css"  href="/css/lte_ie7-min.css?v=3bykQUyQmReeuobVyPozcJ9LxRc" />
    <![endif]-->


<link rel="stylesheet" type="text/css"  href="/css/jquery-ui-min.css?v=eXDHTEJM0lIAmDe5k0I0Ad4nxNo" />


<link rel="stylesheet" type="text/css"  href="/css/journal.css?v=T7ZVxJfgk9jNxLAJ2qHz1vZpgYU" />


<link rel="stylesheet" type="text/css" media="print" href="/css/print-min.css?v=T5lb0B3q6EXBsuDluc5V5w+AkRc" />


  <link rel="stylesheet" href="http://f.fontdeck.com/s/css/js/www.plosone.org/24557.css" type="text/css"/>

  <!--chartbeat -->
  <script type="text/javascript">var _sf_startpt = (new Date()).getTime()</script>
  <script>document.documentElement.className += ' js';</script>

  <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <meta http-equiv="X-UA-Compatible" content="IE=EmulateIE7; IE=EmulateIE9"/>
  <meta name="description" content="PLOS ONE: an inclusive, peer-reviewed, open-access resource from the PUBLIC LIBRARY OF SCIENCE. Reports of well-performed scientific studies from all disciplines freely available to the whole world."/>
  <meta name="keywords" content="PLOS, Public Library of Science, Open Access, Open-Access, Science, Medicine, Biology, Research, Peer-review, Inclusive, Interdisciplinary, Ante-disciplinary, Physics, Chemistry, Engineering"/>
  <meta name="almHost" content="http://alm.plos.org/api/v3/articles"/>
  <meta name="searchHost" content="http://api.plos.org/search" />
  <meta name="termsHost" content="http://api.plos.org/terms" />
  <meta name="solrApiKey" content="plos"/>
  <meta name="almAPIKey" content="3pezRBRXdyzYW6ztfwft" />
  <meta name="currentJournal" content="PLoSONE" />
  <meta name="almRequestBatchSize" content="" />

  <meta name="citation_publisher" content="Public Library of Science"/>
  <meta name="citation_doi" content="10.1371/journal.pone.0001840"/>
  <meta name="dc.identifier" content="10.1371/journal.pone.0001840" />

    <meta name="citation_title" content="Seeing ‘Where’ through the Ears: Effects of Learning-by-Doing and Long-Term Sensory Deprivation on Localization Based on Image-to-Sound Substitution"/>
    <meta itemprop="name" content="Seeing ‘Where’ through the Ears: Effects of Learning-by-Doing and Long-Term Sensory Deprivation on Localization Based on Image-to-Sound Substitution"/>

      <meta name="citation_author" content="Michael J. Proulx"/>
            <meta name="citation_author_institution" content="Institute of Experimental Psychology, Heinrich-Heine-University Düsseldorf, Düsseldorf, Germany"/>
      <meta name="citation_author" content="Petra Stoerig"/>
            <meta name="citation_author_institution" content="Institute of Experimental Psychology, Heinrich-Heine-University Düsseldorf, Düsseldorf, Germany"/>
      <meta name="citation_author" content="Eva Ludowig"/>
            <meta name="citation_author_institution" content="Institute of Experimental Psychology, Heinrich-Heine-University Düsseldorf, Düsseldorf, Germany"/>
      <meta name="citation_author" content="Inna Knoll"/>
            <meta name="citation_author_institution" content="Institute of Experimental Psychology, Heinrich-Heine-University Düsseldorf, Düsseldorf, Germany"/>

    <meta name="citation_date" content="2008/3/26"/>

  <meta name="citation_pdf_url" content="http://dx.plos.org/10.1371/journal.pone.0001840.pdf" />

      <meta name="citation_journal_title" content="PLOS ONE" />
    <meta name="citation_firstpage" content="e1840"/>
    <meta name="citation_issue" content="3"/>
    <meta name="citation_volume" content="3"/>
    <meta name="citation_issn" content="1932-6203"/>

    <meta name="citation_journal_abbrev" content="PLoS ONE" />

      <meta name="citation_reference" content="citation_title=Vision: A Computational Investigation into the Human Representation and Processing of Visual Information.; citation_author=D Marr; citation_number=1; citation_pages=397; citation_date=1982; citation_publisher=WH Freeman; " />
      <meta name="citation_reference" content="citation_title=Sensory substitution and the human-machine interface.; citation_author=P Bach-y-Rita; citation_author=SW Kercel; citation_journal_title=Trends Cogn Sci; citation_volume=7; citation_number=2; citation_pages=541-546; citation_date=2003; " />
      <meta name="citation_reference" content="citation_title=Auditory coding of visual patterns for the blind.; citation_author=P Arno; citation_author=M-C Wanet-Defalque; citation_author=C Capelle; citation_author=M Catalan-Ahumada; citation_author=C Veraart; citation_journal_title=Perception; citation_volume=28; citation_number=3; citation_pages=1013-1030; citation_date=1999; " />
      <meta name="citation_reference" content="citation_title=The perception of visual images encoded in musical form: a study in crossmodal information transfer.; citation_author=J Cronly-Dillon; citation_author=K Persaud; citation_author=RPF Gregory; citation_journal_title=Proc R Soc Lond B Biol Sci; citation_volume=266; citation_number=4; citation_pages=2427-2433; citation_date=1999; " />
      <meta name="citation_reference" content="citation_title=Blind subjects construct conscious mental images of visual scenes encoded in musical form.; citation_author=J Cronly-Dillon; citation_author=K Persaud; citation_author=R Blore; citation_journal_title=Proc R Soc Lond B Biol Sci; citation_volume=267; citation_number=5; citation_pages=2231-2238; citation_date=2000; " />
      <meta name="citation_reference" content="citation_title=Assessment of sensory substitution prosthesis potentialities in minimalist conditions of learning.; citation_author=C Poirier; citation_author=M-A Richard; citation_author=D Tranduy; citation_author=C Veraart; citation_journal_title=Appl Cogn Psychol; citation_volume=20; citation_number=6; citation_pages=447-460; citation_date=2006; " />
      <meta name="citation_reference" content="citation_title=Crossmodal plasticity revealed by electrotactile stimulation of the tongue in the congenitally blind.; citation_author=M Ptito; citation_author=SM Moesgaard; citation_author=A Gjedde; citation_author=R Kupers; citation_journal_title=Brain; citation_volume=128; citation_number=7; citation_pages=609-614; citation_date=2005; " />
      <meta name="citation_reference" content="citation_title=Learning to perceive with a visuo-auditory substitution system: Localisation and object recognition with ‘The vOICe’.; citation_author=M Auvray; citation_author=S Hanneton; citation_author=JK O'Regan; citation_journal_title=Perception; citation_volume=36; citation_number=8; citation_pages=416-430; citation_date=2007; " />
      <meta name="citation_reference" content="citation_title=Cross-modal activation of visual cortex during depth perception using auditory substitution of vision.; citation_author=L Renier; citation_author=O Collignon; citation_author=C Poirier; citation_author=D Tranduy; citation_author=A Vanlierde; citation_journal_title=Neuroimage; citation_volume=26; citation_number=9; citation_pages=573-580; citation_date=2005; " />
      <meta name="citation_reference" content="citation_title=Human spatial navigation via a visuo-tactile sensory substitution system.; citation_author=H Segond; citation_author=D Weiss; citation_author=E Sampaio; citation_journal_title=Perception; citation_volume=34; citation_number=10; citation_pages=1231-1249; citation_date=2005; " />
      <meta name="citation_reference" content="citation_title=An experimental system for auditory image representations.; citation_author=PBL Meijer; citation_journal_title=IEEE Trans Biomed Eng; citation_volume=39; citation_number=11; citation_pages=112-121; citation_date=1992; " />
      <meta name="citation_reference" content="citation_title=The metamodal organization of the brain.; citation_author=A Pascual-Leone; citation_author=R Hamilton; citation_journal_title=Prog Brain Res; citation_volume=134; citation_number=12; citation_pages=1-19; citation_date=2001; " />
      <meta name="citation_reference" content="citation_title=Braille character discrimination in blindfolded human subjects.; citation_author=T Kauffman; citation_author=H Théoret; citation_author=A Pascual-Leone; citation_journal_title=NeuroReport; citation_volume=13; citation_number=13; citation_pages=571-574; citation_date=2002; " />
      <meta name="citation_reference" content="citation_title=Early-blind human subjects localize sound sources better than sighted subjects.; citation_author=N Lessard; citation_author=M Paré; citation_author=F Lepore; citation_author=M Lassonde; citation_journal_title=Nature; citation_volume=395; citation_number=14; citation_pages=278-280; citation_date=1998; " />
      <meta name="citation_reference" content="citation_title=The occipital cortex in the blind: Lessons about plasticity and vision.; citation_author=A Amedi; citation_author=LB Merabet; citation_author=F Bermpohl; citation_author=A Pascual-Leone; citation_journal_title=Curr Dir Psychol Sci; citation_volume=14; citation_number=15; citation_pages=306-311; citation_date=2005; " />
      <meta name="citation_reference" content="citation_title=Image-to-sound conversion: experience-induced plasticity in auditory cortex of blindfolded adults.; citation_author=B Pollok; citation_author=I Schnitzler; citation_author=P Stoerig; citation_author=T Mierdorf; citation_author=A Schnitzler; citation_journal_title=Exp Brain Res; citation_volume=167; citation_number=16; citation_pages=287-291; citation_date=2005; " />
      <meta name="citation_reference" content="citation_title=An alternative to null-hypothesis significance tests.; citation_author=PR Killeen; citation_journal_title=Psychol Sci; citation_volume=16; citation_number=17; citation_pages=345-353; citation_date=2005; " />
      <meta name="citation_reference" content="citation_author=J Cohen; citation_number=18; citation_pages=567; citation_date=1988; citation_publisher=Lawrence Erlbaum Associates; " />

  <link rel="canonical" href="http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0001840" />

    <meta name="twitter:card" content="summary"/>
    <meta name="twitter:site" content="@plosone"/>
    <meta name="twitter:title" content="Seeing ‘Where’ through the Ears: Effects of Learning-by-Doing and Long-Term Sensory Deprivation on Localization Based on Image-to-Sound Substitution"/>
    <meta name="twitter:description" content="BackgroundSensory substitution devices for the blind translate inaccessible visual information into a format that intact sensory pathways can process. We here tested image-to-sound conversion-based localization of visual stimuli (LEDs and objects) in 13 blindfolded participants.Methods and FindingsSubjects were assigned to different roles as a function of two variables: visual deprivation (blindfolded continuously (Bc) for 24 hours per day for 21 days; blindfolded for the tests only (Bt)) and system use (system not used (Sn); system used for tests only (St); system used continuously for 21 days (Sc)). The effect of learning-by-doing was assessed by comparing the performance of eight subjects (BtSt) who only used the mobile substitution device for the tests, to that of three subjects who, in addition, practiced with it for four hours daily in their normal life (BtSc and BcSc); two subjects who did not use the device at all (BtSn and BcSn) allowed assessment of its use in the tasks we employed. The impact of long-term sensory deprivation was investigated by blindfolding three of those participants throughout the three week-long experiment (BcSn, BcSn/c, and BcSc); the other ten subjects were only blindfolded during the tests (BtSn, BtSc, and the eight BtSt subjects). Expectedly, the two subjects who never used the substitution device, while fast in finding the targets, had chance accuracy, whereas subjects who used the device were markedly slower, but showed much better accuracy which improved significantly across our four testing sessions. The three subjects who freely used the device daily as well as during tests were faster and more accurate than those who used it during tests only; however, long-term blindfolding did not notably influence performance.ConclusionsTogether, the results demonstrate that the device allowed blindfolded subjects to increasingly know where something was by listening, and indicate that practice in naturalistic conditions effectively improved &ldquo;visual&rdquo; localization performance."/>
      <meta name="twitter:image" content="http://dx.plos.org/10.1371/journal.pone.0001840.g005"/>

  <meta property="og:title" content="Seeing ‘Where’ through the Ears: Effects of Learning-by-Doing and Long-Term Sensory Deprivation on Localization Based on Image-to-Sound Substitution" />
  <meta property="og:type" content="article" />
  <meta property="og:url" content="http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0001840" />

 <!--end articleInfoX-->

  <link rel="pingback" href="http://www.plosone.org/pingback" />


  <link rel="shortcut icon" href="/images/favicon.ico" type="image/x-icon"/>
  <link rel="home" title="home" href="/"/>
  <link rel="alternate" type="application/rss+xml"
        title="PLOS ONE: New Articles"
        href="http://www.plosone.org/article/feed"/>
</head>
<body>

  <div id="page-wrap">
    <div id="topbanner" class="cf">

<!-- Div for the ad at the top of journal home page-->
<div class="center">
  <div class="title">Advertisement</div>
  <iframe id='a3ac9da4' name='a3ac9da4'
    src='http://ads.plos.org/www/delivery/afr.php?zoneid=345&amp;cb=7030'
    frameborder='0' scrolling='no' width='730' height='90'>
    <a href='http://ads.plos.org/www/delivery/ck.php?n=a3ac9da4&amp;cb=8146'
      target='_top'><img src='http://ads.plos.org/www/delivery/avw.php?zoneid=345&amp;cb=2379&amp;n=a3ac9da4'
      border='0' alt=''/>
    </a>
  </iframe>
</div>    </div>

    <div id="pagehdr-wrap">
      <div id="pagehdr">
        <div id="user" class="nav">
          <ul>
            <li><a href="http://www.plos.org">plos.org</a></li>
            <li><a href="https://register.plos.org/ambra-registration/register.action">create account</a></li>
            <li class="btn-style"><a
              href="/user/secure/secureRedirect.action?goTo=%2Farticle%2FfetchArticle.action%3FarticleURI%3Dinfo%253Adoi%252F10.1371%252Fjournal.pone.0001840">sign in</a>
            </li>
          </ul>
        </div>
        <div class="logo">
          <a href="/"><img src="/images/logo.png" alt="PLOS ONE"></a>
        </div>

<div id="nav-main" class="nav">
  <ul>
        <li id="mn-01"><a href="/taxonomy" class="areas-link">Subject Areas</a></li>
    <li id="mn-02"><a href="javascript:void(0);">For Authors</a>
      <div class="submenu" style="width: 540px; margin-left: -300px;">
        <div class="block">
          <div class="submit-script">
            <h3>Submit your Manuscript</h3>
            <ul>
              <li>Fair, rigorous peer review</li>
              <li>Broad scope and wide reach</li>
            </ul>
            <a href="/static/submissionInstructions" class="btn">get started</a>
          </div>
        </div>
        <div class="menu">
          <ul>
            <li><a href="/static/publish">Why Publish with PLOS ONE</a></li>
            <li><a href="/static/publication">Publication Criteria</a></li>
            <li><a href="/static/editorial">Editorial Policies</a></li>
            <li><a href="/static/guidelines">Preparing A Manuscript</a></li>
            <li><a href="/static/figureGuidelines">Figure and Table Guidelines</a></li>
          <li><a href="/static/supportingInformation">Supporting Information Guidelines</a></li>
            <li><a href="/static/submissionInstructions">Submitting a Manuscript</a></li>
          </ul>
        </div>
      </div>
    </li>

    <li id="mn-03"><a href="javascript:void(0);">About Us</a>
      <div class="submenu" style="width:248px; margin-left:-30px;">
        <div class="menu">
          <ul>
            <li><a href="/static/information">Journal Information</a></li>
            <li><a href="/static/edboard">Editorial Board</a></li>
            <li><a href="/static/reviewerGuidelines">Reviewer Guidelines</a></li>
            <li><a href="/static/almInfo">Article-Level Metrics</a></li>
            <li><a href="/static/license">Open-Access License</a></li>
            <li><a href="/static/downloads">Media Downloads</a></li>
            <li><a href="/static/commentGuidelines">Guidelines for Comments</a></li>
            <li><a href="/static/corrections">Corrections</a></li>
            <li><a href="/static/help">Help Using this Site</a></li>
            <li><a href="/static/contact">Contact Us</a></li>
          </ul>
        </div>
      </div>
    </li>
  </ul>
<div id="db">
  <form name="searchForm" action="/search/simple?noSearchFlag=true&amp;query=&amp;articleURI=info%3Adoi%2F10.1371%2Fjournal.pone.0001840" method="get" >
<input type="hidden" name="from" value="globalSimpleSearch" id="from"/><input type="hidden" name="filterJournals" value="PLoSONE" id="filterJournals"/>    <fieldset>
      <legend>Search</legend>
      <label for="search">Search</label>
      <div class="wrap">
        <input id="search" type="text" name="query" placeholder="Search">
        <input type="image" alt="SEARCH" src="/images/icon.search.gif">
      </div>
    </fieldset>
  </form>
    <a id="advSearch" href="/search/advanced?noSearchFlag=true&amp;query=&amp;articleURI=info%3Adoi%2F10.1371%2Fjournal.pone.0001840&filterJournals=PLoSONE">advanced search</a>
</div></div>

      </div>
      <!-- pagehdr-->
    </div>
    <!-- pagehdr-wrap -->

  <!--body and html tags gets closed in global_footer.ftl-->
<script type="text/javascript" src="/javascript/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<div id="pagebdy-wrap">
  <div id="pagebdy">

    <div id="article-block" class="cf">

<div class="article-meta cf">
  <ul id="almSignPost" style="display: none;"></ul>
  <div class="article-type">
    <span class="type oa">Open Access</span>
      <span class="type pr">Peer-Reviewed</span>
  </div>
</div>

<div class="header" id="hdr-article">

<div class="article-kicker">
      <span id="article-type-heading">
        Research Article
      </span>
</div>  <h1 property="dc:title" datatype="" rel="dc:type" href="http://purl.org/dc/dcmitype/Text">
    Seeing ‘Where’ through the Ears: Effects of Learning-by-Doing and Long-Term Sensory Deprivation on Localization Based on Image-to-Sound Substitution
  </h1>

  <ul class="authors">
      <li>


        <span rel="dc:creator" class="author">
          <span class="person" property="foaf:name" typeof="foaf:Person">
            Michael J. Proulx
              <span class="corresponding">mail</span>, 
          </span>
        </span>

          <div class="author_meta">
            <div class="author_inner">


              
              <p><span class="email">* E-mail:</span> <a href="mailto:Michael.Proulx@uni-duesseldorf.de">Michael.Proulx@uni-duesseldorf.de</a></p>

                <p>Affiliation:
                  Institute of Experimental Psychology, Heinrich-Heine-University Düsseldorf, Düsseldorf, Germany
                </p>


              <span class="close">X</span>

            </div>
          </div>
      </li>
      <li>


        <span rel="dc:creator" class="author">
          <span class="person" property="foaf:name" typeof="foaf:Person">
            Petra Stoerig, 
          </span>
        </span>

          <div class="author_meta">
            <div class="author_inner">


              
              

                <p>Affiliation:
                  Institute of Experimental Psychology, Heinrich-Heine-University Düsseldorf, Düsseldorf, Germany
                </p>


              <span class="close">X</span>

            </div>
          </div>
      </li>
      <li>


        <span rel="dc:creator" class="author">
          <span class="person" property="foaf:name" typeof="foaf:Person">
            Eva Ludowig, 
          </span>
        </span>

          <div class="author_meta">
            <div class="author_inner">


              
              

                <p>Affiliation:
                  Institute of Experimental Psychology, Heinrich-Heine-University Düsseldorf, Düsseldorf, Germany
                </p>


              <span class="close">X</span>

            </div>
          </div>
      </li>
      <li>


        <span rel="dc:creator" class="author">
          <span class="person" property="foaf:name" typeof="foaf:Person">
            Inna Knoll
          </span>
        </span>

          <div class="author_meta">
            <div class="author_inner">


              
              

                <p>Affiliation:
                  Institute of Experimental Psychology, Heinrich-Heine-University Düsseldorf, Düsseldorf, Germany
                </p>


              <span class="close">X</span>

            </div>
          </div>
      </li>
  </ul>
  <ul class="date-doi-line">
    <li>Published: March 26, 2008</li>
    <li>DOI: 10.1371/journal.pone.0001840</li>
  </ul>


</div><!--end header-->
<div class="main cf" id="pjax-container">
  

<div class="nav items-5" id="nav-article">
  <ul>
  <li>
        <span class="active" name="article">Article</span>
  </li>
  <li>
      <a href="/article/authors/info%3Adoi%2F10.1371%2Fjournal.pone.0001840" name="authors">About the Authors</a>
  </li>
  <li>
      <a href="/article/metrics/info%3Adoi%2F10.1371%2Fjournal.pone.0001840" name="metrics">Metrics</a>
  </li>
  <li>
      <a href="/article/comments/info%3Adoi%2F10.1371%2Fjournal.pone.0001840" name="comments">Comments</a>
  </li>
  <li>
      <a href="/article/related/info%3Adoi%2F10.1371%2Fjournal.pone.0001840" name="related">Related Content</a>
  </li>
  </ul>
</div>

<script type="text/javascript">
  var selected_tab = "article";
</script>
  <div id="figure-thmbs" class="carousel cf">
    <div class="wrapper">
      <div class="slider">
              <div class="item">
                <a href="#pone-0001840-g001" data-doi="info:doi/10.1371/journal.pone.0001840" data-uri="info:doi/10.1371/journal.pone.0001840.g001" title="Figure 1">
                  <span class="thmb-wrap">
                    <img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001840.g001&representation=PNG_I" alt="">
                  </span>
                </a>
              </div>
              <div class="item">
                <a href="#pone-0001840-g002" data-doi="info:doi/10.1371/journal.pone.0001840" data-uri="info:doi/10.1371/journal.pone.0001840.g002" title="Figure 2">
                  <span class="thmb-wrap">
                    <img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001840.g002&representation=PNG_I" alt="">
                  </span>
                </a>
              </div>
              <div class="item">
                <a href="#pone-0001840-g003" data-doi="info:doi/10.1371/journal.pone.0001840" data-uri="info:doi/10.1371/journal.pone.0001840.g003" title="Figure 3">
                  <span class="thmb-wrap">
                    <img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001840.g003&representation=PNG_I" alt="">
                  </span>
                </a>
              </div>
              <div class="item">
                <a href="#pone-0001840-g004" data-doi="info:doi/10.1371/journal.pone.0001840" data-uri="info:doi/10.1371/journal.pone.0001840.g004" title="Figure 4">
                  <span class="thmb-wrap">
                    <img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001840.g004&representation=PNG_I" alt="">
                  </span>
                </a>
              </div>
              <div class="item">
                <a href="#pone-0001840-g005" data-doi="info:doi/10.1371/journal.pone.0001840" data-uri="info:doi/10.1371/journal.pone.0001840.g005" title="Figure 5">
                  <span class="thmb-wrap">
                    <img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001840.g005&representation=PNG_I" alt="">
                  </span>
                </a>
              </div>
      </div>
    </div>
  </div>

  <div class="nav-col">
    <div class="nav" id="nav-article-page">
      <ul>
        <li class="nav-col-comments"><a href="/article/comments/info%3Adoi%2F10.1371%2Fjournal.pone.0001840">Reader Comments (1)</a></li>
          <li id="nav-figures"><a data-doi="info:doi/10.1371/journal.pone.0001840" >Figures</a></li>
      </ul>
    </div>
  </div>

  <div class="article">







<div class="abstract"><a id="abstract0" name="abstract0" toc="abstract0" title="Abstract"></a><h2>Abstract</h2>
<h3>Background</h3>
<a id="article1.front1.article-meta1.abstract1.sec1.p1" name="article1.front1.article-meta1.abstract1.sec1.p1"></a><p>Sensory substitution devices for the blind translate inaccessible visual information into a format that intact sensory pathways can process. We here tested image-to-sound conversion-based localization of visual stimuli (LEDs and objects) in 13 blindfolded participants.</p>


<h3>Methods and Findings</h3>
<a id="article1.front1.article-meta1.abstract1.sec2.p1" name="article1.front1.article-meta1.abstract1.sec2.p1"></a><p>Subjects were assigned to different roles as a function of two variables: visual deprivation (blindfolded continuously (Bc) for 24 hours per day for 21 days; blindfolded for the tests only (Bt)) and system use (system not used (Sn); system used for tests only (St); system used continuously for 21 days (Sc)). The effect of learning-by-doing was assessed by comparing the performance of eight subjects (BtSt) who only used the mobile substitution device for the tests, to that of three subjects who, in addition, practiced with it for four hours daily in their normal life (BtSc and BcSc); two subjects who did not use the device at all (BtSn and BcSn) allowed assessment of its use in the tasks we employed. The impact of long-term sensory deprivation was investigated by blindfolding three of those participants throughout the three week-long experiment (BcSn, BcSn/c, and BcSc); the other ten subjects were only blindfolded during the tests (BtSn, BtSc, and the eight BtSt subjects). Expectedly, the two subjects who never used the substitution device, while fast in finding the targets, had chance accuracy, whereas subjects who used the device were markedly slower, but showed much better accuracy which improved significantly across our four testing sessions. The three subjects who freely used the device daily as well as during tests were faster and more accurate than those who used it during tests only; however, long-term blindfolding did not notably influence performance.</p>


<h3>Conclusions</h3>
<a id="article1.front1.article-meta1.abstract1.sec3.p1" name="article1.front1.article-meta1.abstract1.sec3.p1"></a><p>Together, the results demonstrate that the device allowed blindfolded subjects to increasingly know where something was by listening, and indicate that practice in naturalistic conditions effectively improved “visual” localization performance.</p>

</div>


<div class="articleinfo"><p><strong>Citation: </strong>Proulx MJ, Stoerig P, Ludowig E, Knoll I (2008) Seeing ‘Where’ through the Ears: Effects of Learning-by-Doing and Long-Term Sensory Deprivation on Localization Based on Image-to-Sound Substitution. PLoS ONE 3(3):
          e1840.
            doi:10.1371/journal.pone.0001840</p><p><strong>Editor: </strong>Malika Auvray, University of Oxford, United Kingdom</p><p><strong>Received:</strong> September 13, 2007; <strong>Accepted:</strong> February 14, 2008; <strong>Published:</strong> March 26, 2008</p><p><strong>Copyright:</strong> © 2008 Proulx et al. This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p><p><strong>Funding: </strong>We gratefully acknowledge financial support by the Volkswagen Stiftung (I/80 742) and the Anton-Betz-Stiftung. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</p><p><strong>Competing interests:</strong> The authors have declared that no competing interests exist.</p></div>





<div id="section1" class="section"><a id="s1" name="s1" toc="s1" title="Introduction"></a><h3>Introduction</h3><a id="article1.body1.sec1.p1" name="article1.body1.sec1.p1"></a><p>Vision is “to know what is where by looking” (p. 3 <a href="#pone.0001840-Marr1">[1]</a>). This definition is intuitively appealing because it describes two central purposes of vision: object recognition and localization. The blind have to rely largely on auditory and tactile information for finding and identifying objects. Sensory substitution aims at supplementing the available aids (such as the cane, echolocation devices, and Braille script) by converting visual information into a tactile or auditory format (see <a href="#pone.0001840-BachyRita1">[2]</a> for a general review). The resultant tactile arrays or sound patterns inform a blind person whether, where, and what silent objects fall within the field of view of the camera whose input they represent. Although substitution devices are capable of providing both what and where information, most studies have explored the potential of sensory substitution for stimulus discrimination, often using very simple stimuli <a href="#pone.0001840-Arno1">[3]</a>–<a href="#pone.0001840-Ptito1">[7]</a>. As only one study <a href="#pone.0001840-Auvray1">[8]</a> has had their subjects localize and explore real objects with a hand-held camera whose signals were converted into sound patterns, localization performance has hardly been addressed (however see <a href="#pone.0001840-Renier1">[9]</a> for a study of the estimation of distance in depth for simple stimuli using a joystick and computer interface and <a href="#pone.0001840-Segond1">[10]</a> for a study of spatial navigation). Moreover, studies to date have exclusively employed in-session learning to show that training improves discrimination performance over sessions in blind participants as well as in subjects blindfolded during training <a href="#pone.0001840-Arno1">[3]</a>–<a href="#pone.0001840-Auvray1">[8]</a>.</p>
<a id="article1.body1.sec1.p2" name="article1.body1.sec1.p2"></a><p>The present study used an image-to-sound conversion program, The vOICe <a href="#pone.0001840-Meijer1">[11]</a>, to examine the perceptual learning of manual localization based on the sounds generated by translating the images from a video camera hidden in sunglasses (see <a href="#pone-0001840-g001">Figure 1</a>). The use of a head-mounted rather than a handheld camera (cf. <a href="#pone.0001840-Auvray1">[8]</a>) requires a different coordinate system and different sensory-motor contingencies for using the camera to allow one to grasp objects. Localization was assessed in three experiments. In the first, the participants had to manually indicate the location of a lit LED in a horizontal array of 18 possible target locations. The second experiment examined whether the learning would transfer to a more challenging LED task where there were 164 possible target locations. In the third experiment objects that were placed singly on a large table had to be located and grasped. On the basis of subjects' grasping precision we were also able to consider the ability of participants to take account of features of the object, such as its size.</p>
<div class="figure" id="pone-0001840-g001"><div class="img"><a name="pone-0001840-g001" title="Click for larger image " href="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001840.g001&amp;representation=PNG_M" data-doi="info:doi/10.1371/journal.pone.0001840" data-uri="info:doi/10.1371/journal.pone.0001840.g001"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001840.g001&amp;representation=PNG_I" alt="thumbnail" class="thumbnail"></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001840.g001/powerpoint">
                    PPT
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001840.g001/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001840.g001/largerimage">
                    PNG
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001840.g001/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pone.0001840.g001.PNG_L"></span>)
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001840.g001/originalimage">
                    TIFF
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001840.g001/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pone.0001840.g001.TIF"></span>)
                </a></li></ul></div><p><strong>Figure 1.  <span>An illustration of the sensory substitution device and its conversion principles.</span></strong></p><a id="article1.body1.sec1.fig1.caption1.p1" name="article1.body1.sec1.fig1.caption1.p1"></a><p>A) The vOICe program is installed on the notebook computer in the backpack. The camera is hidden in the glasses and the earphones provide the result of the image-to-sound conversion. B) Conversion principles for The vOICe.</p>
<span>doi:10.1371/journal.pone.0001840.g001</span></div><a id="article1.body1.sec1.p3" name="article1.body1.sec1.p3"></a><p>Unlike all published studies on sensory substitution that employed structured in-session learning to show that training improves the performance of both blind and blindfolded subjects, we here compared within-session learning to a learning-by-doing approach in naturalistic conditions (see <a href="#pone-0001840-g002">Figure 2</a> for the conditions that defined our subjects). This naturalistic learning was investigated by providing the mobile substitution system to three subjects for use in their daily lives. Two of these subjects had the system continuously for 21 days (BtSc and BcSc); the third had it for the final 10 days only (BcSn/c), and therefore provided a within-subject assessment of the effects of daily practice on performance. Eight subjects used the system during the tests only (BtSt); this group essentially replicated the normal subject group in other studies of sensory substitution (e.g. <a href="#pone.0001840-Auvray1">[8]</a>) who only benefit from in-session practice. Two final subjects did not use the system at all (BtSn and BcSn). The three groups allowed us to assess the effect of using the system during the tests, and to compare in-session to in-session plus naturalistic learning.</p>
<div class="figure" id="pone-0001840-g002"><div class="img"><a name="pone-0001840-g002" title="Click for larger image " href="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001840.g002&amp;representation=PNG_M" data-doi="info:doi/10.1371/journal.pone.0001840" data-uri="info:doi/10.1371/journal.pone.0001840.g002"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001840.g002&amp;representation=PNG_I" alt="thumbnail" class="thumbnail"></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001840.g002/powerpoint">
                    PPT
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001840.g002/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001840.g002/largerimage">
                    PNG
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001840.g002/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pone.0001840.g002.PNG_L"></span>)
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001840.g002/originalimage">
                    TIFF
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001840.g002/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pone.0001840.g002.TIF"></span>)
                </a></li></ul></div><p><strong>Figure 2.  <span>Assignment of subjects to the experimental conditions.</span></strong></p><a id="article1.body1.sec1.fig2.caption1.p1" name="article1.body1.sec1.fig2.caption1.p1"></a><p>Subject assignment to the experimental conditions was created by partially crossing blindfolding with use of the sensory substitution system. Bc = Blindfolding continuous (for 21 days); Bt = Blindfolding test-only; Sc = System use continuous (in daily life and for all tests); St = System test-only (for all test but not in daily life); Sn = no System used (for the tests or in daily life). Note that subject BcSn/c was a cross between BcSc and BcSn because he was blindfolded continuously for 21 days, did not have the system for the first 11 days, but did use it in daily life and the tests for the final 10 days. The colors represent the extent of system use in <a href="#pone-0001840-g003">Figures 3</a> to <a href="#pone-0001840-g004"></a><a href="#pone-0001840-g005">5</a>, the shapes the extent of blindfolding in <a href="#pone-0001840-g003">Figures 3</a> and <a href="#pone-0001840-g005">5</a>.</p>
<span>doi:10.1371/journal.pone.0001840.g002</span></div><a id="article1.body1.sec1.p4" name="article1.body1.sec1.p4"></a><p>Finally, we studied the impact of sensory deprivation on the learning of sensory substitution by blindfolding three of the thirteen subjects (BcSn, BcSn/c, and BcSc) for the entirety of 21 days (24 hours per day), the longest, non-clinical period of visual deprivation in the literature (for a previously long duration of 5 days, see <a href="#pone.0001840-PascualLeone1">[12]</a>). The ten others were only blindfolded during the laboratory tasks, similar to previous research (BtSn, BtSc, and the eight BtSt subjects). Long-term rather than test-only blindfolding was used in three participants because it may enhance perceptual learning both for the remaining modalities that have to compensate for the visual deprivation, and by rendering subjects more dependent on the system (e.g., <a href="#pone.0001840-Kauffman1">[13]</a>–<a href="#pone.0001840-Lessard1">[14]</a>).</p>
<a id="article1.body1.sec1.p5" name="article1.body1.sec1.p5"></a><p>Taken together, the study has three contributions to the literature on sensory substitution: First it focused on localization (see also <a href="#pone.0001840-Auvray1">[8]</a>, <a href="#pone.0001840-Renier1">[9]</a>, <a href="#pone.0001840-Segond1">[10]</a>); second, it compared in-session to naturalistic learning by providing some subjects with the equipment necessary for practicing with the device in their daily lives; and third it examined the effects of long-term sensory deprivation on the learning of sensory substitution.</p>
</div>

<div id="section2" class="section"><a id="s2" name="s2" toc="s2" title="Results"></a><h3>Results</h3>
<h4>Experiment 1: Horizontally Located Light Source</h4>
<h5>System use (continuous, test-only or not at all).</h5><a id="article1.body1.sec2.sec1.sec1.p1" name="article1.body1.sec2.sec1.sec1.p1"></a><p>Sn subjects (who wore no device for the tests; green shapes in <a href="#pone-0001840-g003">Figure 3</a>) were much faster than those who used the system; however, accuracy was expectedly at chance level. There were 18 LEDs that could potentially be the target, and the subjects without the system had to press almost as many, on average, before hitting the target LED (mean 15 LEDs per trial). There was no change in accuracy (<em>r</em> = 0.13, <em>p</em> = 0.36, <em>p</em><sub>rep</sub> = 0.60, <em>d</em> = 0.26) over the sessions. In contrast, response times improved strongly as a function of session number (<em>r</em> = −0.70, <em>p</em> = 0.012, <em>p</em><sub>rep</sub> = 0.95, <em>d</em> = 1.96) for the Sn subjects, presumably because they learned to hit as many LEDs as they could, as fast as they could, and increasingly used both hands for the task.</p>
<div class="figure" id="pone-0001840-g003"><div class="img"><a name="pone-0001840-g003" title="Click for larger image " href="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001840.g003&amp;representation=PNG_M" data-doi="info:doi/10.1371/journal.pone.0001840" data-uri="info:doi/10.1371/journal.pone.0001840.g003"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001840.g003&amp;representation=PNG_I" alt="thumbnail" class="thumbnail"></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001840.g003/powerpoint">
                    PPT
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001840.g003/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001840.g003/largerimage">
                    PNG
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001840.g003/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pone.0001840.g003.PNG_L"></span>)
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001840.g003/originalimage">
                    TIFF
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001840.g003/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pone.0001840.g003.TIF"></span>)
                </a></li></ul></div><p><strong>Figure 3.  <span>The horizontal LED task results from Experiment 1.</span></strong></p><a id="article1.body1.sec2.sec1.sec1.fig1.caption1.p1" name="article1.body1.sec2.sec1.sec1.fig1.caption1.p1"></a><p>Top) Response times as a function of session. The inserted array shows the potential target LEDs (black) in the perimeter. The subjects knew that these were confined to the horizontal meridian. Bottom) Accuracy is plotted as a function of the number of LEDs pushed on a given trial until hitting the target (“1” is perfect performance). All error bars denote standard error of the means in this and all figures.</p>
<span>doi:10.1371/journal.pone.0001840.g003</span></div><a id="article1.body1.sec2.sec1.sec1.p2" name="article1.body1.sec2.sec1.sec1.p2"></a><p>The data of the BtSt subjects who only used the device for the testing session are plotted with blue triangles in <a href="#pone-0001840-g003">Figure 3</a>. This group took much longer to find the target LED than the Sn subjects, and their response times did not decrease across sessions (<em>r</em> = −0.15, <em>p</em> = 0.211, <em>p</em><sub>rep</sub> = 0.71, <em>d</em> = 0.30). In further contrast to the Sn subjects, their accuracy improved from session to session (mean n trials 8.4 in session 1 versus 3.2 in session 4; <em>r</em> = −0.43, <em>p</em> = 0.007, <em>p</em><sub>rep</sub> = 0.96, <em>d</em> = 0.95). The Sc subjects who used the device during the tests as well as in daily life were faster (mean RT 38 s; red shapes in <a href="#pone-0001840-g003">Figure 3</a>) than the St (80.5 s), but slower than the Sn subjects s (mean 8 s). These Sc subjects showed excellent accuracy which improved significantly (mean n trials 3.3 in session 1 versus 1.5 in session 4; <em>r</em> = −0.71, <em>p = </em>0.011, <em>p</em><sub>rep</sub> = 0.95, <em>d</em> = 2.0) along with search time (<em>r</em> = −0.65, <em>p = </em>0.022, <em>p</em><sub>rep</sub> = 0.92, <em>d</em> = 1.71) across sessions. Note that they became almost as fast (mean 22 s) in Session 4 as an Sn subject who systematically pressed all LEDs in Session 1 (subject BcSn/c, mean 17.5 s). That Sc subjects improved both in accuracy and speed suggests that their daily use of the system outside of the testing sessions, and the many opportunities it provided for learning to adjust their image-to-sound guided behavior to the camera's field of view, contributed to their significant improvement on both counts in this laboratory task.</p>

<h5>Blindfolding (continuous or test-only).</h5><a id="article1.body1.sec2.sec1.sec2.p1" name="article1.body1.sec2.sec1.sec2.p1"></a><p>We compared the Bc and Bt subjects to investigate whether continued visual deprivation affected performance in this task. Response times did not decrease substantially for either group (Bc, <em>r</em> = −0.13, <em>p</em> = 0.343, <em>p</em><sub>rep</sub> = 0.61, <em>d</em> = 0.26; Bt, <em>r</em> = −0.12, <em>p</em> = 0.226, <em>p</em><sub>rep</sub> = 0.70, <em>d</em> = 0.24), but, as seen in <a href="#pone-0001840-g003">Figure 3</a>, was lower for the Bc subjects throughout. Although accuracy for both the Bt and Bc groups improved similarly from the first (9.3 trials-to-hit for Bc, 8.9 for Bt) to the last (5.4 for Bc, 4.4 for Bt) session, it improved consistently only for the Bt (<em>r</em> = −0.30, <em>p</em> = 0.03, <em>p</em><sub>rep</sub> = 0.91, <em>d</em> = 0.63), but not the Bc subjects (<em>r</em> = −0.29, <em>p</em> = 0.18, <em>p</em><sub>rep</sub> = 0.74, <em>d</em> = 0.61).</p>

<h5>Continuous system use and blindfolding.</h5><a id="article1.body1.sec2.sec1.sec3.p1" name="article1.body1.sec2.sec1.sec3.p1"></a><p>Both Sc subjects performed well and exhibited perceptual learning. The continuously blindfolded subject BcSc initially had higher accuracy than BtSc; both exhibited improvement (BcSc: <em>r</em> = −0.89, <em>p</em> = 0.058, <em>p</em><sub>rep</sub> = 0.87, <em>d</em> = 3.90), though BtSc had a higher correlation between accuracy and testing session (BtSc <em>r</em> = −0.97, <em>p</em> = 0.017, <em>p</em><sub>rep</sub> = 0.93, <em>d</em> = 7.98). Conversely, BtSc had faster RTs to begin with, and though she showed some improvement (<em>r</em> = −0.88, <em>p</em> = 0.063, <em>p</em><sub>rep</sub> = 0.86, <em>d</em> = 3.71), BcSc had a higher correlation (<em>r</em> = −0.95, <em>p</em> = 0.024, <em>p</em><sub>rep</sub> = 0.92, <em>d</em> = 6.08) and better accuracy throughout. Unsurprisingly, in the first two tests where he performed without the system, BcSn/c was fast and inaccurate. When he first used a system in session 3, his search time as well as his accuracy increased dramatically. In session 4, the second session he performed while using the device, his search times already decreased by half, and his localization was as precise as that of the two other Sc subjects who had performed three prior sessions with the system.</p>



<h4>Experiment 2: Hexagonally Located Light Source</h4>
<a id="article1.body1.sec2.sec2.p1" name="article1.body1.sec2.sec2.p1"></a><p>In Experiment 1, all subjects knew that the target LED would be located on the horizontal row of 18 LEDs. To investigate whether the learning would transfer to a task in which subjects did not know where in the perimeter the targets might be, and had to consider all 164 LEDs as possible targets, we used a different arrangement at the end of the 4<sup>th</sup> and final session. The subjects were not informed that only six LEDs were actually used, or that each served as target twice.</p>
<a id="article1.body1.sec2.sec2.p2" name="article1.body1.sec2.sec2.p2"></a><p><a href="#pone-0001840-g004">Figure 4</a> depicts the results. The bottom panel shows the mean number of LEDs pressed up to and including the target.</p>
<div class="figure" id="pone-0001840-g004"><div class="img"><a name="pone-0001840-g004" title="Click for larger image " href="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001840.g004&amp;representation=PNG_M" data-doi="info:doi/10.1371/journal.pone.0001840" data-uri="info:doi/10.1371/journal.pone.0001840.g004"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001840.g004&amp;representation=PNG_I" alt="thumbnail" class="thumbnail"></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001840.g004/powerpoint">
                    PPT
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001840.g004/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001840.g004/largerimage">
                    PNG
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001840.g004/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pone.0001840.g004.PNG_L"></span>)
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001840.g004/originalimage">
                    TIFF
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001840.g004/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pone.0001840.g004.TIF"></span>)
                </a></li></ul></div><p><strong>Figure 4.  <span>The hexagonal LED task results from Experiment 2.</span></strong></p><a id="article1.body1.sec2.sec2.fig1.caption1.p1" name="article1.body1.sec2.sec2.fig1.caption1.p1"></a><p>Each of the six hexagonally arranged LEDs shown in the insert served twice as target, but as subjects were not informed about this array, they had to consider all 164 LEDs as possible targets. Mean response time (top) and number of trials to hit the target (bottom) as a function of the subject's condition (visual deprivation and system use for each subject).</p>
<span>doi:10.1371/journal.pone.0001840.g004</span></div><h5>System use (continuous, test-only or not at all).</h5><a id="article1.body1.sec2.sec2.sec1.p1" name="article1.body1.sec2.sec2.sec1.p1"></a><p>The Sn subjects needed many more trials to hit the target than in the preceding tests (mean trials to hit 73.6), and their response times were relatively fast (mean 45.8 s). As in the ‘horizontal’ task, the BtSt subjects took longer to find the targets than the Sn subjects (BtSt, mean 92.3 s; Sn, mean 45.8 s; <em>t</em>(82) = 3.45, <em>p = </em>0.0004, <em>p</em><sub>rep</sub> = 0.99, <em>d</em> = 0.76; the significant <em>p</em> value for this and all tests is 0.01 when Bonferroni corrected for multiple comparisons), but had much better accuracy (mean 7.4 trials compared to 73.6 trials for Sn; t(23) = 5.97, p&lt;0.0001, p<sub>rep</sub> = 1.0, d = −2.5). The Sc subjects performed even better than the BtSt subjects, pressing no more than one or two LEDs adjacent to the target on almost all of the trials (mean trials to hit 2.5, median 2, mode 1; <em>t</em>(70) = 4.6, <em>p</em>&lt;0.0001, <em>p</em><sub>rep</sub> = 1.0, <em>d</em> = 1.1. Furthermore, their response times were statistically similar to those of Sn subjects (with device, mean 57.5 s; without device, mean 45.8 s; <em>t</em>(58) = 1.1, <em>p = </em>0.13, <em>p</em><sub>rep</sub> = 0.77, <em>d</em> = 0.28), and considerably faster than those of the BtSt subjects (92.3 s versus 57.5 s; <em>t</em>(94) = 2.5, <em>p = </em>0.008, <em>p</em><sub>rep</sub> = 0.96, <em>d</em> = 0.51). The increased difficulty of this task confirms the superior localization performance the Sc subjects had obtained after using the system in daily life for 21 or even just 10 days.</p>

<h5>Blindfolding (continuous or test-only).</h5><a id="article1.body1.sec2.sec2.sec2.p1" name="article1.body1.sec2.sec2.sec2.p1"></a><p>The effect of visual deprivation on the hexagonal LED task, independent of system-use, was mixed. The Bt subjects who were seeing in daily life found the target faster than the Bc subjects who were blindfolded continuously (32 s for Bt versus 67 s for Bc; <em>t</em>(55) = 3.6, <em>p = </em>0.0003, <em>p</em><sub>rep</sub> = 0.99, <em>d</em> = 0.98). The accuracy of the Bt and Bc subjects was not statistically different (<em>t</em>(41) = 1.68, <em>p = </em>0.17, <em>p</em><sub>rep</sub> = 0.75, <em>d</em> = 0.30). However, the numerical trend indicated that the Bc subjects had better accuracy than the Bt subjects (mean n trials to find the target was 26 for Bc versus 39 for Bt).</p>

<h5>Continuous system use and blindfolding.</h5><a id="article1.body1.sec2.sec2.sec3.p1" name="article1.body1.sec2.sec2.sec3.p1"></a><p>The most interesting finding of this experiment, in comparison to Experiment 1, is that the two Sc subjects that used the device daily not only had greater accuracy than the Sn subjects but also had search times that were as fast as or faster than the Sn subjects. The difference in accuracy is clear in <a href="#pone-0001840-g004">Figure 4</a>. Beyond the previous analyses that demonstrated that the response times were not statistically distinguishable for the Sn versus the Sc subjects, it is also interesting to note that subject BtSc was faster than the fastest Sn subject (24 s for BtSc versus 40 s for BtSn; <em>t</em>(12) = 1.65, <em>p = </em>0.062, <em>p</em><sub>rep</sub> = 0.86, <em>d</em> = 0.95).</p>



<h4>Experiment 3: Finding Objects on a Table</h4>
<a id="article1.body1.sec2.sec3.p1" name="article1.body1.sec2.sec3.p1"></a><p>Whereas the first two experiments focused solely on “where” information, the final one also considered object features pertaining to “what” information, such as an object's size and shape. Different ordinary objects were used on each trial, and subjects had to localize and grasp them. This allowed us to analyze search times as well as how directly the subjects reached for the objects and how appropriate their hand grip was for the object.</p>
<h5>System use (continuous, test-only or not at all) and search time.</h5><a id="article1.body1.sec2.sec3.sec1.p1" name="article1.body1.sec2.sec3.sec1.p1"></a><p><a href="#pone-0001840-g005">Figure 5</a> shows the search time data from the third experiment, where subjects had to find various everyday objects, presented one at a time on a table. They exhibited high variability and, for the Sc and Sn subjects, search times correlated weakly with session number (Sc, <em>r</em> = −0.26, <em>p</em> = 0.26, <em>p</em><sub>rep</sub> = 0.68, <em>d</em> = 0.54; Sn, <em>r</em> = −0.29, <em>p</em> = 0.21, <em>p</em><sub>rep</sub> = 0.71, <em>d</em> = 0.61; top panel of <a href="#pone-0001840-g005">Figure 5</a>). The BtSt subjects showed no improvement in search time (<em>r</em> = 0.026, <em>p</em> = 0.44, <em>p</em><sub>rep</sub> = 0.54, <em>d</em> = 0.05). As we had to use different tables for the BtSt subjects and the other participants, absolute search times cannot be compared. Note, however, that the search times for the BtSt and two Sc subjects are very similar on average, suggesting that the difference in table size did not have much impact on the search time results.</p>
<div class="figure" id="pone-0001840-g005"><div class="img"><a name="pone-0001840-g005" title="Click for larger image " href="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001840.g005&amp;representation=PNG_M" data-doi="info:doi/10.1371/journal.pone.0001840" data-uri="info:doi/10.1371/journal.pone.0001840.g005"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001840.g005&amp;representation=PNG_I" alt="thumbnail" class="thumbnail"></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001840.g005/powerpoint">
                    PPT
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001840.g005/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001840.g005/largerimage">
                    PNG
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001840.g005/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pone.0001840.g005.PNG_L"></span>)
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001840.g005/originalimage">
                    TIFF
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001840.g005/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pone.0001840.g005.TIF"></span>)
                </a></li></ul></div><p><strong>Figure 5.  <span>The table task results from Experiment 3.</span></strong></p><a id="article1.body1.sec2.sec3.sec1.fig1.caption1.p1" name="article1.body1.sec2.sec3.sec1.fig1.caption1.p1"></a><p>Top) Mean response time as a function of session. Bottom) Rating of grasp precision as a function of session (1 = indirect; 2 = relatively direct; 3 = direct). The photograph at the top right shows a directed grasp of the target object by a subject (BcSc) using the substitution device.</p>
<span>doi:10.1371/journal.pone.0001840.g005</span></div>
<h5>Blindfolding (continuous or test-only) and search time.</h5><a id="article1.body1.sec2.sec3.sec2.p1" name="article1.body1.sec2.sec3.sec2.p1"></a><p>Variability was also high when considering the Bc versus the Bt subjects, and again there was no clear improvement in search time across session numbers (<em>p</em>&gt;0.25, <em>p</em><sub>rep</sub>&lt;0.70). A comparison of the subjects who used no device (BcSn, BtSn, and BcSn/c during the first three sessions) reveals no advantage of continuous blindfolding (in the absence of using the system) in this task.</p>

<h5>System use (continuous, test-only or not at all) and directed grasping.</h5><a id="article1.body1.sec2.sec3.sec3.p1" name="article1.body1.sec2.sec3.sec3.p1"></a><p>We analyzed the grasping behavior of subjects to determine how much the subjects' grasping took account of the objects' position, size and orientation (see <a href="#pone-0001840-g005">Figure 5</a> bottom panel). The Sn subjects had ratings that corresponded with indirect grasping across all sessions. This reflects their strategy: to slide their hands across the entire table, stepping sideways to reach all edges until an object was discovered tactually. The St subjects started almost as poorly, but increased their directness of grasping up to session 3 where it reached a plateau (<em>r</em> = 0.93, <em>p</em> = .034, <em>p</em><sub>rep</sub> = 0.90, <em>d</em> = 5.06). Finally, the Sc subjects clearly improved directness in their grasping of the object (<em>r</em> = 0.84, <em>p</em> = .005, <em>p</em><sub>rep</sub> = 0.97, <em>d</em> = 3.10).</p>

<h5>Blindfolding (continuous or test-only) and directed grasping.</h5><a id="article1.body1.sec2.sec3.sec4.p1" name="article1.body1.sec2.sec3.sec4.p1"></a><p>There was an improvement in directed grasping across test sessions for the Bt (<em>r</em> = 0.27, <em>p</em> = .048, <em>p</em><sub>rep</sub> = 0.88, <em>d</em> = 0.56) and Bc subjects (<em>r</em> = 0.45, <em>p</em> = .081, <em>p</em><sub>rep</sub> = 0.84, <em>d</em> = 1.01). Note that there were more Bt subjects than Bc (11 versus 2) which resulted in the lower correlation having a lower, and therefore statistically significant, <em>p</em> value. The Bc subjects, however, had the higher correlation and effect size, suggesting that continuous blindfolding had a positive effect on grasping.</p>

<h5>Blindfolding and system use (single subject analyses).</h5><a id="article1.body1.sec2.sec3.sec5.p1" name="article1.body1.sec2.sec3.sec5.p1"></a><p>BtSc performed very precisely from the first session, and BcSc grasped all objects directly in the last one (see <a href="#pone-0001840-g005">Figure 5</a> for an example photograph). BcSn/c performed only a single session with the system. Nevertheless, he also grasped three objects directly, and two relatively direct, suggesting that the practice he had with the system in his daily life, plus perhaps having to adapt to daily life with a blindfold in the absence of the system for the first half of the period, played a substantial role in improving his search strategy as well as his reaching (see supporting online material for video examples of subject BcSn/c in <a href="#pone.0001840.s001">Movie S1</a> and BcSc in <a href="#pone.0001840.s002">Movie S2</a> and <a href="#pone.0001840.s003">Movie S3</a>). BcSn/c also had search times that were faster than BcSc in the final session even though he had less experience with the system overall (30 s for BcSn/c versus 70 s for BcSc; <em>t</em>(4) = 1.66, <em>p = </em>0.085, <em>p</em><sub>rep</sub> = 0.83, <em>d</em> = 1.67). Although this difference is not statistically significant due to low power and might arise because of individual differences between the subjects, there is also a possibility that one who has adapted to sensory deprivation in the absence of using a substitution device (as the blind have) may be able to learn to use such a device more quickly and with better performance.</p>


</div>

<div id="section3" class="section"><a id="s3" name="s3" toc="s3" title="Discussion"></a><h3>Discussion</h3><a id="article1.body1.sec3.p1" name="article1.body1.sec3.p1"></a><p>Here we examined the impact of naturalistic learning and sensory deprivation on the perceptual learning of object localization via image-to-sound substitution. As noted in the Introduction, this study has three primary contributions to the literature on sensory substitution: 1) it focused primarily on the less studied localization (see also <a href="#pone.0001840-Auvray1">[8]</a>, <a href="#pone.0001840-Renier1">[9]</a>, <a href="#pone.0001840-Segond1">[10]</a>); 2) it employed a naturalistic, learning-by-doing approach in addition to the in-session practice that is normally employed, and 3) it featured the longest non-clinical blindfolding of subjects in addition to the standard in-session-only blindfolding.</p>
<a id="article1.body1.sec3.p2" name="article1.body1.sec3.p2"></a><p>1. Localization performance, hitherto only tested with a hand-held camera <a href="#pone.0001840-Auvray1">[8]</a> or a joystick <a href="#pone.0001840-Renier1">[9]</a>, <a href="#pone.0001840-Segond1">[10]</a>, which might also impede one's ability to make free use of one's hands, improved in both Experiments 1 and 3 in subjects who used the system. No such improvement occurred in the Sn subjects who had chance accuracy throughout. These subjects were much faster than those who used the system, at least as long as the target array was limited. The Sn subjects decreased their search times consistently over sessions in Experiment 1 due to employing more effective strategies, such as using two hands to touch the LEDs more quickly. Search times also decreased in the initially much slower St and Sc subjects who were particularly challenged by having to adjust to the smaller field of view of the camera, and to the sweep time of the conversion program, as both required appropriate adaptation of head and, especially in Experiment 3, body movements. Nevertheless, Sc and St subjects had to press fewer LEDs before hitting the target in Experiments 1 and 2, and unlike the Sn subjects, also improved the precision of their grasping of objects in Experiment 3. As our targets were clearly defined – only the target LED was lit, and only one object was positioned on the table at a time – we cannot conclude that a more difficult task, such as finding a particular object among distracters, will be learned as effectively. However, the improvements we observed in the hand posture during reaching gives reason for cautious optimism.</p>
<a id="article1.body1.sec3.p3" name="article1.body1.sec3.p3"></a><p>2. Previous studies only focused on laboratory practice. Our BtSt group essentially replicates this approach, and, as in other reports <a href="#pone.0001840-Arno1">[3]</a>–<a href="#pone.0001840-Segond1">[10]</a>, revealed statistically significant improvements across four sessions. However, the Sc subjects who used the substitution system immersively in their daily life had superior performance to that of the St, in-laboratory, users of the device in all three experiments. Together, the results suggest that the additional daily practice and the opportunities for learning-by-doing in naturalistic conditions it afforded effectively improved performance on the localization tasks we presented. Future research will have to show whether the naturalistic-learning conditions or the additional hours of practice account for the Sc subjects' enhanced performance.</p>
<a id="article1.body1.sec3.p4" name="article1.body1.sec3.p4"></a><p>3. Continuous, rather than test-only visual deprivation might lead to greater perceptual learning of localization than just using the system alone. Although our results are not as straightforward in this respect as a previous study <a href="#pone.0001840-Kauffman1">[13]</a> which found that Braille learning profited markedly from five days of continuous blindfolding, the results from our third experiment suggest that the combination of immersive use and extended sensory deprivation may be particularly effective. Subject BcSn/c, who spent the first half of the experiment blindfolded but only had the device for the second half, exhibited very rapid learning and even had superior localization performance in Experiment 3 over that of BcSc who had the system for the duration of the experiment. Although any conclusion we could draw is tempered by the small number of our Bc subjects, the blind for whom the system is designed, may thus progress faster.</p>
<a id="article1.body1.sec3.p5" name="article1.body1.sec3.p5"></a><p>The learning necessary to use the device involves not only perceptually matching the auditory input to a representation of an object or scene that is derived from vision or touch, but other types of learning as well. Subjects must learn to remap egocentric space to match the camera's viewpoint, angle, and field of view. They must adjust their head and body movements to these properties, so as not to miss a possibly vital part of the scene. In addition, they must learn to adapt their movements to the sweep rate used by the system which only provides a snapshot of the scene every one or two seconds; in fact, many subjects made fast, large head movements in the early testing sessions and noticeably more deliberate, slower, and smaller head movements later in the study. Future studies that try to determine the most effective training protocols will have to address these different types of learning. Moreover, as the adult brain that has been subject to actual, peripheral blindness is very likely different from one that has been exposed to short-term blindfolding <a href="#pone.0001840-Amedi1">[15]</a>, studies with blind subjects are important for understanding the learning that accompanies sensory substitution and for improving such systems for use by the blind.</p>
<a id="article1.body1.sec3.p6" name="article1.body1.sec3.p6"></a><p>In summary, the adult auditory system can learn to localize targets based on an image-to-sound conversion system, and immersive practice holds hope for providing the perceptual learning required to localize things quickly and accurately. Most of our results speak to the question of object localization. However, the increased directness of the grasping in Experiment 3 suggests that the subjects also gained general knowledge of the objects' size and shape. By allowing blindfolded subjects to increasingly hear where silent objects are, the system provides knowledge about what is where by listening.</p>
</div>

<div id="section4" class="section"><a id="s4" name="s4" toc="s4" title="Materials and Methods"></a><h3>Materials and Methods</h3>
<h4>Participants</h4>
<a id="article1.body1.sec4.sec1.p1" name="article1.body1.sec4.sec1.p1"></a><p>Having obtained approval from the University's Ethics Committee, we tested 13 sighted subjects (see <a href="#pone-0001840-g002">Figure 2</a>) with informed verbal consent. All were informed about the principles of image-to-sound conversion, and subjects who used the mobile substitution system only during tests (St, System-test-only) as well as those who additionally used it in daily life (Sc, System-continuously) were instructed in its use. None had prior experience with sensory substitution. As the utility of the substitution system was difficult to judge on solely its own merits, a comparison group (two additional subjects who never used the system (Sn, System-none)) was included. Of the ten subjects who were blindfolded only for the testing sessions (Bt subjects, Blindfolded-test-only, five female, age 23–46 yrs), nine were students, and one was associated with the laboratory. The three subjects who were visually-deprived during the entire experiment (Bc subjects, Blindfolded-continuously, one female, age 25–39 yrs) were selected from among a large number of volunteers; they had to be intrinsically motivated (for instance by having a blind relative) and had to be living with someone who agreed (in writing) to look after them during the experimental period. On day 1, one Bt and one Bc subject were equipped with a substitution device (see <a href="#pone-0001840-g001">Figure 1</a>); subject BcSn/c received his system on day 11 to use for the second half of the period, and thus served as an intra-subject control. All Sc subjects were asked to use the system daily for at least four hours. Compliance was very good, as established through close contact with the experimenters and the daily reports subjects provided. Sc and Sn subjects participated in further experiments during this period <a href="#pone.0001840-Pollok1">[16]</a>, and received financial compensation. All subjects were blindfolded during the laboratory tests where Sc and St subjects used a substitution device.</p>


<h4>Sensory Substitution Device</h4>
<a id="article1.body1.sec4.sec2.p1" name="article1.body1.sec4.sec2.p1"></a><p>Our substitution system consisted of a small video camera hidden in sunglasses (Mace Security Products Eyeglasses Camera ST-137W) and a notebook computer (IBM ThinkPad) that received the camera's digitalized signals, and converted them into sound patterns played to the subject by means of stereo headphones (<a href="#pone-0001840-g001">Figure 1</a>). The camera provided a field-of-view that subtended approximately 39° by 31° of visual angle. The vOICe program uses three major conversion dimensions: 1) laterality is coded by stereo panning and the time provided by the left-to-right scanning transformation of each image (the precision of the time scanning is fixed, and users can choose the rate of scanning to occur every one or two seconds), so that the sound pertaining to an object on the right of the image will be heard late in the scan and predominantly through the right ear; 2) elevation is coded by frequency, so that down is represented by low frequencies and up by high frequencies (an exponential distribution from 500 Hz to 5000 Hz); 3) pixel brightness is coded by loudness (<a href="#pone-0001840-g001">Figure 1b</a>). A single bright object on an otherwise dark surface will thus generate a sound pattern whose loudness reflects its brightness, whose duration and frequency spectrum represent its size, and whose frequency modulations represent its shape (see supporting online material, Movie S4 for an example image converted into sound).</p>


<h4>Statistical analyses</h4>
<a id="article1.body1.sec4.sec3.p1" name="article1.body1.sec4.sec3.p1"></a><p>The study primarily focused on the perceptual learning of sensory substitution. We were therefore interested in how the variables impacted the performance of the subjects over time. A negative correlation (Pearson's <em>r</em>) of search times and errors with testing session was expected if performance improved over the three weeks of the study. For each experiment we first analyzed the data in terms of the manipulation of system use, then the manipulation of blindfolding, and finally we looked at individual subjects to consider the interaction between blindfolding and system use.</p>
<a id="article1.body1.sec4.sec3.p2" name="article1.body1.sec4.sec3.p2"></a><p>All data analyses were conducted using the <em>p</em><sub>rep</sub> statistic <a href="#pone.0001840-Killeen1">[17]</a>. Note that we also provide the standard <em>p</em> statistic for comparison and standard interpretation. We include the <em>p</em><sub>rep</sub> statistic because it overcomes a primary problem with null hypothesis statistical tests (i.e., the inability to accept or reject the null hypothesis), and it also provides a measure of the probability of replicability that is of primary importance in all research, but especially when considering small-<em>n</em> research such as that presented here. Thus, data can be interpreted with the following guideline: the higher the <em>p</em><sub>rep</sub> statistic, the greater the likelihood that the results will be replicable. The values of <em>p</em><sub>rep</sub> are directly proportional to <em>p</em> values, however: Values of <em>p</em><sub>rep</sub> greater than 0.9 are equal to <em>p</em> values significant at an alpha level of 0.05. We also provide effect sizes (using Cohen's <em>d</em>) for an additional evaluation of our results <a href="#pone.0001840-Cohen1">[18]</a>.</p>
<h5>Experiment 1: Horizontally Located Light Source.</h5><a id="article1.body1.sec4.sec3.sec1.p1" name="article1.body1.sec4.sec3.sec1.p1"></a><p>A semi-cylindrical perimeter fitted with touch-sensitive red LED buttons was used for this task (see inset depiction in <a href="#pone-0001840-g003">Figure 3</a>). With a diameter of 90 cm and a radius of 45 cm, it formed a semicircle in the horizontal plane, with 165 LED buttons arranged in a star-like pattern. All subjects were blindfolded during testing, and first moved their hands over the perimeter's inner surface to acquaint themselves with the layout of the LEDs. Seated centrally, they started each trial by pressing a start button located on the table in front of them. This activated one of the LEDs as well as a small loudspeaker at the top-center of the perimeter that began a buzzing sound (500 Hz, adjustable volume) which continued until the subject pressed the appropriate – illuminated – LED button. This response extinguished both the light and the sound, informing the subjects that they had found the target. The subjects that used the vOICe device (St and Sc) could still hear the sound that announced the start and continuation of a new trial, and none reported any difficulty hearing the output of the device as a result of the external steady tone. Subjects were informed about this procedure, and also knew that only the 18 LEDs along the horizontal row would be used. Ten subjects used the audiovisual substitution system for the tests (Bc and Bt), one performed it first twice without, then twice with the system (BcSn/c), and two performed it without the device (BtSn and BcSn). All subjects using a device started with a sweep rate of one image per two seconds, but were free to accelerate the sweep rate to one image/s after one to four series. Whereas the subjects with the device were instructed to localize the LED before attempting to press it, the subjects without the device simply pressed as many LED buttons as necessary until the correct one was reached. A PC recorded each LED button pressed during the search, and measured the time from the onset of the light stimulus to the correct response.</p>
<a id="article1.body1.sec4.sec3.sec1.p2" name="article1.body1.sec4.sec3.sec1.p2"></a><p>Each LED subtended 1.9° at a viewing distance of approximately 45 cm. The experiment used 18 LEDs. They were distributed evenly along the horizontal meridian, with a center-to-center distance of 6.3°; only the distance between the two most central target LEDs was twice as large, because the centralmost LED that normally serves as continuously-lit fixation spot was covered with black felt. Each LED had a luminance of ~8 cd/m<sup>2</sup>, and was illuminated once per series. Ambient luminance was low (0.15–0.5 cd/m<sup>2</sup>) to increase target salience for the subjects who used the device. One or two series were given per session; only BtSc enthusiastically performed five in the second session.</p>

<h5>Experiment 2: Hexagonally Located Light Source.</h5><a id="article1.body1.sec4.sec3.sec2.p1" name="article1.body1.sec4.sec3.sec2.p1"></a><p>The apparatus and procedure for Experiment 2 was similar to that used for Experiment 1, except for the changes noted below.</p>

<h4>Participants</h4>
<a id="article1.body1.sec4.sec3.sec2.sec1.p1" name="article1.body1.sec4.sec3.sec2.sec1.p1"></a><p>The two Sn, the three Sc, and five St subjects participated.</p>


<h4>Apparatus and Procedure</h4>
<a id="article1.body1.sec4.sec3.sec2.sec2.p1" name="article1.body1.sec4.sec3.sec2.sec2.p1"></a><p>As illustrated in the inset depiction in <a href="#pone-0001840-g004">Figure 4</a>, two active LEDs were in the upper quadrants, two on the horizontal, and two in the lower quadrants. All subjects used a sweep rate of 1 image/s.</p>


<h5>Experiment 3: Finding Objects on a Table.</h5><a id="article1.body1.sec4.sec3.sec3.p1" name="article1.body1.sec4.sec3.sec3.p1"></a><p>For each of the five trials per session, a single object was placed on a large table completely covered with black felt-like cloth to provide enhanced contrast to aid the subjects in their search for the objects placed on it. Table size was 2.6×1.4 m for the BtSt subjects who were tested in Düsseldorf, and 2 by 1.1 m for the other five subjects who were tested at the Jülich Research Center where parts of the experiments were conducted. Object position was varied pseudo-randomly, and care was taken to mask any auditory cues to the object's position that could result from hearing the experimenter's footsteps or the placement itself. The subject was asked to try and find the object, and started searching while standing by the long side of the table. Five different objects that varied both in size (e.g. a pen, a CD, a trainer, a large box) and in contrast (a white shirt rolled into a ball, a gray plush mouse) to the cloth were used for each testing period, with new objects selected for each session. The subjects did not know the identity of the objects until they found and grasped them.</p>
<a id="article1.body1.sec4.sec3.sec3.p2" name="article1.body1.sec4.sec3.sec3.p2"></a><p>As in Experiment 1, all subjects were blindfolded. The Sc and St subjects used the device during the tests; the one who had the system for 10 days at the end of the experimental period (BcSn/c) performed it with the device in only the final, fourth testing session. The two Sn subjects performed blindly throughout, but, unlike those using a system, were allowed to slide their hands across the surface of the table to find the objects; as in the previous experiments, Sc and St subjects were asked to use the sound patterns for this purpose. Trials were recorded by digital video camera (Sony Digital Handycam), to time the searches by using the camera's digital clock and to assess the precision of the grasping movements. Note that the data for the second session by BcSc is missing because the camera did not record that session.</p>
<a id="article1.body1.sec4.sec3.sec3.p3" name="article1.body1.sec4.sec3.sec3.p3"></a><p>Grasping for the objects was coded as either: indirect (coded as 1), relatively direct (2), or direct (3) by two raters. Sliding the hands in a sweeping manner, rather than towards an object, was coded as 1. Reaching that was directed in the general vicinity of the object, but was followed by a tactual search, was coded as 2. Direct grasping (3) was attested when the reaching movement was directed at the object, errors were confined to those of depth (over- or under-reaching grasps), and the hand-posture was largely appropriate to the size, shape and orientation of the object. The average across the five objects tested in each session was taken, and then subjected to the analyses and figural depiction described in the Results section. A comparison between the primary and second rater resulted in a high interrater reliability (<em>r</em> = .966; full agreement on 96% of the coded trials).</p>


</div>

<div id="section5" class="section"><a id="s5" name="s5" toc="s5" title="Supporting Information"></a><h3>Supporting Information</h3><div class="figshare_widget" doi="10.1371/journal.pone.0001840"></div><a name="pone.0001840.s001" id="pone.0001840.s001"></a><p class="siTitle"><strong><a href="/article/fetchSingleRepresentation.action?uri=info:doi/10.1371/journal.pone.0001840.s001">Movie S1. </a></strong></p><a id="article1.body1.sec5.supplementary-material1.caption1.p1" name="article1.body1.sec5.supplementary-material1.caption1.p1"></a><p class="preSiDOI">Here the subject that was continuously blindfolded and had the sensory substitution device for only the final session of Experiment 3 is shown directly grasping an object.</p>
<p class="siDoi">doi:10.1371/journal.pone.0001840.s001</p><a id="article1.body1.sec5.supplementary-material1.caption1.p2" name="article1.body1.sec5.supplementary-material1.caption1.p2"></a><p class="postSiDOI">(2.39 MB MOV)</p>
<a name="pone.0001840.s002" id="pone.0001840.s002"></a><p class="siTitle"><strong><a href="/article/fetchSingleRepresentation.action?uri=info:doi/10.1371/journal.pone.0001840.s002">Movie S2. </a></strong></p><a id="article1.body1.sec5.supplementary-material2.caption1.p1" name="article1.body1.sec5.supplementary-material2.caption1.p1"></a><p class="preSiDOI">Here the subject that was continuously blindfolded and had the sensory substitution device continuously is shown directly grasping an object.</p>
<p class="siDoi">doi:10.1371/journal.pone.0001840.s002</p><a id="article1.body1.sec5.supplementary-material2.caption1.p2" name="article1.body1.sec5.supplementary-material2.caption1.p2"></a><p class="postSiDOI">(1.15 MB MPG)</p>
<a name="pone.0001840.s003" id="pone.0001840.s003"></a><p class="siTitle"><strong><a href="/article/fetchSingleRepresentation.action?uri=info:doi/10.1371/journal.pone.0001840.s003">Movie S3. </a></strong></p><a id="article1.body1.sec5.supplementary-material3.caption1.p1" name="article1.body1.sec5.supplementary-material3.caption1.p1"></a><p class="preSiDOI">Here the subject that was continuously blindfolded and had the sensory substitution device continuously is shown directly grasping another object.</p>
<p class="siDoi">doi:10.1371/journal.pone.0001840.s003</p><a id="article1.body1.sec5.supplementary-material3.caption1.p2" name="article1.body1.sec5.supplementary-material3.caption1.p2"></a><p class="postSiDOI">(0.46 MB MPG)</p>
<a name="pone.0001840.s004" id="pone.0001840.s004"></a><p class="siTitle"><strong><a href="/article/fetchSingleRepresentation.action?uri=info:doi/10.1371/journal.pone.0001840.s004">Movie S4. </a></strong></p><a id="article1.body1.sec5.supplementary-material4.caption1.p1" name="article1.body1.sec5.supplementary-material4.caption1.p1"></a><p class="preSiDOI">Here is a video demonstrating the conversion principles in <a href="#pone-0001840-g001">Figure 1</a>. Here an image of three squares is transformed into sound with a sweep rate of two seconds.</p>
<p class="siDoi">doi:10.1371/journal.pone.0001840.s004</p><a id="article1.body1.sec5.supplementary-material4.caption1.p2" name="article1.body1.sec5.supplementary-material4.caption1.p2"></a><p class="postSiDOI">(0.05 MB MPG)</p>
</div>





<div><a id="ack" name="ack" toc="ack" title="Acknowledgments"></a><h3>Acknowledgments</h3>
<a id="article1.back1.ack1.p1" name="article1.back1.ack1.p1"></a><p>We thank the subjects for participating in this challenging study, Manfred Mittelstaedt for programming, and Peter Meijer for freely providing The vOICe (<a href="http://www.seeingwithsound.com">www.seeingwithsound.com</a>) and for comments on a previous draft of this manuscript.</p>
</div><div class="contributions"><a id="authcontrib" name="authcontrib" toc="authcontrib" title="Author Contributions"></a><h3>Author Contributions</h3><p>Conceived and designed the experiments: PS. Performed the experiments: PS EL IK. Analyzed the data: MP. Wrote the paper: MP PS. Other: Financed the study: PS.</p></div><div><a id="references" name="references" toc="references" title="References"></a><h3>References</h3><ol class="references"><li><span class="label">1.
              </span><a name="pone.0001840-Marr1" id="pone.0001840-Marr1"></a>Marr D (1982) Vision: A Computational Investigation into the Human Representation and Processing of Visual Information. San Francisco: WH Freeman.   <ul class="find-nolinks"></ul></li><li><span class="label">2.
              </span><a name="pone.0001840-BachyRita1" id="pone.0001840-BachyRita1"></a>Bach-y-Rita P, Kercel SW (2003) Sensory substitution and the human-machine interface. Trends Cogn Sci  7: 541–546.  <ul class="find" data-citedArticleID="1078244" data-doi="10.1016/j.tics.2003.10.013"><li><a href="http://dx.doi.org/10.1016/j.tics.2003.10.013" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Sensory+substitution+and+the+human-machine+interface." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Sensory+substitution+and+the+human-machine+interface.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">3.
              </span><a name="pone.0001840-Arno1" id="pone.0001840-Arno1"></a>Arno P, Wanet-Defalque M-C, Capelle C, Catalan-Ahumada M, Veraart C (1999) Auditory coding of visual patterns for the blind. Perception  28: 1013–1030.  <ul class="find" data-citedArticleID="1078240" data-doi="10.1068/p2607"><li><a href="http://dx.doi.org/10.1068/p2607" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Auditory+coding+of+visual+patterns+for+the+blind." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Auditory+coding+of+visual+patterns+for+the+blind.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">4.
              </span><a name="pone.0001840-CronlyDillon1" id="pone.0001840-CronlyDillon1"></a>Cronly-Dillon J, Persaud K, Gregory RPF (1999) The perception of visual images encoded in musical form: a study in crossmodal information transfer. Proc R Soc Lond B Biol Sci  266: 2427–2433.  <ul class="find" data-citedArticleID="1078248" data-doi="10.1098/rspb.1999.0942"><li><a href="http://dx.doi.org/10.1098/rspb.1999.0942" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=The+perception+of+visual+images+encoded+in+musical+form%3A+a+study+in+crossmodal+information+transfer." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22The+perception+of+visual+images+encoded+in+musical+form%3A+a+study+in+crossmodal+information+transfer.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">5.
              </span><a name="pone.0001840-CronlyDillon2" id="pone.0001840-CronlyDillon2"></a>Cronly-Dillon J, Persaud K, Blore R (2000) Blind subjects construct conscious mental images of visual scenes encoded in musical form. Proc R Soc Lond B Biol Sci  267: 2231–2238.  <ul class="find" data-citedArticleID="1078250" data-doi="10.1098/rspb.2000.1273"><li><a href="http://dx.doi.org/10.1098/rspb.2000.1273" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Blind+subjects+construct+conscious+mental+images+of+visual+scenes+encoded+in+musical+form." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Blind+subjects+construct+conscious+mental+images+of+visual+scenes+encoded+in+musical+form.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">6.
              </span><a name="pone.0001840-Poirier1" id="pone.0001840-Poirier1"></a>Poirier C, Richard M-A, Tranduy D, Veraart C (2006) Assessment of sensory substitution prosthesis potentialities in minimalist conditions of learning. Appl Cogn Psychol  20: 447–460.  <ul class="find" data-citedArticleID="1078264" data-doi="10.1002/acp.1186"><li><a href="http://dx.doi.org/10.1002/acp.1186" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Assessment+of+sensory+substitution+prosthesis+potentialities+in+minimalist+conditions+of+learning." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Assessment+of+sensory+substitution+prosthesis+potentialities+in+minimalist+conditions+of+learning.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">7.
              </span><a name="pone.0001840-Ptito1" id="pone.0001840-Ptito1"></a>Ptito M, Moesgaard SM, Gjedde A, Kupers R (2005) Crossmodal plasticity revealed by electrotactile stimulation of the tongue in the congenitally blind. Brain  128: 609–614.  <ul class="find" data-citedArticleID="1078268" data-doi="10.1093/brain/awh380"><li><a href="http://dx.doi.org/10.1093/brain/awh380" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Crossmodal+plasticity+revealed+by+electrotactile+stimulation+of+the+tongue+in+the+congenitally+blind." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Crossmodal+plasticity+revealed+by+electrotactile+stimulation+of+the+tongue+in+the+congenitally+blind.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">8.
              </span><a name="pone.0001840-Auvray1" id="pone.0001840-Auvray1"></a>Auvray M, Hanneton S, O'Regan JK (2007) Learning to perceive with a visuo-auditory substitution system: Localisation and object recognition with ‘The vOICe’. Perception  36: 416–430.  <ul class="find" data-citedArticleID="1078242" data-doi="10.1068/p5631"><li><a href="http://dx.doi.org/10.1068/p5631" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Learning+to+perceive+with+a+visuo-auditory+substitution+system%3A+Localisation+and+object+recognition+with+%E2%80%98The+vOICe%E2%80%99." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Learning+to+perceive+with+a+visuo-auditory+substitution+system%3A+Localisation+and+object+recognition+with+%E2%80%98The+vOICe%E2%80%99.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">9.
              </span><a name="pone.0001840-Renier1" id="pone.0001840-Renier1"></a>Renier L, Collignon O, Poirier C, Tranduy D, Vanlierde A, et al.  (2005) Cross-modal activation of visual cortex during depth perception using auditory substitution of vision. Neuroimage  26: 573–580.  <ul class="find" data-citedArticleID="1078270" data-doi="10.1016/j.neuroimage.2005.01.047"><li><a href="http://dx.doi.org/10.1016/j.neuroimage.2005.01.047" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Cross-modal+activation+of+visual+cortex+during+depth+perception+using+auditory+substitution+of+vision." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Cross-modal+activation+of+visual+cortex+during+depth+perception+using+auditory+substitution+of+vision.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">10.
              </span><a name="pone.0001840-Segond1" id="pone.0001840-Segond1"></a>Segond H, Weiss D, Sampaio E (2005) Human spatial navigation via a visuo-tactile sensory substitution system. Perception  34: 1231–1249.  <ul class="find" data-citedArticleID="1078272" data-doi="10.1068/p3409"><li><a href="http://dx.doi.org/10.1068/p3409" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Human+spatial+navigation+via+a+visuo-tactile+sensory+substitution+system." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Human+spatial+navigation+via+a+visuo-tactile+sensory+substitution+system.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">11.
              </span><a name="pone.0001840-Meijer1" id="pone.0001840-Meijer1"></a>Meijer PBL (1992) An experimental system for auditory image representations. IEEE Trans Biomed Eng  39: 112–121.  <ul class="find" data-citedArticleID="1078260" data-doi="10.1109/10.121642"><li><a href="http://dx.doi.org/10.1109/10.121642" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=An+experimental+system+for+auditory+image+representations." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22An+experimental+system+for+auditory+image+representations.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">12.
              </span><a name="pone.0001840-PascualLeone1" id="pone.0001840-PascualLeone1"></a>Pascual-Leone A, Hamilton R (2001) The metamodal organization of the brain. Prog Brain Res  134: 1–19.  <ul class="find" data-citedArticleID="1078262" data-doi="10.1016/s0079-6123(01)34028-1"><li><a href="http://dx.doi.org/10.1016/s0079-6123(01)34028-1" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=The+metamodal+organization+of+the+brain." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22The+metamodal+organization+of+the+brain.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">13.
              </span><a name="pone.0001840-Kauffman1" id="pone.0001840-Kauffman1"></a>Kauffman T, Théoret H, Pascual-Leone A (2002) Braille character discrimination in blindfolded human subjects. NeuroReport  13: 571–574.  <ul class="find" data-citedArticleID="1078252" data-doi="10.1097/00001756-200204160-00007"><li><a href="http://dx.doi.org/10.1097/00001756-200204160-00007" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Braille+character+discrimination+in+blindfolded+human+subjects." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Braille+character+discrimination+in+blindfolded+human+subjects.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">14.
              </span><a name="pone.0001840-Lessard1" id="pone.0001840-Lessard1"></a>Lessard N, Paré M, Lepore F, Lassonde M (1998) Early-blind human subjects localize sound sources better than sighted subjects. Nature  395: 278–280.  <ul class="find" data-citedArticleID="1078256" data-doi="10.1038/26228"><li><a href="http://dx.doi.org/10.1038/26228" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Early-blind+human+subjects+localize+sound+sources+better+than+sighted+subjects." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Early-blind+human+subjects+localize+sound+sources+better+than+sighted+subjects.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">15.
              </span><a name="pone.0001840-Amedi1" id="pone.0001840-Amedi1"></a>Amedi A, Merabet LB, Bermpohl F, Pascual-Leone A (2005) The occipital cortex in the blind: Lessons about plasticity and vision. Curr Dir Psychol Sci  14: 306–311.  <ul class="find" data-citedArticleID="1078238" data-doi="10.1111/j.0963-7214.2005.00387.x"><li><a href="http://dx.doi.org/10.1111/j.0963-7214.2005.00387.x" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=The+occipital+cortex+in+the+blind%3A+Lessons+about+plasticity+and+vision." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22The+occipital+cortex+in+the+blind%3A+Lessons+about+plasticity+and+vision.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">16.
              </span><a name="pone.0001840-Pollok1" id="pone.0001840-Pollok1"></a>Pollok B, Schnitzler I, Stoerig P, Mierdorf T, Schnitzler A (2005) Image-to-sound conversion: experience-induced plasticity in auditory cortex of blindfolded adults. Exp Brain Res  167: 287–291.  <ul class="find" data-citedArticleID="1078266" data-doi="10.1007/s00221-005-0060-8"><li><a href="http://dx.doi.org/10.1007/s00221-005-0060-8" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Image-to-sound+conversion%3A+experience-induced+plasticity+in+auditory+cortex+of+blindfolded+adults." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Image-to-sound+conversion%3A+experience-induced+plasticity+in+auditory+cortex+of+blindfolded+adults.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">17.
              </span><a name="pone.0001840-Killeen1" id="pone.0001840-Killeen1"></a>Killeen PR (2005) An alternative to null-hypothesis significance tests. Psychol Sci  16: 345–353.  <ul class="find" data-citedArticleID="1078254" data-doi="10.1111/j.0956-7976.2005.01538.x"><li><a href="http://dx.doi.org/10.1111/j.0956-7976.2005.01538.x" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=An+alternative+to+null-hypothesis+significance+tests." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22An+alternative+to+null-hypothesis+significance+tests.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">18.
              </span><a name="pone.0001840-Cohen1" id="pone.0001840-Cohen1"></a>Cohen J (1988) Statistical power analysis for the behavioral sciences (2nd ed). Hillsdale, NJ: Lawrence Erlbaum Associates.   <ul class="find-nolinks"></ul></li></ol></div>

  </div>

      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001840.XML" value="67790"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001840.PDF" value="286221"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001840.g001.PNG_L" value="911955"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001840.g001.PNG_M" value="108344"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001840.g001.PNG_S" value="8056"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001840.g001.TIF" value="1850102"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001840.g001.PNG_I" value="39248"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001840.g002.PNG_L" value="58651"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001840.g002.PNG_M" value="62396"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001840.g002.PNG_S" value="9731"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001840.g002.TIF" value="447986"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001840.g002.PNG_I" value="24517"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001840.g003.PNG_L" value="185456"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001840.g003.PNG_M" value="87289"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001840.g003.PNG_S" value="12390"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001840.g003.TIF" value="601248"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001840.g003.PNG_I" value="47992"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001840.g004.PNG_L" value="92974"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001840.g004.PNG_M" value="53564"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001840.g004.PNG_S" value="10111"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001840.g004.TIF" value="742800"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001840.g004.PNG_I" value="40187"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001840.g005.PNG_L" value="69895"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001840.g005.PNG_M" value="66282"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001840.g005.PNG_S" value="11472"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001840.g005.TIF" value="554400"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001840.g005.PNG_I" value="40226"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001840.s001.MOV" value="2389410"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001840.s002.MPG" value="1153850"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001840.s003.MPG" value="462491"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001840.s004.MPG" value="46508"/>

</div>
<div class="sidebar">

  <div class="article-actions cf">
      <div class="download">
        <span class="btn"><a href="/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0001840&amp;representation=PDF" title="Download" target="_blank">Download PDF</a></span>
      </div>
      <div class="btn-reveal dropdown">
        <div class="dropdown-icon">
          <span class="btn">&nbsp;</span>
        </div>

        <div class="content">
          <ul class="bullet">
            <li><a href="/article/citationList.action?articleURI=info%3Adoi%2F10.1371%2Fjournal.pone.0001840" title="Download citations">Citation</a></li>
            <li><a href="/article/fetchObjectAttachment.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0001840&amp;representation=XML" title="Download article XML">XML</a></li>
          </ul>
        </div>
      </div> <!-- end btn-reveal dropdown-->


    <div class="btn-reveal flt-l">
        <span class="btn">Print</span>
        <div class="content">
            <ul class="bullet">
                <li id="print-article"><a href="#" onclick="if(typeof(_gaq) != 'undefined'){ _gaq.push(['_trackEvent','Article', 'Print', 'Click']); } window.print(); return false;" title="Print Article">Print article</a></li>
                <li>
                  <a href="https://www.odysseypress.com/onlinehost/reprint_order.php?type=A&page=0&journal=7&doi=10.1371/journal.pone.0001840&volume=&issue=&title=Seeing ‘Where’ through the Ears: Effects of Learning-by-Doing and Long-Term Sensory Deprivation on Localization Based on Image-to-Sound Substitution&author_name=Michael%20J.%20Proulx%2C%20Petra%20Stoerig%2C%20Eva%20Ludowig%2C%20Inna%20Knoll&start_page=1&end_page=8" title="Odyssey Press">EzReprint</a>
                </li>
            </ul>
        </div>
    </div>

    <div class="btn-reveal flt-r">
        <span class="btn">Share</span>
        <div class="content">
            <ul class="social">
                <li><a href="http://www.reddit.com/submit?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0001840" target="_blank" title="Submit to Reddit"><img src="/images/icon.reddit.16.png" width="16" height="16" alt="Reddit">Reddit</a></li>

                <li><a href="https://plus.google.com/share?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0001840" target="_blank" title="Share on Google+"><img src="/images/icon.gplus.16.png" width="16" height="16" alt="Google+">Google+</a></li>

                <li><a href="http://www.stumbleupon.com/submit?url=http%3A%2F%2Fwww.plosone.org%2Farticle%2Finfo%253Adoi%252F10.1371%252Fjournal.pone.0001840" target="_blank" title="Add to StumbleUpon"><img src="/images/icon.stumble.16.png" width="16" height="16" alt="StumbleUpon">StumbleUpon</a></li>

                <li><a href="http://www.facebook.com/share.php?u=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0001840&amp;t=Seeing%20%E2%80%98Where%E2%80%99%20through%20the%20Ears%3A%20Effects%20of%20Learning-by-Doing%20and%20Long-Term%20Sensory%20Deprivation%20on%20Localization%20Based%20on%20Image-to-Sound%20Substitution" target="_blank" title="Share on Facebook"><img src="/images/icon.fb.16.png" width="16" height="16" alt="Facebook">Facebook</a></li>

                <li><a href="http://www.linkedin.com/shareArticle?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0001840&title=Seeing%20%E2%80%98Where%E2%80%99%20through%20the%20Ears%3A%20Effects%20of%20Learning-by-Doing%20and%20Long-Term%20Sensory%20Deprivation%20on%20Localization%20Based%20on%20Image-to-Sound%20Substitution&summary=Checkout%20this%20article%20I%20found%20at%20PLOS" target="_blank" title="Add to LinkedIn"><img src="/images/icon.linkedin.16.png" width="16" height="16" alt="Mendeley">LinkedIn</a></li>

                <li><a href="http://www.citeulike.org/posturl?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0001840&amp;title=Seeing%20%E2%80%98Where%E2%80%99%20through%20the%20Ears%3A%20Effects%20of%20Learning-by-Doing%20and%20Long-Term%20Sensory%20Deprivation%20on%20Localization%20Based%20on%20Image-to-Sound%20Substitution" target="_blank" title="Add to CiteULike"><img src="/images/icon.cul.16.png" width="16" height="16" alt="CiteULike">CiteULike</a></li>

                <li><a href="http://www.mendeley.com/import/?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0001840" target="_blank" title="Add to Mendeley"><img src="/images/icon.mendeley.16.png" width="16" height="16" alt="Mendeley">Mendeley</a></li>

                <li><a href="https://www.pubchase.com/library?add_aid=10.1371%2Fjournal.pone.0001840&amp;source=plos" target="_blank" title="Add to PubChase"><img src="/images/icon.pc.16.png" width="16" height="16" alt="PubChase">PubChase</a></li>


                <script type="text/javascript">
                    // replace tweet with one that's pre-shortened to 140 chars
                    function truncateTweetText() {
                        var twtTitle = 'Seeing ‘Where’ through the Ears: Effects of Learning-by-Doing and Long-Term Sensory Deprivation on Localization Based on Image-to-Sound Substitution';
                        var twtUrl = 'http://dx.plos.org/10.1371/journal.pone.0001840';
                        // all URLs posted to twitter get auto-shortened to 20 chars.
                        var maxLength = 140 - (20 + 1);
                        // truncate the title to include space for twtTag and ellipsis (here, 10 = tag length + space + ellipsis)
                        if (twtTitle.length > maxLength) { twtTitle = twtTitle.substr(0, (maxLength - 10)) + '...'; }
                        // set the href to use the shortened tweet
                        $('#twitter-share-link').prop('href', 'http://twitter.com/intent/tweet?text=' + encodeURIComponent('#PLOSONE: ' + twtTitle + ' ' + twtUrl));
                    }
                </script>
                <li><a href="http://twitter.com/intent/tweet?text=#PLOSONE%3A%20Seeing%20%E2%80%98Where%E2%80%99%20through%20the%20Ears%3A%20Effects%20of%20Learning-by-Doing%20and%20Long-Term%20Sensory%20Deprivation%20on%20Localization%20Based%20on%20Image-to-Sound%20Substitution http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0001840" onclick="truncateTweetText();" target="_blank" title="Share on Twitter" id="twitter-share-link"><img src="/images/icon.twtr.16.png" width="16" height="16" alt="Twitter">Twitter</a></li>

                <li><a href="/article/email/info%3Adoi%2F10.1371%2Fjournal.pone.0001840" title="Email this article"><img src="/images/icon.email.16.png" width="16" height="16" alt="Email">Email</a></li>
            </ul>
        </div>
    </div><!--end btn-reveal flt-r-->
</div><!-- end article-actions-->

<!-- begin Crossmark -->

<a id="open-crossmark" href="#" style="margin-top: -28px; display:block"><img style="border: 0; display: none;
 padding: 10px 0 18px 0;"  id="crossmark-icon" src="/images/logo-crossmark-bw.png" /></a>
<div id="crossmark-dialog" style="display: none;" title="">
    <!-- the external CrossMark data is loaded inside this iframe -->
    <iframe id="crossmark-dialog-frame" frameborder="0"></iframe>
</div>

<!-- end crossmark -->


<div class="block" id="subject-area-sidebar-block">
    <div class="header">
        <h3>Subject Areas</h3><div title="More information" id="subject-area-sidebar-block-help-icon"><img align="right"
                                                                                                           alt="info" src="/images/button_info.png"/><div id="subject-area-sidebar-block-help"><img align="right"
                                                                                                                                                                                                    src="/images/button_info.png"/><p>
        <b>We want your feedback.</b> Do these subject areas make sense for this article? If not, click the flag
        next to the incorrect subject area and we will review it. Thanks for your help!
    </p></div></div>
    </div>


    <ul id="subject-area-sidebar-list">

















          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Blindness%22" title="Search for articles in the subject area:'Blindness'"><div class="flagText">Blindness</div></a>
              <div data-categoryid="19047" data-articleid="26504"
                   data-categoryname="Blindness"
                   class="flagImage" title="Flag 'Blindness' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Cameras%22" title="Search for articles in the subject area:'Cameras'"><div class="flagText">Cameras</div></a>
              <div data-categoryid="18833" data-articleid="26504"
                   data-categoryname="Cameras"
                   class="flagImage" title="Flag 'Cameras' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Computers%22" title="Search for articles in the subject area:'Computers'"><div class="flagText">Computers</div></a>
              <div data-categoryid="1957" data-articleid="26504"
                   data-categoryname="Computers"
                   class="flagImage" title="Flag 'Computers' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Learning%22" title="Search for articles in the subject area:'Learning'"><div class="flagText">Learning</div></a>
              <div data-categoryid="20059" data-articleid="26504"
                   data-categoryname="Learning"
                   class="flagImage" title="Flag 'Learning' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Perimeters%22" title="Search for articles in the subject area:'Perimeters'"><div class="flagText">Perimeters</div></a>
              <div data-categoryid="36323" data-articleid="26504"
                   data-categoryname="Perimeters"
                   class="flagImage" title="Flag 'Perimeters' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Sensory+deprivation%22" title="Search for articles in the subject area:'Sensory deprivation'"><div class="flagText">Sensory deprivation</div></a>
              <div data-categoryid="46471" data-articleid="26504"
                   data-categoryname="Sensory deprivation"
                   class="flagImage" title="Flag 'Sensory deprivation' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Sensory+perception%22" title="Search for articles in the subject area:'Sensory perception'"><div class="flagText">Sensory perception</div></a>
              <div data-categoryid="46099" data-articleid="26504"
                   data-categoryname="Sensory perception"
                   class="flagImage" title="Flag 'Sensory perception' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Vision%22" title="Search for articles in the subject area:'Vision'"><div class="flagText">Vision</div></a>
              <div data-categoryid="32965" data-articleid="26504"
                   data-categoryname="Vision"
                   class="flagImage" title="Flag 'Vision' as inappropriate"></div>
          </li>
    </ul>
</div>

<div class="ad">
    <div class="title">Advertisement</div>






  <iframe id='a0852f54' name='a0852f54'
    src='http://ads.plos.org/www/delivery/afr.php?zoneid=381&amp;cb=3390'
    frameborder='0' scrolling='no' width='160' height='600'>
    <a href='http://ads.plos.org/www/delivery/ck.php?n=a0852f54&amp;cb=440'
      target='_top'><img src='http://ads.plos.org/www/delivery/avw.php?zoneid=381&amp;cb=2892&amp;n=a0852f54'
      border='0' alt=''/>
    </a>
  </iframe>



</div>

<div id="twitter-alm-timeline" class="twitter-alm-timeline"></div>

<div class="block sidebar-comments">
    <div class="header">
        <h3>Comments</h3>
    </div>
      <p><a href="/annotation/listThread.action?root=7877">Academic Editor's comments (Malika Auvray)</a><br>Posted by PLoS_ONE_Group</p>
</div>

</div><!-- sidebar -->
    </div>
  </div>
</div>
<script src="http://wl.figshare.com/static/p_widget.js" type="text/javascript"></script><div id="pageftr">
  <div class="ftr-cols cf">
    <div class="col col-1">
      <img src="/images/logo-plos-footer.png" alt="PLOS Logo" class="logo" />
      <p><a href="/static/releaseNotes">Ambra 2.9.16</a> Managed Colocation provided <br />by <a href="http://www.isc.org/">Internet Systems Consortium</a>.<p>
      <div class="nav nav-aux">
        <a href="/static/privacy">Privacy Policy</a> |
        <a href="/static/terms">Terms of Use</a> |
        <a href="http://www.plos.org/advertise/">Advertise</a> |
        <a href="http://www.plos.org/about/media-inquiries/">Media Inquiries</a>
      </div>
    </div>
    <div class="col col-2">
      <p><a href="http://www.plos.org/publications/journals/">Publications</a></p>
      <div class="nav">
        <ul>
          <li><a href="http://www.plosbiology.org">PLOS Biology</a></li>
          <li><a href="http://www.plosmedicine.org">PLOS Medicine</a></li>
          <li><a href="http://www.ploscompbiol.org">PLOS Computational Biology</a></li>
          <li><a href="http://currents.plos.org">PLOS Currents</a></li>
          <li><a href="http://www.plosgenetics.org">PLOS Genetics</a></li>
          <li><a href="http://www.plospathogens.org">PLOS Pathogens</a></li>
          <li><a href="http://www.plosone.org">PLOS ONE</a></li>
          <li><a href="http://www.plosntds.org">PLOS Neglected Tropical Diseases</a></li>
        </ul>
      </div>
    </div>
    <div class="col col-3">
      <div class="nav">
        <p><a href="http://www.plos.org">plos.org</a></p>
        <p><a href="http://blogs.plos.org">Blogs</a></p>
        <p><a href="http://www.ploscollections.org">Collections</a></p>
        <p><a href="/feedback/new">Send us feedback</a></p>

        <p>California (US) corporation #C2354500, based in San Francisco</p>
      </div>
    </div>
  </div>
</div><!-- pageftr -->

</div><!-- end page-wrap, this div is in header.ftl -->
<script type="text/javascript" src="/javascript/jquery-1.8.1-min.js?v=Tm7VCOzZz3lE03ghpkS6SWkHbyI"></script>
<script type="text/javascript" src="/javascript/ga-min.js?v=lNQ4gt8QcPDatjsdOFl_FGpPhLY"></script>
<script type="text/javascript" src="/javascript/jquery.hoverIntent-min.js?v=mRiGNYY9cIXxVb8u0K_MdW7hHnc"></script>
<script type="text/javascript" src="/javascript/jquery.placeholder-min.js?v=21Pn56Ur9h1N4K4VZDa0nqI3Pxo"></script>
<script type="text/javascript" src="/javascript/jquery.jsonp-2.4.0-min.js?v=lqTpzoHfSq3I5Ygo01qq5WankEo"></script>
<script type="text/javascript" src="/javascript/jquery-ui-1.9.2.custom-min.js?v=raSSlfNO0YsV5uUpAKmTB9n5VTc"></script>
<script type="text/javascript" src="/javascript/jquery.tooltip-min.js?v=cw+6Smh+mdryIA25xvqIvHMrnZM"></script>
<script type="text/javascript" src="/javascript/jquery.uniform-min.js?v=kYUAnX6W2W_2fK3RIuQ2m_YFG9U"></script>
<script type="text/javascript" src="/javascript/jquery.pjax-min.js?v=939kLBjL5_YKbx71T1RHjYaD4l8"></script>
<script type="text/javascript" src="/javascript/imagesloaded-min.js?v=XeuAp8Gc3mvQUo+wZCSF8ttPwvw"></script>
<script type="text/javascript" src="/javascript/figviewer-min.js?v=yPUa0sUQ_iHkI+IRv2i9bjyZJFo"></script>
<script type="text/javascript" src="/javascript/global-min.js?v=0Q3PwjeaWtXYDnqIsQvnL_ou0qs"></script>
<script type="text/javascript" src="/javascript/jquery.touchswipe-min.js?v=huaek_e6HqTduvCNAN91dJolTyw"></script>
<script type="text/javascript" src="/javascript/jquery.base64-min.js?v=VwV1zeVqKZj5FCAdlK0q5NRxbBg"></script>
<script type="text/javascript" src="/javascript/alm-min.js?v=Y5gm6B0b4Kx2YHNObNrgEeBgXlY"></script>
<script type="text/javascript" src="/javascript/taxonomy-browser-min.js?v=vBVMuDMYkGJCXIUxLe35GoyiJNw"></script>
<script type="text/javascript" src="/javascript/jquery.filterize-min.js?v=j0ZKVnHyk2nhFy8eIuNJkp7xaM0"></script>
<script type="text/javascript" src="/javascript/plosone-min.js?v=TK4H4arL_XBSwwJq+K1N3kqYfAI"></script>
<script type="text/javascript" src="/javascript/twitter-min.js?v=xKgcxLsQFXy+at1ao1NVke8nFlM"></script>
<script type="text/javascript" src="/javascript/crossmark.1.4-min.js?v=3FO4k0SjwTaGNnKGNSqthar1080"></script>
<script type="text/javascript">
  var _sf_async_config={uid:16579,domain:"plosone.org"};
  (function(){
    function loadChartbeat() {
      window._sf_endpt=(new Date()).getTime();
      var e = document.createElement('script');
      e.setAttribute('language', 'javascript');
      e.setAttribute('type', 'text/javascript');
      e.setAttribute('src',
          (("https:" == document.location.protocol) ? "https://a248.e.akamai.net/chartbeat.download.akamai.com/102508/" : "http://static.chartbeat.com/") +
              "js/chartbeat.js");
      document.body.appendChild(e);
    }
    var oldonload = window.onload;
    window.onload = (typeof window.onload != 'function') ?
        loadChartbeat : function() { oldonload(); loadChartbeat(); };
  })();
</script>
<!-- <script type="application/javascript" src="http://crossmark.crossref.org/javascripts/v1.3/crossmark.min.js"></script> -->

</body>
</html>
