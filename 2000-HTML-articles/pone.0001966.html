

 



<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"
      xmlns:foaf="http://xmlns.com/foaf/0.1/"
      xmlns:dc="http://purl.org/dc/terms/"
      xmlns:doi="http://dx.doi.org/"
      xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
      xmlns:xsd="http://www.w3.org/2001/XMLSchema-datatypes#"
      lang="en" xml:lang="en"
      itemscope itemtype="http://schema.org/Article"
      class="no-js">
<head prefix="og: http://ogp.me/ns#">
  <title>PLOS ONE: Phonological Representations Are Unconsciously Used when Processing Complex, Non-Speech Signals</title>


<link rel="stylesheet" type="text/css"  href="/css/global-min.css?v=izteQ6tu7kgsJZW_xmrYizvKiHM" />


    <!--[if lte IE 7]>
<link rel="stylesheet" type="text/css"  href="/css/lte_ie7-min.css?v=3bykQUyQmReeuobVyPozcJ9LxRc" />
    <![endif]-->


<link rel="stylesheet" type="text/css"  href="/css/jquery-ui-min.css?v=eXDHTEJM0lIAmDe5k0I0Ad4nxNo" />


<link rel="stylesheet" type="text/css"  href="/css/journal.css?v=T7ZVxJfgk9jNxLAJ2qHz1vZpgYU" />


<link rel="stylesheet" type="text/css" media="print" href="/css/print-min.css?v=T5lb0B3q6EXBsuDluc5V5w+AkRc" />


  <link rel="stylesheet" href="http://f.fontdeck.com/s/css/js/www.plosone.org/24557.css" type="text/css"/>

  <!--chartbeat -->
  <script type="text/javascript">var _sf_startpt = (new Date()).getTime()</script>
  <script>document.documentElement.className += ' js';</script>

  <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <meta http-equiv="X-UA-Compatible" content="IE=EmulateIE7; IE=EmulateIE9"/>
  <meta name="description" content="PLOS ONE: an inclusive, peer-reviewed, open-access resource from the PUBLIC LIBRARY OF SCIENCE. Reports of well-performed scientific studies from all disciplines freely available to the whole world."/>
  <meta name="keywords" content="PLOS, Public Library of Science, Open Access, Open-Access, Science, Medicine, Biology, Research, Peer-review, Inclusive, Interdisciplinary, Ante-disciplinary, Physics, Chemistry, Engineering"/>
  <meta name="almHost" content="http://alm.plos.org/api/v3/articles"/>
  <meta name="searchHost" content="http://api.plos.org/search" />
  <meta name="termsHost" content="http://api.plos.org/terms" />
  <meta name="solrApiKey" content="plos"/>
  <meta name="almAPIKey" content="3pezRBRXdyzYW6ztfwft" />
  <meta name="currentJournal" content="PLoSONE" />
  <meta name="almRequestBatchSize" content="" />

  <meta name="citation_publisher" content="Public Library of Science"/>
  <meta name="citation_doi" content="10.1371/journal.pone.0001966"/>
  <meta name="dc.identifier" content="10.1371/journal.pone.0001966" />

    <meta name="citation_title" content="Phonological Representations Are Unconsciously Used when Processing Complex, Non-Speech Signals"/>
    <meta itemprop="name" content="Phonological Representations Are Unconsciously Used when Processing Complex, Non-Speech Signals"/>

      <meta name="citation_author" content="Mahan Azadpour"/>
            <meta name="citation_author_institution" content="Cognitive Neuroscience Sector, SISSA (International School for Advanced Studies), Trieste, Italy"/>
      <meta name="citation_author" content="Evan Balaban"/>
            <meta name="citation_author_institution" content="Cognitive Neuroscience Sector, SISSA (International School for Advanced Studies), Trieste, Italy"/>
            <meta name="citation_author_institution" content="Behavioral Neurosciences Program, McGill University, Montreal, Canada"/>

    <meta name="citation_date" content="2008/4/16"/>

  <meta name="citation_pdf_url" content="http://dx.plos.org/10.1371/journal.pone.0001966.pdf" />

      <meta name="citation_journal_title" content="PLOS ONE" />
    <meta name="citation_firstpage" content="e1966"/>
    <meta name="citation_issue" content="4"/>
    <meta name="citation_volume" content="3"/>
    <meta name="citation_issn" content="1932-6203"/>

    <meta name="citation_journal_abbrev" content="PLoS ONE" />

      <meta name="citation_reference" content="citation_title=Identification of a pathway for intelligible speech in the left temporal lobe.; citation_author=SK Scott; citation_author=CC Blank; citation_author=S Rosen; citation_author=RJ Wise; citation_journal_title=Brain; citation_volume=123; citation_number=1; citation_pages=2400-2406; citation_date=2000; " />
      <meta name="citation_reference" content="citation_title=Defining a left-lateralized response specific to intelligible speech using fMRI.; citation_author=C Narain; citation_author=SK Scott; citation_author=RJ Wise; citation_author=S Rosen; citation_author=A Leff; citation_journal_title=Cerebral Cortex; citation_volume=13; citation_number=2; citation_pages=1362-1368; citation_date=2003; " />
      <meta name="citation_reference" content="citation_title=The functional neuroanatomy of prelexical processing in speech perception.; citation_author=SK Scott; citation_author=RJ Wise; citation_journal_title=Cognition; citation_volume=92; citation_number=3; citation_pages=13-45; citation_date=2004; " />
      <meta name="citation_reference" content="citation_title=Neural correlates of switching from auditory to speech perception.; citation_author=G Dehaene-Lambertz; citation_author=C Pallier; citation_author=W Semiclaes; citation_author=L Sprenger-Charolles; citation_author=A Jobert; citation_author=S Dehaene; citation_journal_title=NeuroImage; citation_volume=24; citation_number=4; citation_pages=21-33; citation_date=2005; " />
      <meta name="citation_reference" content="citation_title=Motor cortex maps articulatory features of speech sounds.; citation_author=F Pulvermüller; citation_author=M Huss; citation_author=F Kherif; citation_author=F Moscoso del Prado Martin; citation_author=O Hauk; citation_author=Y Shtyrov; citation_journal_title=Proc Natl Acad Sci USA; citation_volume=103; citation_number=5; citation_pages=7865-7870; citation_date=2006; " />
      <meta name="citation_reference" content="citation_title=Now you hear it, now you don't: transient traces of consonants and their nonspeech analogues in the human brain.; citation_author=J Obleser; citation_author=SK Scott; citation_author=C Eulitz; citation_journal_title=Cerebral Cortex; citation_volume=16; citation_number=6; citation_pages=1069-1076; citation_date=2006; " />
      <meta name="citation_reference" content="citation_title=Neural correlates of intelligibility in speech investigated with noise vocoded speech–a positron emission tomography study.; citation_author=SK Scott; citation_author=S Rosen; citation_author=H Lang; citation_author=RJ Wise; citation_journal_title=J Acoust Soc Amer; citation_volume=120; citation_number=7; citation_pages=1075-1083; citation_date=2006; " />
      <meta name="citation_reference" content="citation_title=Speech perception under conditions of spectral transformation.; citation_author=B Blesser; citation_journal_title=J Speech Hear Res; citation_volume=15; citation_number=8; citation_pages=5-41; citation_date=1972; " />
      <meta name="citation_reference" content="citation_title=Plasticity in speech perception: spectrally-rotated speech, revisited.; citation_author=S Rosen; citation_author=R Finn; citation_author=A Faulkner; citation_journal_title=ARO Abstracts; citation_volume=25; citation_number=9; citation_pages=50-51; citation_date=2002; " />
      <meta name="citation_reference" content="citation_title=A review of perceptual cues and cue robustness.; citation_author=R Wright; citation_title=Phonetically Based Phonology; citation_number=10; citation_pages=34-57; citation_date=2004; citation_publisher=Cambridge University Press; " />
      <meta name="citation_reference" content="citation_title=Nonparametric statistics, 2nd Edition.; citation_author=S Siegel; citation_author=NJ Castellan; citation_number=11; citation_date=1988; citation_publisher=McGraw-Hill; " />
      <meta name="citation_reference" content="citation_title=Randomization, bootstrap and Monte Carlo methods in biology.; citation_author=BFJ Manly; citation_number=12; citation_date=1997; citation_publisher=Chapman & Hall/CRC; " />
      <meta name="citation_reference" content="citation_title=Control meyhods used in a study of the vowels.; citation_author=GE Peterson; citation_author=HL Barney; citation_journal_title=J Acoust Soc Amer; citation_volume=24; citation_number=13; citation_pages=175-184; citation_date=1952; " />
      <meta name="citation_reference" content="citation_title=Studio sperimentale delle caratteristiche elettroacustiche delle vocali toniche ed atone in bisillabi italiani.; citation_author=E Fava; citation_author=EM Caldognetto; citation_title=Studi di fonetica e fonologia; citation_number=14; citation_pages=35-79; citation_date=1976; citation_publisher=Bulzoni; " />
      <meta name="citation_reference" content="citation_title=Formant pattern ambiguity of vowel sounds.; citation_author=D Maurer; citation_author=C D'Heureuse; citation_author=T Landis; citation_journal_title=Int J Neurosci; citation_volume=100; citation_number=15; citation_pages=39-76; citation_date=2000; " />
      <meta name="citation_reference" content="citation_title=A specialization for speech perception.; citation_author=AM Liberman; citation_author=IG Mattingly; citation_journal_title=Science; citation_volume=243; citation_number=16; citation_pages=489-494; citation_date=1989; " />
      <meta name="citation_reference" content="citation_title=J Acoust Soc Amer; citation_author=DH Whalen; citation_author=RR Benson; citation_author=M Richardson; citation_author=B Swainson; citation_author=VP Clark; citation_volume=119; citation_number=17; citation_pages=575-581; citation_date=2006; " />
      <meta name="citation_reference" content="citation_title=Praat: doing phonetics by computer. Version 4.1.10.; citation_author=P Boersma; citation_author=D Weenink; citation_number=18; citation_date=downloaded april 2006; " />

  <link rel="canonical" href="http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0001966" />

    <meta name="twitter:card" content="summary"/>
    <meta name="twitter:site" content="@plosone"/>
    <meta name="twitter:title" content="Phonological Representations Are Unconsciously Used when Processing Complex, Non-Speech Signals"/>
    <meta name="twitter:description" content="Neuroimaging studies of speech processing increasingly rely on artificial speech-like sounds whose perceptual status as speech or non-speech is assigned by simple subjective judgments; brain activation patterns are interpreted according to these status assignments. The na&iuml;ve perceptual status of one such stimulus, spectrally-rotated speech (not consciously perceived as speech by na&iuml;ve subjects), was evaluated in discrimination and forced identification experiments. Discrimination of variation in spectrally-rotated syllables in one group of na&iuml;ve subjects was strongly related to the pattern of similarities in phonological identification of the same stimuli provided by a second, independent group of na&iuml;ve subjects, suggesting either that (1) na&iuml;ve rotated syllable perception involves phonetic-like processing, or (2) that perception is solely based on physical acoustic similarity, and similar sounds are provided with similar phonetic identities. Analysis of acoustic (Euclidean distances of center frequency values of formants) and phonetic similarities in the perception of the vowel portions of the rotated syllables revealed that discrimination was significantly and independently influenced by both acoustic and phonological information. We conclude that simple subjective assessments of artificial speech-like sounds can be misleading, as perception of such sounds may initially and unconsciously utilize speech-like, phonological processing."/>
      <meta name="twitter:image" content="http://dx.plos.org/10.1371/journal.pone.0001966.g004"/>

  <meta property="og:title" content="Phonological Representations Are Unconsciously Used when Processing Complex, Non-Speech Signals" />
  <meta property="og:type" content="article" />
  <meta property="og:url" content="http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0001966" />

 <!--end articleInfoX-->

  <link rel="pingback" href="http://www.plosone.org/pingback" />


  <link rel="shortcut icon" href="/images/favicon.ico" type="image/x-icon"/>
  <link rel="home" title="home" href="/"/>
  <link rel="alternate" type="application/rss+xml"
        title="PLOS ONE: New Articles"
        href="http://www.plosone.org/article/feed"/>
</head>
<body>

  <div id="page-wrap">
    <div id="topbanner" class="cf">

<!-- Div for the ad at the top of journal home page-->
<div class="center">
  <div class="title">Advertisement</div>
  <iframe id='a3ac9da4' name='a3ac9da4'
    src='http://ads.plos.org/www/delivery/afr.php?zoneid=345&amp;cb=4565'
    frameborder='0' scrolling='no' width='730' height='90'>
    <a href='http://ads.plos.org/www/delivery/ck.php?n=a3ac9da4&amp;cb=9249'
      target='_top'><img src='http://ads.plos.org/www/delivery/avw.php?zoneid=345&amp;cb=8881&amp;n=a3ac9da4'
      border='0' alt=''/>
    </a>
  </iframe>
</div>    </div>

    <div id="pagehdr-wrap">
      <div id="pagehdr">
        <div id="user" class="nav">
          <ul>
            <li><a href="http://www.plos.org">plos.org</a></li>
            <li><a href="https://register.plos.org/ambra-registration/register.action">create account</a></li>
            <li class="btn-style"><a
              href="/user/secure/secureRedirect.action?goTo=%2Farticle%2FfetchArticle.action%3FarticleURI%3Dinfo%253Adoi%252F10.1371%252Fjournal.pone.0001966">sign in</a>
            </li>
          </ul>
        </div>
        <div class="logo">
          <a href="/"><img src="/images/logo.png" alt="PLOS ONE"></a>
        </div>

<div id="nav-main" class="nav">
  <ul>
        <li id="mn-01"><a href="/taxonomy" class="areas-link">Subject Areas</a></li>
    <li id="mn-02"><a href="javascript:void(0);">For Authors</a>
      <div class="submenu" style="width: 540px; margin-left: -300px;">
        <div class="block">
          <div class="submit-script">
            <h3>Submit your Manuscript</h3>
            <ul>
              <li>Fair, rigorous peer review</li>
              <li>Broad scope and wide reach</li>
            </ul>
            <a href="/static/submissionInstructions" class="btn">get started</a>
          </div>
        </div>
        <div class="menu">
          <ul>
            <li><a href="/static/publish">Why Publish with PLOS ONE</a></li>
            <li><a href="/static/publication">Publication Criteria</a></li>
            <li><a href="/static/editorial">Editorial Policies</a></li>
            <li><a href="/static/guidelines">Preparing A Manuscript</a></li>
            <li><a href="/static/figureGuidelines">Figure and Table Guidelines</a></li>
          <li><a href="/static/supportingInformation">Supporting Information Guidelines</a></li>
            <li><a href="/static/submissionInstructions">Submitting a Manuscript</a></li>
          </ul>
        </div>
      </div>
    </li>

    <li id="mn-03"><a href="javascript:void(0);">About Us</a>
      <div class="submenu" style="width:248px; margin-left:-30px;">
        <div class="menu">
          <ul>
            <li><a href="/static/information">Journal Information</a></li>
            <li><a href="/static/edboard">Editorial Board</a></li>
            <li><a href="/static/reviewerGuidelines">Reviewer Guidelines</a></li>
            <li><a href="/static/almInfo">Article-Level Metrics</a></li>
            <li><a href="/static/license">Open-Access License</a></li>
            <li><a href="/static/downloads">Media Downloads</a></li>
            <li><a href="/static/commentGuidelines">Guidelines for Comments</a></li>
            <li><a href="/static/corrections">Corrections</a></li>
            <li><a href="/static/help">Help Using this Site</a></li>
            <li><a href="/static/contact">Contact Us</a></li>
          </ul>
        </div>
      </div>
    </li>
  </ul>
<div id="db">
  <form name="searchForm" action="/search/simple?noSearchFlag=true&amp;query=&amp;articleURI=info%3Adoi%2F10.1371%2Fjournal.pone.0001966" method="get" >
<input type="hidden" name="from" value="globalSimpleSearch" id="from"/><input type="hidden" name="filterJournals" value="PLoSONE" id="filterJournals"/>    <fieldset>
      <legend>Search</legend>
      <label for="search">Search</label>
      <div class="wrap">
        <input id="search" type="text" name="query" placeholder="Search">
        <input type="image" alt="SEARCH" src="/images/icon.search.gif">
      </div>
    </fieldset>
  </form>
    <a id="advSearch" href="/search/advanced?noSearchFlag=true&amp;query=&amp;articleURI=info%3Adoi%2F10.1371%2Fjournal.pone.0001966&filterJournals=PLoSONE">advanced search</a>
</div></div>

      </div>
      <!-- pagehdr-->
    </div>
    <!-- pagehdr-wrap -->

  <!--body and html tags gets closed in global_footer.ftl-->
<script type="text/javascript" src="/javascript/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<div id="pagebdy-wrap">
  <div id="pagebdy">

    <div id="article-block" class="cf">

<div class="article-meta cf">
  <ul id="almSignPost" style="display: none;"></ul>
  <div class="article-type">
    <span class="type oa">Open Access</span>
      <span class="type pr">Peer-Reviewed</span>
  </div>
</div>

<div class="header" id="hdr-article">

<div class="article-kicker">
      <span id="article-type-heading">
        Research Article
      </span>
</div>  <h1 property="dc:title" datatype="" rel="dc:type" href="http://purl.org/dc/dcmitype/Text">
    Phonological Representations Are Unconsciously Used when Processing Complex, Non-Speech Signals
  </h1>

  <ul class="authors">
      <li>


        <span rel="dc:creator" class="author">
          <span class="person" property="foaf:name" typeof="foaf:Person">
            Mahan Azadpour, 
          </span>
        </span>

          <div class="author_meta">
            <div class="author_inner">


              
              

                <p>Affiliation:
                  Cognitive Neuroscience Sector, SISSA (International School for Advanced Studies), Trieste, Italy
                </p>


              <span class="close">X</span>

            </div>
          </div>
      </li>
      <li>


        <span rel="dc:creator" class="author">
          <span class="person" property="foaf:name" typeof="foaf:Person">
            Evan Balaban
              <span class="corresponding">mail</span>
          </span>
        </span>

          <div class="author_meta">
            <div class="author_inner">


              
              <p><span class="email">*</span>To whom correspondence should be addressed. E-mail: <a href="mailto:evan.balaban@mcgill.ca">evan.balaban@mcgill.ca</a></p>

                <p>Affiliations:
                  Cognitive Neuroscience Sector, SISSA (International School for Advanced Studies), Trieste, Italy, 
                  Behavioral Neurosciences Program, McGill University, Montreal, Canada
                </p>


              <span class="close">X</span>

            </div>
          </div>
      </li>
  </ul>
  <ul class="date-doi-line">
    <li>Published: April 16, 2008</li>
    <li>DOI: 10.1371/journal.pone.0001966</li>
  </ul>


</div><!--end header-->
<div class="main cf" id="pjax-container">
  

<div class="nav items-5" id="nav-article">
  <ul>
  <li>
        <span class="active" name="article">Article</span>
  </li>
  <li>
      <a href="/article/authors/info%3Adoi%2F10.1371%2Fjournal.pone.0001966" name="authors">About the Authors</a>
  </li>
  <li>
      <a href="/article/metrics/info%3Adoi%2F10.1371%2Fjournal.pone.0001966" name="metrics">Metrics</a>
  </li>
  <li>
      <a href="/article/comments/info%3Adoi%2F10.1371%2Fjournal.pone.0001966" name="comments">Comments</a>
  </li>
  <li>
      <a href="/article/related/info%3Adoi%2F10.1371%2Fjournal.pone.0001966" name="related">Related Content</a>
  </li>
  </ul>
</div>

<script type="text/javascript">
  var selected_tab = "article";
</script>
  <div id="figure-thmbs" class="carousel cf">
    <div class="wrapper">
      <div class="slider">
              <div class="item">
                <a href="#pone-0001966-g001" data-doi="info:doi/10.1371/journal.pone.0001966" data-uri="info:doi/10.1371/journal.pone.0001966.g001" title="Figure 1">
                  <span class="thmb-wrap">
                    <img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001966.g001&representation=PNG_I" alt="">
                  </span>
                </a>
              </div>
              <div class="item">
                <a href="#pone-0001966-t001" data-doi="info:doi/10.1371/journal.pone.0001966" data-uri="info:doi/10.1371/journal.pone.0001966.t001" title="Table 1">
                  <span class="thmb-wrap">
                    <img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001966.t001&representation=PNG_I" alt="">
                  </span>
                </a>
              </div>
              <div class="item">
                <a href="#pone-0001966-t002" data-doi="info:doi/10.1371/journal.pone.0001966" data-uri="info:doi/10.1371/journal.pone.0001966.t002" title="Table 2">
                  <span class="thmb-wrap">
                    <img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001966.t002&representation=PNG_I" alt="">
                  </span>
                </a>
              </div>
              <div class="item">
                <a href="#pone-0001966-g002" data-doi="info:doi/10.1371/journal.pone.0001966" data-uri="info:doi/10.1371/journal.pone.0001966.g002" title="Figure 2">
                  <span class="thmb-wrap">
                    <img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001966.g002&representation=PNG_I" alt="">
                  </span>
                </a>
              </div>
              <div class="item">
                <a href="#pone-0001966-g003" data-doi="info:doi/10.1371/journal.pone.0001966" data-uri="info:doi/10.1371/journal.pone.0001966.g003" title="Figure 3">
                  <span class="thmb-wrap">
                    <img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001966.g003&representation=PNG_I" alt="">
                  </span>
                </a>
              </div>
              <div class="item">
                <a href="#pone-0001966-g004" data-doi="info:doi/10.1371/journal.pone.0001966" data-uri="info:doi/10.1371/journal.pone.0001966.g004" title="Figure 4">
                  <span class="thmb-wrap">
                    <img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001966.g004&representation=PNG_I" alt="">
                  </span>
                </a>
              </div>
      </div>
    </div>
  </div>

  <div class="nav-col">
    <div class="nav" id="nav-article-page">
      <ul>
        <li class="nav-col-comments"><a href="/article/comments/info%3Adoi%2F10.1371%2Fjournal.pone.0001966">Reader Comments (3)</a></li>
          <li id="nav-figures"><a data-doi="info:doi/10.1371/journal.pone.0001966" >Figures</a></li>
      </ul>
    </div>
  </div>

  <div class="article">







<div class="abstract"><a id="abstract0" name="abstract0" toc="abstract0" title="Abstract"></a><h2>Abstract</h2><a id="article1.front1.article-meta1.abstract1.p1" name="article1.front1.article-meta1.abstract1.p1"></a><p>Neuroimaging studies of speech processing increasingly rely on artificial speech-like sounds whose perceptual status as speech or non-speech is assigned by simple subjective judgments; brain activation patterns are interpreted according to these status assignments. The naïve perceptual status of one such stimulus, spectrally-rotated speech (not consciously perceived as speech by naïve subjects), was evaluated in discrimination and forced identification experiments. Discrimination of variation in spectrally-rotated syllables in one group of naïve subjects was strongly related to the pattern of similarities in phonological identification of the same stimuli provided by a second, independent group of naïve subjects, suggesting either that (1) naïve rotated syllable perception involves phonetic-like processing, or (2) that perception is solely based on physical acoustic similarity, and similar sounds are provided with similar phonetic identities. Analysis of acoustic (Euclidean distances of center frequency values of formants) and phonetic similarities in the perception of the vowel portions of the rotated syllables revealed that discrimination was significantly and independently influenced by both acoustic and phonological information. We conclude that simple subjective assessments of artificial speech-like sounds can be misleading, as perception of such sounds may initially and unconsciously utilize speech-like, phonological processing.</p>
</div>


<div class="articleinfo"><p><strong>Citation: </strong>Azadpour M, Balaban E (2008) Phonological Representations Are Unconsciously Used when Processing Complex, Non-Speech Signals. PLoS ONE 3(4):
          e1966.
            doi:10.1371/journal.pone.0001966</p><p><strong>Editor: </strong>Olaf Sporns, Indiana University, United States of America</p><p><strong>Received:</strong> October 31, 2007; <strong>Accepted:</strong> March 6, 2008; <strong>Published:</strong> April 16, 2008</p><p><strong>Copyright:</strong> © 2008 Azadpour, Balaban. This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p><p><strong>Funding: </strong>This work was supported by the Scuola Internazionale Superiore di Studi Avanzati SISSA (Italy), NSERC (Canada), and the Canadian Fund for Innovation.</p><p><strong>Competing interests:</strong> The authors have declared that no competing interests exist.</p></div>





<div id="section1" class="section"><a id="s1" name="s1" toc="s1" title="Introduction"></a><h3>Introduction</h3><a id="article1.body1.sec1.p1" name="article1.body1.sec1.p1"></a><p>Recent behavioral and neuroimaging studies comparing speech and non-speech sound processing in the human brain have relied heavily on digitally-manipulated or synthesized sounds with speech-like acoustical properties that are not subjectively reported to be “speech” by listeners <a href="#pone.0001966-Scott1">[1]</a>–<a href="#pone.0001966-Scott3">[7]</a>. Such stimuli do not appear to be initially processed by the phonological system, but can acquire phonological properties as a result of exposure and/or training <a href="#pone.0001966-DehaeneLambertz1">[4]</a>, <a href="#pone.0001966-Blesser1">[8]</a>, <a href="#pone.0001966-Rosen1">[9]</a>. However, detailed studies of the initial perceptual properties of these stimuli are lacking, especially with respect to the important question of the extent to which such sounds may be unconsciously perceived in a speech-like manner. The assumption of distinct speech and non-speech modes in the initial perception of such sounds (as reflected by conscious subjective reports) is evaluated here using putatively non-speech stimuli played to naïve listeners.</p>
<a id="article1.body1.sec1.p2" name="article1.body1.sec1.p2"></a><p>This study uses spectrally-rotated speech <a href="#pone.0001966-Blesser1">[8]</a> (<a href="#pone-0001966-g001">Figure 1</a>; recorded examples provided in Supporting Information <a href="#pone.0001966.s008">Audio S1</a>, <a href="#pone.0001966.s009">Audio S2</a>), obtained by inverting the speech spectrum around a center frequency, a manipulation that has been used as a non-phonological control stimulus in human neuroimaging studies <a href="#pone.0001966-Scott1">[1]</a>–<a href="#pone.0001966-Scott2">[3]</a>, <a href="#pone.0001966-Obleser1">[6]</a>, <a href="#pone.0001966-Scott3">[7]</a>. People can learn to perceive spectrally-rotated syllables, words and sentences as speech, but only after some hours of training <a href="#pone.0001966-Blesser1">[8]</a>, <a href="#pone.0001966-Rosen1">[9]</a>. As part of a larger project examining how people learn to fluently perceive spectrally-rotated speech as a result of training, we first wanted to document the perceptual starting point before learning has taken place; that is, how rotated speech is naively perceived.</p>
<div class="figure" id="pone-0001966-g001"><div class="img"><a name="pone-0001966-g001" title="Click for larger image " href="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001966.g001&amp;representation=PNG_M" data-doi="info:doi/10.1371/journal.pone.0001966" data-uri="info:doi/10.1371/journal.pone.0001966.g001"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001966.g001&amp;representation=PNG_I" alt="thumbnail" class="thumbnail"></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001966.g001/powerpoint">
                    PPT
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001966.g001/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001966.g001/largerimage">
                    PNG
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001966.g001/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pone.0001966.g001.PNG_L"></span>)
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001966.g001/originalimage">
                    TIFF
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001966.g001/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pone.0001966.g001.TIF"></span>)
                </a></li></ul></div><p><strong>Figure 1.  <span>Sound spectrograms of the normal (left) and spectrally-rotated (right, 1.5 kHz rotation frequency) syllable sequence <em>\ba\-\be\-\bo\-\bu\</em> spoken by a male Italian speaker.</span></strong></p><span>doi:10.1371/journal.pone.0001966.g001</span></div><a id="article1.body1.sec1.p3" name="article1.body1.sec1.p3"></a><p>Spectrally-rotated speech syllables were produced by flipping the spectrum of speech around a center frequency of 1.5 kHz by applying a custom digital implementation of the original algorithm <a href="#pone.0001966-Blesser1">[8]</a> to 75 Italian syllables (15 consonants, and the 5 cardinal vowels /a/,/e/,/i/,/o/,/u/ with the open and close forms of both /e/ and /o/ merged, see below) recorded from a male native speaker. This rotation frequency was chosen based on pilot work indicating that listeners who had experience with rotated speech, and those naïve to it, were less likely to describe sounds rotated about this frequency as “speech-like” than sounds produced with higher rotation frequencies used in previous studies <a href="#pone.0001966-Scott1">[1]</a>–<a href="#pone.0001966-Scott2">[3]</a>, <a href="#pone.0001966-Obleser1">[6]</a>, <a href="#pone.0001966-Scott3">[7]</a>, <a href="#pone.0001966-Blesser1">[8]</a> (see <a href="#s4">Materials and Methods</a>, Supporting Information <a href="#pone.0001966.s001">Text S1</a>, <a href="#pone.0001966.s002">Tables S1</a>, <a href="#pone.0001966.s003">S2</a>, <a href="#pone.0001966.s004">S3</a>, <a href="#pone.0001966.s005">S4</a>). The spectral rotation operation inverts spectral information while preserving the general form of the amplitude envelope and the temporal relationships of spectral trajectories. The perception of rotated sounds by naive listeners was studied by comparing the performance of independent groups of naïve subjects on sound discrimination and forced phonological identification tasks.</p>
<a id="article1.body1.sec1.p4" name="article1.body1.sec1.p4"></a><p>First, perceptual distances of rotated syllables from each other were constructed using data from a same/different discrimination task performed by an initial group of native-speaking Italian listeners with no prior experience with rotated speech. Subjects were presented with three rotated syllables in a row, and were instructed to indicate whether the third sound was similar to any of the other two. The number of false “same” judgments given to stimulus pairs that differed in the identity of a single consonant or vowel was used as a metric of perceptual distance, on the assumption that pairs of stimuli perceived to be more similar to each other should have a higher incidence of erroneous “same” judgments.</p>
<a id="article1.body1.sec1.p5" name="article1.body1.sec1.p5"></a><p>Data on the phonological distances of rotated sounds was then collected by compelling a second, independent set of native-speaking Italian listeners (with no previous exposure to rotated stimuli) to provide phonological descriptions of the same spectrally-rotated syllables used in the discrimination experiment. Participants typed onomatopoetic descriptions of rotated syllables on a computer keyboard (in Italian, unlike English, there is good correspondence between phonological patterns for vowels and consonants for written and spoken forms). Using these primary data, identification matrices were compiled for both rotated consonants and vowels, which represent the proportion of cases in which each rotated vowel or consonant (a row of the matrix) is identified with each unrotated vowel or consonant (the columns of the matrix). Phonological distances were quantified using the numbers in the identification matrix row values of two rotated sounds, on the assumption that sounds that are more phonologically distant should exhibit a more divergent pattern of identification values across vowel and consonant categories.</p>
<a id="article1.body1.sec1.p6" name="article1.body1.sec1.p6"></a><p>By using stimuli that none of the subjects reported hearing as speech, and comparing phonological distance with perceptual distance data collected from two independent, naïve groups of subjects, we could assure that any pattern of resemblances between these two data sets reflects the naïve perceptual status of the stimuli. If the pattern of perceptual differences significantly resembles the pattern of phonological distances, there are two possible explanations: either variation in rotated syllables is naïvely perceived in a phonetic-like manner to some extent, or perception is solely based on physical acoustic similarity, and two sounds that are acoustically similar will be given similar phonological descriptions. These explanations can be tested by quantifying the acoustic distance relations among stimulus sounds and examining whether phonological and acoustic distances independently account for variation in discrimination performance.</p>
<a id="article1.body1.sec1.p7" name="article1.body1.sec1.p7"></a><p>In order to be able to measure the physical acoustic similarities between sounds in a perceptually-valid, uniform and reliable way, we focused on a subset of the information in the stimulus sounds, the vowels. Vowels were selected because consonants vary widely in their meaningful acoustic features–for example, high-frequency bursts of wide-band noise are important for fricatives, while glides and liquids do not contain these but instead rely on changing frequency relations between more discrete tonal energy bands called formants <a href="#pone.0001966-Wright1">[10]</a>. Any attempt to compare acoustic similarity among consonants that do not contain the same structural acoustic features runs up against the unsolved problem of how to weight differences in shared and unshared features in a perceptually meaningful way. Consonants also depend on dynamically-varying acoustic properties which are more difficult to measure robustly. Vowels, on the other hand, are an important class of speech sounds that all have a similar acoustic structure, allowing a subset of perceptually-relevant acoustic features to be measured and compared in a robust and uniform way. Physical acoustic distances for each pair of vowels were defined by the differences in center frequencies of the three lowest energy bands or formants in their Discrete Fourier Transform spectrum, measured in the steady-state portion at the center of each vowel. By looking in detail at the correspondence between discrimination errors, phonological identification results and acoustic physical distance measures for the vowel portions of the stimuli, the relative roles of processing based on phonology and processing based on physical acoustic similarity could be quantitatively evaluated.</p>
<a id="article1.body1.sec1.p8" name="article1.body1.sec1.p8"></a><p>If rotated vowel sounds are not initially processed phonologically, naïve discrimination errors should be inversely related to physical acoustic distance; any relations between phonological distance and discrimination errors should be due to phonological distances mirroring acoustic distances. Under this model, acoustic distances will significantly explain some proportion of the variance in discrimination performance, and phonological distances will not explain any significant variance in addition to that explained by acoustic distance. If, on the other hand, the sounds are initially processed by the native language phonological system, there should be an independent effect of phonological distance on the discrimination of such sounds–phonological distances should explain a significant proportion of the variance in discrimination performance in addition to that explained by acoustic distances.</p>
</div>

<div id="section2" class="section"><a id="s2" name="s2" toc="s2" title="Results"></a><h3>Results</h3>
<h4>Discrimination performance</h4>
<a id="article1.body1.sec2.sec1.p1" name="article1.body1.sec2.sec1.p1"></a><p>The naïve perception of rotated speech syllables was evaluated using a “same or different” discrimination experiment with a set of 20 naïve participants, none of whom reported perceiving any of the rotated syllables as speech. Subjects listened to trials containing three stimuli in a row, and judged whether the third stimulus was the same as the first or second stimulus or different from both. Separate runs tested discrimination of rotated consonant and vowel pairs, each using 10 subjects; the stimuli within each trial of a single run differed only by a consonant or a vowel respectively (see <a href="#s4">Materials and Methods</a>). The number of erroneous “same” judgments over all the repetitions of each pair in different consonantal or vowel contexts was used to construct vowel and consonant discrimination error matrices for each subject, in which each cell represents the number of discrimination errors for the pair of vowels or consonants in the corresponding row and column (10 pairs from 5 vowels, 105 pairs from 15 consonants). These data averaged across subjects provided the matrix of discrimination errors for each vowel or consonant pair. Significant heterogeneity was found in the proportion of discrimination errors among different rotated vowel pairs, and among different rotated consonant pairs (Friedman nonparametric two-way ANOVA <a href="#pone.0001966-Siegel1">[11]</a>, F<sub>r</sub> = 59.9, df = 9, p&lt;0.0001 for vowels; F<sub>r</sub> = 221, df = 104, p&lt;0.0001 for consonants), demonstrating that rotated syllables initially have differential perceptual identities.</p>


<h4>Phonological identification</h4>
<a id="article1.body1.sec2.sec2.p1" name="article1.body1.sec2.sec2.p1"></a><p>To assess whether phonology could play a role in the initial perceptual status of rotated sounds, we needed to assess the naive perceptual identity of rotated syllables when processed phonologically. An independent group of ten native-speaking Italian participants, who were naïve to rotated speech and to the purpose of the experiment, described their phonological perception of the rotated syllables by typing them onomatopoetically on a computer keyboard. The participants were told to describe each sound with a syllable, as would be used to explain what it sounded like to a friend who is not present, and were free to listen to each sound as many times as they liked before recording a response. All 10 participants reported that they did not perceive any of the rotated syllables as speech. Participants requested to hear each stimulus an average of 4.03±0.96 times before providing a description (ranging from 2.60±1.07 times to 7.10±3.69 for particular stimuli), indicating that the stimuli were not easy to consciously identify with speech sounds. The phonological identity of each rotated vowel and consonant was obtained from these descriptions. The phonological identities were averaged over all the repetitions of consonants and vowels in different contexts, and identification matrices were constructed for each subject. The close forms for the vowel ‘e’ and the vowel ‘o’ were combined into the same category as their respective open forms, since these were not separable by written symbols. Identification matrices for consonants and vowels showed high similarities among the individual participants, and were combined to produce summary identification matrices of the onomatopoetic responses of all participants (<a href="#pone-0001966-t001">Tables 1</a> &amp; <a href="#pone-0001966-t002">2</a> for vowels and consonants, respectively; additional information on the relatively rare identifications of rotated consonants as vowels is given in the Supporting Information <a href="#pone.0001966.s001">Text S1</a>, <a href="#pone.0001966.s006">Tables S5</a>, <a href="#pone.0001966.s007">S6</a>). Phonological distances between pairs of rotated vowels and consonants (5 vowels yield 10 paired comparisons, 15 consonants yield 105 paired comparisons) were obtained from these phonological descriptions by calculating the differences between the perceptual identification percentages of each pair of rotated vowels and consonants (see <a href="#s4">Materials and Methods</a>).</p>
<div class="figure" id="pone-0001966-t001"><div class="img"><a name="pone-0001966-t001" title="Click for larger image " href="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001966.t001&amp;representation=PNG_M" data-doi="info:doi/10.1371/journal.pone.0001966" data-uri="info:doi/10.1371/journal.pone.0001966.t001"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001966.t001&amp;representation=PNG_I" alt="thumbnail" class="thumbnail"></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001966.t001/powerpoint">
                    PPT
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001966.t001/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001966.t001/largerimage">
                    PNG
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001966.t001/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pone.0001966.t001.PNG_L"></span>)
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001966.t001/originalimage">
                    TIFF
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001966.t001/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pone.0001966.t001.TIF"></span>)
                </a></li></ul></div><p><strong>Table 1.  <span>Percentages of identification of rotated vowels (rows) as unrotated vowels (columns).</span></strong></p><span>doi:10.1371/journal.pone.0001966.t001</span></div><div class="figure" id="pone-0001966-t002"><div class="img"><a name="pone-0001966-t002" title="Click for larger image " href="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001966.t002&amp;representation=PNG_M" data-doi="info:doi/10.1371/journal.pone.0001966" data-uri="info:doi/10.1371/journal.pone.0001966.t002"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001966.t002&amp;representation=PNG_I" alt="thumbnail" class="thumbnail"></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001966.t002/powerpoint">
                    PPT
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001966.t002/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001966.t002/largerimage">
                    PNG
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001966.t002/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pone.0001966.t002.PNG_L"></span>)
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001966.t002/originalimage">
                    TIFF
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001966.t002/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pone.0001966.t002.TIF"></span>)
                </a></li></ul></div><p><strong>Table 2.  <span>Percentages of identification of rotated consonants (rows) as unrotated speech sounds (columns).</span></strong></p><span>doi:10.1371/journal.pone.0001966.t002</span></div><a id="article1.body1.sec2.sec2.p2" name="article1.body1.sec2.sec2.p2"></a><p>The pattern of estimated phonological distances of rotated sounds were significantly related to the pattern of discrimination errors for both the rotated vowel and consonant pairs (Mantel test <a href="#pone.0001966-Manly1">[12]</a>, Vowels r = −0.81, n = 10, p&lt;0.008; Consonants r = −0.30, n = 105, p&lt;0.002). Fewer discrimination errors were produced as phonological distances increased (<a href="#pone-0001966-g002">Figure 2a</a> for vowels, 2b for consonants), suggesting a significant relationship between naïvely-perceived phonological attributes of rotated stimuli and non-linguistic perceptual judgments about them. However, this does not address the potential role played by physical acoustic distance in these discrimination judgments, since phonologically similar sounds are also acoustically similar.</p>
<div class="figure" id="pone-0001966-g002"><div class="img"><a name="pone-0001966-g002" title="Click for larger image " href="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001966.g002&amp;representation=PNG_M" data-doi="info:doi/10.1371/journal.pone.0001966" data-uri="info:doi/10.1371/journal.pone.0001966.g002"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001966.g002&amp;representation=PNG_I" alt="thumbnail" class="thumbnail"></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001966.g002/powerpoint">
                    PPT
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001966.g002/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001966.g002/largerimage">
                    PNG
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001966.g002/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pone.0001966.g002.PNG_L"></span>)
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001966.g002/originalimage">
                    TIFF
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001966.g002/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pone.0001966.g002.TIF"></span>)
                </a></li></ul></div><p><strong>Figure 2.  <span>Discrimination error (average number of errors) as a function of phonological distance (normalized units) for (a) pairs of rotated vowels and (b) pairs of rotated consonants.</span></strong></p><a id="article1.body1.sec2.sec2.fig1.caption1.p1" name="article1.body1.sec2.sec2.fig1.caption1.p1"></a><p>In the vowel diagram (a), each point represents the average number of errors made over all subjects for the 15 repetitions involving each of the 10 vowel pairs (one repetition for each of the 15 consonantal contexts). In the consonant diagram (b), each point represents the average number of errors made over all subjects for the 5 repetitions involving each of the 105 consonant pairs (one repetition for each of the 5 vowel contexts).</p>
<span>doi:10.1371/journal.pone.0001966.g002</span></div>

<h4>Acoustic analysis</h4>
<a id="article1.body1.sec2.sec3.p1" name="article1.body1.sec2.sec3.p1"></a><p>To analyze the role that physical acoustic similarities play in naïve rotated speech perception, we focused on an important subset of the information in the stimulus sounds, the vowels, for reasons explained in the introduction. The perception of vowel sounds has previously been shown to be related to the frequencies of the first two or three formants (resonant bands of energy concentration) during phoneme release <a href="#pone.0001966-Peterson1">[13]</a>. <a href="#pone-0001966-g003">Figure 3</a> shows rotated vowel stimuli plotted according to their first and second format values, together with ellipses indicating the natural borders of the five cardinal Italian vowel categories used in this experiment <a href="#pone.0001966-Fava1">[14]</a> (open and close forms of ‘e’ and of ‘o’ merged). Each rotated vowel in <a href="#pone-0001966-g003">Figure 3</a> is also labeled according to the highest percentage of the natural vowel category used to phonologically identify it. The spectral rotation center frequency used in this experiment had a special effect on the vowel /i/, leaving it with two widely-spaced formants (at ~500 Hz and 2.8 KHz) which listeners described as being perceptually segregated into a single formant at ~500 Hz and a high frequency tone. Previous work <a href="#pone.0001966-Maurer1">[15]</a> has shown that vowels with a single identifiable formant are perceived according to the vowel whose average of first and second formant values the single formant is closest to; the analysis of the rotated vowel /i/ was carried out accordingly (the same pattern of results reported below is found if all items containing the vowel /i/ are omitted). Acoustic similarities for rotated vowel pairs were defined by the difference in center frequency of the three lowest energy bands (formants) in the steady-state portion at the center of each vowel (sounds with smaller frequency differences between their formants have smaller distances and are more similar), as explained in the <a href="#s4">Materials and Methods</a>. The distance between each rotated vowel and the center of each natural vowel category was similarly calculated, and the distances were averaged over the repetitions of each vowel in different consonantal contexts to construct a matrix representing the distances between each rotated vowel category and each natural vowel category. The identification matrix for rotated vowels had a high degree of correspondence with the matrix of rotated vowel category distances from natural vowel category centers (Spearman rank-order correlation coefficient <a href="#pone.0001966-Siegel1">[11]</a>, rho = −0.80, n = 25, p&lt;0.0001), with rotated vowels being identified with the acoustically-closest natural vowel category, indicating that the naïve perceptual identity of rotated vowels closely follows the formant values used in the native vowel classification scheme <a href="#pone.0001966-Peterson1">[13]</a>, <a href="#pone.0001966-Fava1">[14]</a>.</p>
<div class="figure" id="pone-0001966-g003"><div class="img"><a name="pone-0001966-g003" title="Click for larger image " href="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001966.g003&amp;representation=PNG_M" data-doi="info:doi/10.1371/journal.pone.0001966" data-uri="info:doi/10.1371/journal.pone.0001966.g003"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001966.g003&amp;representation=PNG_I" alt="thumbnail" class="thumbnail"></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001966.g003/powerpoint">
                    PPT
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001966.g003/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001966.g003/largerimage">
                    PNG
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001966.g003/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pone.0001966.g003.PNG_L"></span>)
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001966.g003/originalimage">
                    TIFF
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001966.g003/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pone.0001966.g003.TIF"></span>)
                </a></li></ul></div><p><strong>Figure 3.  <span>Second formant frequency (F2, y-axis) as a function of first formant frequency (F1, x-axis) for spectrally-rotated vowels.</span></strong></p><a id="article1.body1.sec2.sec3.fig1.caption1.p1" name="article1.body1.sec2.sec3.fig1.caption1.p1"></a><p>The letters used as plotting symbols denote the identity of each stimulus vowel before rotation (a = <em>/a/</em>, e = <em>/e/</em>, i = <em>/i/</em>, o =  <em>/o/</em>, u = <em>/u/</em>). The dotted lines outline the elliptical centers of Italian vowel categories <a href="#pone.0001966-Peterson1">[13]</a> and are labeled with their corresponding phonetic identities; the color of each ellipse codes vowel identity and is used to indicate the most common percept of each vowel stimulus (the color of its plotting symbol).</p>
<span>doi:10.1371/journal.pone.0001966.g003</span></div>

<h4>Factors influencing discrimination performance</h4>
<a id="article1.body1.sec2.sec4.p1" name="article1.body1.sec2.sec4.p1"></a><p>The relation between acoustic distances of the vowel pairs and discrimination errors (10 vowel pairs repeated in 15 consonantal contexts, for a total of 150 pairs, <a href="#pone-0001966-g004">Figure 4a</a>), was significant (Mantel test, r = −0.82, n = 150, p&lt;0.0001), with fewer discrimination errors produced as acoustic distances increased. The relationship between discrimination errors and phonological distances for the same 150 pairs (<a href="#pone-0001966-g004">Figure 4b</a>) was also significant (Mantel test, r = −0.63, n = 150, p&lt;0.0001).</p>
<div class="figure" id="pone-0001966-g004"><div class="img"><a name="pone-0001966-g004" title="Click for larger image " href="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001966.g004&amp;representation=PNG_M" data-doi="info:doi/10.1371/journal.pone.0001966" data-uri="info:doi/10.1371/journal.pone.0001966.g004"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0001966.g004&amp;representation=PNG_I" alt="thumbnail" class="thumbnail"></a></div><div class="figure-inline-download">
            Download:
            <ul><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001966.g004/powerpoint">
                    PPT
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001966.g004/powerpoint">
                  PowerPoint slide
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001966.g004/largerimage">
                    PNG
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001966.g004/largerimage">
                  larger image
                  (<span id="info:doi/10.1371/journal.pone.0001966.g004.PNG_L"></span>)
                </a></li><li><div class="icon"><a href="/article/info:doi/10.1371/journal.pone.0001966.g004/originalimage">
                    TIFF
                  </a></div><a href="/article/info:doi/10.1371/journal.pone.0001966.g004/originalimage">
                  original image
                  (<span id="info:doi/10.1371/journal.pone.0001966.g004.TIF"></span>)
                </a></li></ul></div><p><strong>Figure 4.  <span>Discrimination error versus (a) acoustic distance (in Hz) and (b) phonological distance (normalized units) for vowels.</span></strong></p><a id="article1.body1.sec2.sec4.fig1.caption1.p1" name="article1.body1.sec2.sec4.fig1.caption1.p1"></a><p>Each data point in each diagram represents the average number of errors made over all subjects for each vowel pair discrimination in each consonantal context (10 vowel pairs×15 consonantal contexts = 150 data points). The data shown in (b) is represented in a different format in <a href="#pone-0001966-g002">Figure 2b</a>, where all of the 15 consonantal contexts involving the same vowel pair discrimination have been combined (for a total of 10 data points).</p>
<span>doi:10.1371/journal.pone.0001966.g004</span></div><a id="article1.body1.sec2.sec4.p2" name="article1.body1.sec2.sec4.p2"></a><p>The joint relationship of phonological distance and acoustic distance to discrimination performance was examined via stepwise linear multivariate regression. Phonological (PD) and acoustic (AD) distances both contributed significantly toward explaining variation in discrimination errors (F = 112, df = 2, p&lt;0.0001, using both forward and backward models: F-to-remove = 31.3 for PD; F-to-remove = 76.9 for AD). Partial correlation analyses revealed that PD was significantly correlated with discrimination errors when AD was controlled for (r = −0.42, n = 150, p&lt;0.0001), and that discrimination errors and AD were significantly correlated when PD was controlled for (r = −0.59, n = 150, p&lt;0.0001); PD and AD were not themselves significantly correlated when each of their correlations with discrimination errors were controlled for (r = −0.15, n = 150, p = 0.07). These analyses clearly demonstrate that acoustical and phonological properties independently and jointly influence the naive perception of rotated vowels within the context of rotated syllables, and that acoustic distance does not account for the relationship between phonological distance and discrimination errors. Thus, phonological properties play a significant role in the naïve perception of these subjectively “non-speech” sounds.</p>

</div>

<div id="section3" class="section"><a id="s3" name="s3" toc="s3" title="Discussion"></a><h3>Discussion</h3><a id="article1.body1.sec3.p1" name="article1.body1.sec3.p1"></a><p>These results demonstrate that acoustic and phonological properties independently affect naive discrimination of rotated vowel stimuli when presented within the context of rotated syllables. The exponential form of the relationship between phonological and acoustic variation in <a href="#pone-0001966-g004">Figure 4a</a> suggests that when two rotated vowels are acoustically dissimilar, they will be discriminated well regardless of their phonological relationships; as the sounds become more acoustically similar, phonological properties play a greater role in discrimination performance. In the case of the rotated speech sounds studied here, the perceptual system appears to unconsciously utilize phonological information to disambiguate similar sounds, even when these sounds are judged to be non-speech-like.</p>
<a id="article1.body1.sec3.p2" name="article1.body1.sec3.p2"></a><p>Previous investigators have proposed separate but parallel perceptual processes for speech and non-speech sounds that “compete” for perceptual attention, with speech consciously pre-empting or dominating non-speech when phonological percepts are present <a href="#pone.0001966-DehaeneLambertz1">[4]</a>, <a href="#pone.0001966-Liberman1">[16]</a>, <a href="#pone.0001966-Whalen1">[17]</a>. Our results suggest that naively processing rotated syllables in a non-speech mode results in phonetic processing without conscious awareness of the phonetic nature of the stimuli. At a minimum, these results suggest a nuanced relationship between conscious percepts and unconscious modes of speech processing that needs to be re-examined in more detail. Neural imaging experiments using spectrally-rotated speech <a href="#pone.0001966-Scott1">[1]</a>–<a href="#pone.0001966-Scott2">[3]</a>, <a href="#pone.0001966-Obleser1">[6]</a>, <a href="#pone.0001966-Scott3">[7]</a> and other manipulated or synthetic sounds <a href="#pone.0001966-DehaeneLambertz1">[4]</a>, <a href="#pone.0001966-Pulvermller1">[5]</a> as control stimuli for disambiguating brain mechanisms associated with phonological processing need to include rigorous behavioral evaluations of how such stimuli are actually perceived, using more extensive perceptual testing than has been carried out to date. The fact that we went to extra lengths to minimize the ease with which the stimuli used here could be naively perceived in a speech-like manner further underscores this issue. Perhaps the most important implication of this work is that conscious subjective judgments of perceptual “speechiness” should no longer be considered adequate grounds for categorizing sounds for experimental purposes, especially if brain activation patterns are going to be compared among classes of stimuli. Sounds not consciously perceived as speech may still be processed (at least partially) in a speech-like manner.</p>
</div>

<div id="section4" class="section"><a id="s4" name="s4" toc="s4" title="Materials and Methods"></a><h3>Materials and Methods</h3>
<h4>Stimuli</h4>
<a id="article1.body1.sec4.sec1.p1" name="article1.body1.sec4.sec1.p1"></a><p>Stimuli were created from 75 CV syllables (5 vowels, considering the open and closed forms of ‘e’ and of ‘o’ as single vowel categories; 15 consonants [the consonants z, ts, dz, gn, gl, sh were not used in the stimuli]) recorded from a single male Italian speaker, who gave written informed consent for his participation in this experiment. The syllables were approximately 500 ms long with a steady state vowel portion occupying between 120–250 ms of their total length. Spectrally-rotated syllables (rotation frequency = 1.5 kHz) were created by applying a digital version of the algorithm introduced by Blesser <a href="#pone.0001966-Blesser1">[8]</a>, implemented in custom Matlab (The Mathworks, Natick, MA, USA) code, and applied to the recorded syllables. The algorithm consisted of three steps: (1) low pass filtering below 2.8 kHz; (2) modulation of the signal by a cosine wave at 3 kHz; (3) low-pass filtering the modulated sound below 2.8 kHz, resulting in a “mirror image” of the original spectrum around 1.5 kHz. The choice of this rotation frequency involved a compromise between preservation of maximum acoustic information and creation of a signal that would have a low a-priori probability of being initially heard in a speech-like manner. We evaluated the perceptual status of rotated speech with many rotation frequencies; the frequency employed here was the best compromise we could find between “non-speechiness” and the constraints imposed by low-pass filtering (to preserve speech information in the rotated waveform). This center frequency resulted in sounds that were judged by experienced and naïve listeners to be less speech-like in quality than higher rotation frequencies (see Supporting Information <a href="#pone.0001966.s001">Text S1</a>, <a href="#pone.0001966.s002">Tables S1</a>, <a href="#pone.0001966.s003">S2</a>, <a href="#pone.0001966.s004">S3</a>, <a href="#pone.0001966.s005">S4</a>). Thus, we deliberately attempted to make these sounds as dissimilar to speech as possible given the technical constraints of the spectral rotation technique.</p>
<a id="article1.body1.sec4.sec1.p2" name="article1.body1.sec4.sec1.p2"></a><p>For quantifying the acoustical properties of the rotated vowels, the first three formants of the rotated vowel stimuli were estimated. The formants of rotated vowels were obtained by transforming each formant of the original (unrotated) signal F<sub>i</sub> to the value 3000-F<sub>i</sub>. For each unrotated vowel the formant values were measured in a 100 ms window in the steady state portion of the middle of the vowel using the LPC algorithm provided in the Praat software package <a href="#pone.0001966-Boersma1">[18]</a>; this method was validated by checking the measured values against formant peaks observed directly from discrete Fourier transform spectra, and was found to be accurate.</p>


<h4>Experimental design</h4>
<a id="article1.body1.sec4.sec2.p1" name="article1.body1.sec4.sec2.p1"></a><p>Ethics approval for these experiments was obtained from the SISSA Institutional Review Board; written informed consent was obtained from all participants.</p>
<a id="article1.body1.sec4.sec2.p2" name="article1.body1.sec4.sec2.p2"></a><p>A group of naïve, native Italian speakers participated in the discrimination experiment using spectrally-rotated CV syllable stimuli. The experiment was divided into two parts, testing vowel and consonant discrimination separately, each using ten subjects. A two-alternative forced choice ABX procedure (X = A or X = B) was used to examine consonants, and a three-alternative forced choice ABX procedure (X = A or X = B or X is neither A nor B) was used to examine vowels. The interstimulus intervals were 500 ms between A and B, and 800 ms between B and X. The three-alternative forced choice procedure was adopted for vowels based on the results of pilot experiments that found that the two-alternative procedure produced a small number of discrimination errors with vowels; the three-choice procedure was adopted to increase the number of errors and to avoid a ceiling effect. For the trials in which X was neither A nor B, errors were counted for the pair consisting of X and the A or B stimulus that X was incorrectly perceived as. Stimuli on each trial varied either only by a rotated vowel (in the vowel test) or only by a rotated consonant (in the consonant test); trials were repeated for all possible rotated vowel and consonant combinations. The subjects were instructed to listen to each sound sequence and to decide whether the final sound was identical to the first or second sound (consonant test), or identical to the first or second sound or neither (vowel test). Responses were indicated by pressing predefined computer keyboard keys. Subjects were kept blind to the linguistic origin of the stimuli, and to the purpose of the study before the experiment, and were instructed to respond as quickly as possible. The number of incorrectly discriminated trials for each vowel or consonant pair was used to calculate separate error matrices for vowels and consonants.</p>


<h4>Estimation of phonological and acoustic similarity</h4>
<a id="article1.body1.sec4.sec3.p1" name="article1.body1.sec4.sec3.p1"></a><p>Ten native Italian speakers with no previous experience with rotated speech (none of whom had participated in the discrimination experiments) listened to a randomized list of rotated syllables and typed onomatopoetic versions of their perceptions on a computer keyboard. In Italian, there is a good correspondence between spoken and written forms. Participants were instructed to listen to each sound and type an Italian syllable that they would best use to describe that sound. They were free to listen to each syllable as many times as they liked before providing a response. Each subject provided one final judgment on each of the 75 stimuli (15 consonants for each of the five cardinal vowels, open and close forms of ‘e’ and of ‘o’ assigned to single categories). Their final judgments, and the number of times each participant listened to each stimulus, were recorded by a custom-written experimental control program. The phonetic percepts for each consonant and vowel were obtained from the typed responses to each syllable. Identification matrices for consonants and vowels were constructed from the results of each participant. Each row represented one rotated vowel (or consonant), and each column represented a natural vowel (or consonant) category, so that each cell represented the percentage of examples of a particular rotated vowel (or consonant) identified within a natural phonological category. Initial examination of the responses revealed a similar pattern across participants, so their responses were combined into a single identification matrix used for further analysis.</p>
<a id="article1.body1.sec4.sec3.p2" name="article1.body1.sec4.sec3.p2"></a><p>The phonological distances between each pair of rotated vowels or consonants were estimated from the identification matrix by measuring the RMS distance (RMS = root-mean-square) between the two rows of the matrix containing the identification data for the two sounds of the pair. Root-mean-square indicates the mathematical operations of squaring the differences between corresponding numbers in the two sets, taking the mean of these differences, and then taking the square root of this mean. If two rotated sounds have a profile of identification with natural sounds containing similar percentages, their RMS distances will approach zero; pairs of rotated sounds with different percentage identification profiles will have larger RMS distances which will depend on the degree of difference between their percentage identification profiles.</p>
<a id="article1.body1.sec4.sec3.p3" name="article1.body1.sec4.sec3.p3"></a><p>The acoustic distances between two rotated vowels with three formants (all vowels except ‘i’) were estimated from their Euclidean distances in a space defined by the center frequencies of their three lowest formants. When rotated around 1.5 kHz, the vowel ‘i’ had two widely-spaced formants that are perceptually segregated as a low frequency formant and a high frequency tone. Maurer et al <a href="#pone.0001966-Maurer1">[15]</a> have demonstrated that a single formant vowel is perceived according to the vowel whose average of first and second formant values the single formant is closest to. The same procedure was adopted here to estimate the acoustic similarity between rotated ‘i’ vowels (with two frequency bands) and all other vowels (with three frequency bands). Non-i vowels were transformed into a two-frequency-band version for comparison with ‘i’-vowels by averaging their first two formants, and the Euclidean distance between the two-frequency-band versions was calculated. The pattern of the results in all analyses reported here is no different if all stimuli containing the vowel /i/ are omitted from the analyses.</p>


<h4>Statistical analyses</h4>
<a id="article1.body1.sec4.sec4.p1" name="article1.body1.sec4.sec4.p1"></a><p>Linear multivariate regression, correlation and partial correlation analyses were run using Statview (SAS Institute, Cary NC) and MATLAB; Mantel tests were run using a MATLAB script based on the implementation described by Manly <a href="#pone.0001966-Manly1">[12]</a>.</p>

</div>

<div id="section5" class="section"><a id="s5" name="s5" toc="s5" title="Supporting Information"></a><h3>Supporting Information</h3><div class="figshare_widget" doi="10.1371/journal.pone.0001966"></div><a name="pone.0001966.s001" id="pone.0001966.s001"></a><p class="siTitle"><strong><a href="/article/fetchSingleRepresentation.action?uri=info:doi/10.1371/journal.pone.0001966.s001">Text S1. </a></strong></p><p class="siDoi">doi:10.1371/journal.pone.0001966.s001</p><a id="article1.body1.sec5.supplementary-material1.caption1.p1" name="article1.body1.sec5.supplementary-material1.caption1.p1"></a><p class="postSiDOI">(0.02 MB DOC)</p>
<a name="pone.0001966.s002" id="pone.0001966.s002"></a><p class="siTitle"><strong><a href="/article/fetchSingleRepresentation.action?uri=info:doi/10.1371/journal.pone.0001966.s002">Table S1. </a></strong></p><a id="article1.body1.sec5.supplementary-material2.caption1.p1" name="article1.body1.sec5.supplementary-material2.caption1.p1"></a><p class="preSiDOI">Percentages of identification of MANNER feature properties after rotation.</p>
<p class="siDoi">doi:10.1371/journal.pone.0001966.s002</p><a id="article1.body1.sec5.supplementary-material2.caption1.p2" name="article1.body1.sec5.supplementary-material2.caption1.p2"></a><p class="postSiDOI">(0.03 MB DOC)</p>
<a name="pone.0001966.s003" id="pone.0001966.s003"></a><p class="siTitle"><strong><a href="/article/fetchSingleRepresentation.action?uri=info:doi/10.1371/journal.pone.0001966.s003">Table S2. </a></strong></p><a id="article1.body1.sec5.supplementary-material3.caption1.p1" name="article1.body1.sec5.supplementary-material3.caption1.p1"></a><p class="preSiDOI">Percentages of identification of PLACE feature properties after rotation.</p>
<p class="siDoi">doi:10.1371/journal.pone.0001966.s003</p><a id="article1.body1.sec5.supplementary-material3.caption1.p2" name="article1.body1.sec5.supplementary-material3.caption1.p2"></a><p class="postSiDOI">(0.03 MB DOC)</p>
<a name="pone.0001966.s004" id="pone.0001966.s004"></a><p class="siTitle"><strong><a href="/article/fetchSingleRepresentation.action?uri=info:doi/10.1371/journal.pone.0001966.s004">Table S3. </a></strong></p><a id="article1.body1.sec5.supplementary-material4.caption1.p1" name="article1.body1.sec5.supplementary-material4.caption1.p1"></a><p class="preSiDOI">Percentages of identification of VOICING feature properties after rotation.</p>
<p class="siDoi">doi:10.1371/journal.pone.0001966.s004</p><a id="article1.body1.sec5.supplementary-material4.caption1.p2" name="article1.body1.sec5.supplementary-material4.caption1.p2"></a><p class="postSiDOI">(0.03 MB DOC)</p>
<a name="pone.0001966.s005" id="pone.0001966.s005"></a><p class="siTitle"><strong><a href="/article/fetchSingleRepresentation.action?uri=info:doi/10.1371/journal.pone.0001966.s005">Table S4. </a></strong></p><a id="article1.body1.sec5.supplementary-material5.caption1.p1" name="article1.body1.sec5.supplementary-material5.caption1.p1"></a><p class="preSiDOI">Comparison of consonant identification performance with the Blesser study [s1].</p>
<p class="siDoi">doi:10.1371/journal.pone.0001966.s005</p><a id="article1.body1.sec5.supplementary-material5.caption1.p2" name="article1.body1.sec5.supplementary-material5.caption1.p2"></a><p class="postSiDOI">(0.03 MB DOC)</p>
<a name="pone.0001966.s006" id="pone.0001966.s006"></a><p class="siTitle"><strong><a href="/article/fetchSingleRepresentation.action?uri=info:doi/10.1371/journal.pone.0001966.s006">Table S5. </a></strong></p><a id="article1.body1.sec5.supplementary-material6.caption1.p1" name="article1.body1.sec5.supplementary-material6.caption1.p1"></a><p class="preSiDOI">Percentages of identification of rotated consonants (rows) as unrotated vowels (columns) different from the following vowel in the syllable</p>
<p class="siDoi">doi:10.1371/journal.pone.0001966.s006</p><a id="article1.body1.sec5.supplementary-material6.caption1.p2" name="article1.body1.sec5.supplementary-material6.caption1.p2"></a><p class="postSiDOI">(0.04 MB DOC)</p>
<a name="pone.0001966.s007" id="pone.0001966.s007"></a><p class="siTitle"><strong><a href="/article/fetchSingleRepresentation.action?uri=info:doi/10.1371/journal.pone.0001966.s007">Table S6. </a></strong></p><a id="article1.body1.sec5.supplementary-material7.caption1.p1" name="article1.body1.sec5.supplementary-material7.caption1.p1"></a><p class="preSiDOI">Relative percentage of rotated consonants with the indicated feature (rows) identified as a vowel (columns), and the relative mutual information between rotated consonant features and vowel identification (rMI).</p>
<p class="siDoi">doi:10.1371/journal.pone.0001966.s007</p><a id="article1.body1.sec5.supplementary-material7.caption1.p2" name="article1.body1.sec5.supplementary-material7.caption1.p2"></a><p class="postSiDOI">(0.04 MB DOC)</p>
<a name="pone.0001966.s008" id="pone.0001966.s008"></a><p class="siTitle"><strong><a href="/article/fetchSingleRepresentation.action?uri=info:doi/10.1371/journal.pone.0001966.s008">Audio S1. </a></strong></p><a id="article1.body1.sec5.supplementary-material8.caption1.p1" name="article1.body1.sec5.supplementary-material8.caption1.p1"></a><p class="preSiDOI">Normal Syllables. The syllabic sequence \ba\-\be\-\bo\-\bu\ pronounced by a male speaker.</p>
<p class="siDoi">doi:10.1371/journal.pone.0001966.s008</p><a id="article1.body1.sec5.supplementary-material8.caption1.p2" name="article1.body1.sec5.supplementary-material8.caption1.p2"></a><p class="postSiDOI">(0.22 MB WAV)</p>
<a name="pone.0001966.s009" id="pone.0001966.s009"></a><p class="siTitle"><strong><a href="/article/fetchSingleRepresentation.action?uri=info:doi/10.1371/journal.pone.0001966.s009">Audio S2. </a></strong></p><a id="article1.body1.sec5.supplementary-material9.caption1.p1" name="article1.body1.sec5.supplementary-material9.caption1.p1"></a><p class="preSiDOI">Spectrally-rotated Syllables. The spectrally-rotated syllabic sequence \ba\-\be\-bo\-\bu\ pronounced by a male speaker (rotation frequency = 1.5 kHz).</p>
<p class="siDoi">doi:10.1371/journal.pone.0001966.s009</p><a id="article1.body1.sec5.supplementary-material9.caption1.p2" name="article1.body1.sec5.supplementary-material9.caption1.p2"></a><p class="postSiDOI">(0.22 MB WAV)</p>
</div>





<div><a id="ack" name="ack" toc="ack" title="Acknowledgments"></a><h3>Acknowledgments</h3>
<a id="article1.back1.ack1.p1" name="article1.back1.ack1.p1"></a><p>We thank Aniruddh Patel, David Poeppel, and Debra Titone for helpful comments on the manuscript.</p>
</div><div class="contributions"><a id="authcontrib" name="authcontrib" toc="authcontrib" title="Author Contributions"></a><h3>Author Contributions</h3><p>Conceived and designed the experiments: EB MA. Performed the experiments: MA. Analyzed the data: MA. Wrote the paper: EB MA.</p></div><div><a id="references" name="references" toc="references" title="References"></a><h3>References</h3><ol class="references"><li><span class="label">1.
              </span><a name="pone.0001966-Scott1" id="pone.0001966-Scott1"></a>Scott SK, Blank CC, Rosen S, Wise RJ (2000) Identification of a pathway for intelligible speech in the left temporal lobe. Brain  123: 2400–2406.  <ul class="find" data-citedArticleID="1089184" data-doi="10.1093/brain/123.12.2400"><li><a href="http://dx.doi.org/10.1093/brain/123.12.2400" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Identification+of+a+pathway+for+intelligible+speech+in+the+left+temporal+lobe." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Identification+of+a+pathway+for+intelligible+speech+in+the+left+temporal+lobe.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">2.
              </span><a name="pone.0001966-Narain1" id="pone.0001966-Narain1"></a>Narain C, Scott SK, Wise RJ, Rosen S, Leff A, et al.  (2003) Defining a left-lateralized response specific to intelligible speech using fMRI. Cerebral Cortex  13: 1362–1368.  <ul class="find" data-citedArticleID="1089174" data-doi="10.1093/cercor/bhg083"><li><a href="http://dx.doi.org/10.1093/cercor/bhg083" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Defining+a+left-lateralized+response+specific+to+intelligible+speech+using+fMRI." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Defining+a+left-lateralized+response+specific+to+intelligible+speech+using+fMRI.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">3.
              </span><a name="pone.0001966-Scott2" id="pone.0001966-Scott2"></a>Scott SK, Wise RJ (2004) The functional neuroanatomy of prelexical processing in speech perception. Cognition  92: 13–45.  <ul class="find" data-citedArticleID="1089186" data-doi="10.1016/j.cognition.2002.12.002"><li><a href="http://dx.doi.org/10.1016/j.cognition.2002.12.002" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=The+functional+neuroanatomy+of+prelexical+processing+in+speech+perception." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22The+functional+neuroanatomy+of+prelexical+processing+in+speech+perception.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">4.
              </span><a name="pone.0001966-DehaeneLambertz1" id="pone.0001966-DehaeneLambertz1"></a>Dehaene-Lambertz G, Pallier C, Semiclaes W, Sprenger-Charolles L, Jobert A, Dehaene S (2005) Neural correlates of switching from auditory to speech perception. NeuroImage  24: 21–33.  <ul class="find" data-citedArticleID="1089164" data-doi="10.1016/j.neuroimage.2004.09.039"><li><a href="http://dx.doi.org/10.1016/j.neuroimage.2004.09.039" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Neural+correlates+of+switching+from+auditory+to+speech+perception." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Neural+correlates+of+switching+from+auditory+to+speech+perception.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">5.
              </span><a name="pone.0001966-Pulvermller1" id="pone.0001966-Pulvermller1"></a>Pulvermüller F, Huss M, Kherif F, Moscoso del Prado Martin F, Hauk O, Shtyrov Y (2006) Motor cortex maps articulatory features of speech sounds. Proc Natl Acad Sci USA  103: 7865–7870.  <ul class="find" data-citedArticleID="1089180" data-doi="10.1073/pnas.0509989103"><li><a href="http://dx.doi.org/10.1073/pnas.0509989103" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Motor+cortex+maps+articulatory+features+of+speech+sounds." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Motor+cortex+maps+articulatory+features+of+speech+sounds.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">6.
              </span><a name="pone.0001966-Obleser1" id="pone.0001966-Obleser1"></a>Obleser J, Scott SK, Eulitz C (2006) Now you hear it, now you don't: transient traces of consonants and their nonspeech analogues in the human brain. Cerebral Cortex  16: 1069–1076.  <ul class="find" data-citedArticleID="1089176" data-doi="10.1093/cercor/bhj047"><li><a href="http://dx.doi.org/10.1093/cercor/bhj047" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Now+you+hear+it%2C+now+you+don%27t%3A+transient+traces+of+consonants+and+their+nonspeech+analogues+in+the+human+brain." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Now+you+hear+it%2C+now+you+don%27t%3A+transient+traces+of+consonants+and+their+nonspeech+analogues+in+the+human+brain.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">7.
              </span><a name="pone.0001966-Scott3" id="pone.0001966-Scott3"></a>Scott SK, Rosen S, Lang H, Wise RJ (2006) Neural correlates of intelligibility in speech investigated with noise vocoded speech–a positron emission tomography study. J Acoust Soc Amer  120: 1075–1083.  <ul class="find" data-citedArticleID="1089188" data-doi="10.1121/1.2216725"><li><a href="http://dx.doi.org/10.1121/1.2216725" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Neural+correlates+of+intelligibility+in+speech+investigated+with+noise+vocoded+speech%E2%80%93a+positron+emission+tomography+study." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Neural+correlates+of+intelligibility+in+speech+investigated+with+noise+vocoded+speech%E2%80%93a+positron+emission+tomography+study.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">8.
              </span><a name="pone.0001966-Blesser1" id="pone.0001966-Blesser1"></a>Blesser B (1972) Speech perception under conditions of spectral transformation. J Speech Hear Res  15: 5–41.  <ul class="find" data-citedArticleID="1089160"><li><a href="http://www.crossref.org/guestquery/?auth2=&amp;atitle2=Speech+perception+under+conditions+of+spectral+transformation.&amp;auth=&amp;atitle=Speech+perception+under+conditions+of+spectral+transformation." target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Speech+perception+under+conditions+of+spectral+transformation." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Speech+perception+under+conditions+of+spectral+transformation.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">9.
              </span><a name="pone.0001966-Rosen1" id="pone.0001966-Rosen1"></a>Rosen S, Finn R, Faulkner A (2002) Plasticity in speech perception: spectrally-rotated speech, revisited. ARO Abstracts  25: 50–51.  <ul class="find" data-citedArticleID="1089182"><li><a href="http://www.crossref.org/guestquery/?auth2=&amp;atitle2=Plasticity+in+speech+perception%3A+spectrally-rotated+speech%2C+revisited.&amp;auth=&amp;atitle=Plasticity+in+speech+perception%3A+spectrally-rotated+speech%2C+revisited." target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Plasticity+in+speech+perception%3A+spectrally-rotated+speech%2C+revisited." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Plasticity+in+speech+perception%3A+spectrally-rotated+speech%2C+revisited.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">10.
              </span><a name="pone.0001966-Wright1" id="pone.0001966-Wright1"></a>Wright R (2004)  A review of perceptual cues and cue robustness. In: Hayes B, Kircher RM, Steriade D, editors. Phonetically Based Phonology. Cambridge: Cambridge University Press. pp. 34–57.  <ul class="find-nolinks"></ul></li><li><span class="label">11.
              </span><a name="pone.0001966-Siegel1" id="pone.0001966-Siegel1"></a>Siegel S, Castellan NJ (1988) Nonparametric statistics, 2nd Edition. New York: McGraw-Hill.   <ul class="find-nolinks"></ul></li><li><span class="label">12.
              </span><a name="pone.0001966-Manly1" id="pone.0001966-Manly1"></a>Manly BFJ (1997) Randomization, bootstrap and Monte Carlo methods in biology. London: Chapman &amp; Hall/CRC.   <ul class="find-nolinks"></ul></li><li><span class="label">13.
              </span><a name="pone.0001966-Peterson1" id="pone.0001966-Peterson1"></a>Peterson GE, Barney HL (1952) Control meyhods used in a study of the vowels. J Acoust Soc Amer  24: 175–184.  <ul class="find" data-citedArticleID="1089178"><li><a href="http://www.crossref.org/guestquery/?auth2=&amp;atitle2=Control+meyhods+used+in+a+study+of+the+vowels.&amp;auth=&amp;atitle=Control+meyhods+used+in+a+study+of+the+vowels." target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Control+meyhods+used+in+a+study+of+the+vowels." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Control+meyhods+used+in+a+study+of+the+vowels.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">14.
              </span><a name="pone.0001966-Fava1" id="pone.0001966-Fava1"></a>Fava E, Caldognetto EM (1976)  Studio sperimentale delle caratteristiche elettroacustiche delle vocali toniche ed atone in bisillabi italiani. In: Simone R, Vignuzzi U, Ruggiero G, editors. Studi di fonetica e fonologia. Rome: Bulzoni. pp. 35–79.  <ul class="find-nolinks"></ul></li><li><span class="label">15.
              </span><a name="pone.0001966-Maurer1" id="pone.0001966-Maurer1"></a>Maurer D, D'Heureuse C, Landis T (2000) Formant pattern ambiguity of vowel sounds. Int J Neurosci  100: 39–76.  <ul class="find" data-citedArticleID="1089172" data-doi="10.3109/00207450008999677"><li><a href="http://dx.doi.org/10.3109/00207450008999677" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=Formant+pattern+ambiguity+of+vowel+sounds." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22Formant+pattern+ambiguity+of+vowel+sounds.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">16.
              </span><a name="pone.0001966-Liberman1" id="pone.0001966-Liberman1"></a>Liberman AM, Mattingly IG (1989) A specialization for speech perception. Science  243: 489–494.  <ul class="find" data-citedArticleID="1089168" data-doi="10.1126/science.2643163"><li><a href="http://dx.doi.org/10.1126/science.2643163" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=A+specialization+for+speech+perception." target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22A+specialization+for+speech+perception.%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">17.
              </span><a name="pone.0001966-Whalen1" id="pone.0001966-Whalen1"></a>Whalen DH, Benson RR, Richardson M, Swainson B, Clark VP, et al.  (2006) J Acoust Soc Amer 119: 575–581.  <ul class="find" data-citedArticleID="1089192" data-doi="10.1121/1.2139627"><li><a href="http://dx.doi.org/10.1121/1.2139627" target="_new" title="Go to article in CrossRef">
                          View Article
                        </a></li><li><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=PubMed&amp;cmd=Search&amp;doptcmdl=Citation&amp;defaultField=Title+Word&amp;term=J+Acoust+Soc+Amer" target="_new" title="Go to article in PubMed">
                          PubMed/NCBI
                        </a></li><li><a href="http://scholar.google.com/scholar?hl=en&amp;safe=off&amp;q=%22J+Acoust+Soc+Amer%22" target="_new" title="Go to article in Google Scholar">
                          Google Scholar
                        </a></li></ul></li><li><span class="label">18.
              </span><a name="pone.0001966-Boersma1" id="pone.0001966-Boersma1"></a>Boersma P, Weenink D (downloaded april 2006) Praat: doing phonetics by computer. Version 4.1.10.   <a href="http://www.praat.org">http://www.praat.org</a>.  <ul class="find-nolinks"></ul></li></ol></div>

  </div>

      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001966.XML" value="81029"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001966.PDF" value="306022"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001966.g001.PNG_L" value="437139"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001966.g001.PNG_M" value="106752"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001966.g001.PNG_S" value="9458"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001966.g001.TIF" value="879248"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001966.g001.PNG_I" value="36215"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001966.t001.PNG_L" value="34006"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001966.t001.PNG_M" value="44032"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001966.t001.PNG_S" value="10587"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001966.t001.TIF" value="195672"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001966.t001.PNG_I" value="14226"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001966.t002.PNG_L" value="135549"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001966.t002.PNG_M" value="88965"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001966.t002.PNG_S" value="13985"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001966.t002.TIF" value="724514"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001966.t002.PNG_I" value="27949"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001966.g002.PNG_L" value="38987"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001966.g002.PNG_M" value="74478"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001966.g002.PNG_S" value="13911"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001966.g002.TIF" value="190672"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001966.g002.PNG_I" value="49204"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001966.g003.PNG_L" value="72120"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001966.g003.PNG_M" value="87630"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001966.g003.PNG_S" value="11408"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001966.g003.TIF" value="156292"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001966.g003.PNG_I" value="30379"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001966.g004.PNG_L" value="51257"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001966.g004.PNG_M" value="96699"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001966.g004.PNG_S" value="16422"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001966.g004.TIF" value="224940"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001966.g004.PNG_I" value="69717"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001966.s001.DOC" value="24576"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001966.s002.DOC" value="28160"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001966.s003.DOC" value="27136"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001966.s004.DOC" value="26624"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001966.s005.DOC" value="26624"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001966.s006.DOC" value="37376"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001966.s007.DOC" value="37376"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001966.s008.WAV" value="221404"/>
      <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pone.0001966.s009.WAV" value="221398"/>

</div>
<div class="sidebar">

  <div class="article-actions cf">
      <div class="download">
        <span class="btn"><a href="/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0001966&amp;representation=PDF" title="Download" target="_blank">Download PDF</a></span>
      </div>
      <div class="btn-reveal dropdown">
        <div class="dropdown-icon">
          <span class="btn">&nbsp;</span>
        </div>

        <div class="content">
          <ul class="bullet">
            <li><a href="/article/citationList.action?articleURI=info%3Adoi%2F10.1371%2Fjournal.pone.0001966" title="Download citations">Citation</a></li>
            <li><a href="/article/fetchObjectAttachment.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0001966&amp;representation=XML" title="Download article XML">XML</a></li>
          </ul>
        </div>
      </div> <!-- end btn-reveal dropdown-->


    <div class="btn-reveal flt-l">
        <span class="btn">Print</span>
        <div class="content">
            <ul class="bullet">
                <li id="print-article"><a href="#" onclick="if(typeof(_gaq) != 'undefined'){ _gaq.push(['_trackEvent','Article', 'Print', 'Click']); } window.print(); return false;" title="Print Article">Print article</a></li>
                <li>
                  <a href="https://www.odysseypress.com/onlinehost/reprint_order.php?type=A&page=0&journal=7&doi=10.1371/journal.pone.0001966&volume=&issue=&title=Phonological Representations Are Unconsciously Used when Processing Complex, Non-Speech Signals&author_name=Mahan%20Azadpour%2C%20Evan%20Balaban&start_page=1&end_page=7" title="Odyssey Press">EzReprint</a>
                </li>
            </ul>
        </div>
    </div>

    <div class="btn-reveal flt-r">
        <span class="btn">Share</span>
        <div class="content">
            <ul class="social">
                <li><a href="http://www.reddit.com/submit?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0001966" target="_blank" title="Submit to Reddit"><img src="/images/icon.reddit.16.png" width="16" height="16" alt="Reddit">Reddit</a></li>

                <li><a href="https://plus.google.com/share?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0001966" target="_blank" title="Share on Google+"><img src="/images/icon.gplus.16.png" width="16" height="16" alt="Google+">Google+</a></li>

                <li><a href="http://www.stumbleupon.com/submit?url=http%3A%2F%2Fwww.plosone.org%2Farticle%2Finfo%253Adoi%252F10.1371%252Fjournal.pone.0001966" target="_blank" title="Add to StumbleUpon"><img src="/images/icon.stumble.16.png" width="16" height="16" alt="StumbleUpon">StumbleUpon</a></li>

                <li><a href="http://www.facebook.com/share.php?u=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0001966&amp;t=Phonological%20Representations%20Are%20Unconsciously%20Used%20when%20Processing%20Complex%2C%20Non-Speech%20Signals" target="_blank" title="Share on Facebook"><img src="/images/icon.fb.16.png" width="16" height="16" alt="Facebook">Facebook</a></li>

                <li><a href="http://www.linkedin.com/shareArticle?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0001966&title=Phonological%20Representations%20Are%20Unconsciously%20Used%20when%20Processing%20Complex%2C%20Non-Speech%20Signals&summary=Checkout%20this%20article%20I%20found%20at%20PLOS" target="_blank" title="Add to LinkedIn"><img src="/images/icon.linkedin.16.png" width="16" height="16" alt="Mendeley">LinkedIn</a></li>

                <li><a href="http://www.citeulike.org/posturl?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0001966&amp;title=Phonological%20Representations%20Are%20Unconsciously%20Used%20when%20Processing%20Complex%2C%20Non-Speech%20Signals" target="_blank" title="Add to CiteULike"><img src="/images/icon.cul.16.png" width="16" height="16" alt="CiteULike">CiteULike</a></li>

                <li><a href="http://www.mendeley.com/import/?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0001966" target="_blank" title="Add to Mendeley"><img src="/images/icon.mendeley.16.png" width="16" height="16" alt="Mendeley">Mendeley</a></li>

                <li><a href="https://www.pubchase.com/library?add_aid=10.1371%2Fjournal.pone.0001966&amp;source=plos" target="_blank" title="Add to PubChase"><img src="/images/icon.pc.16.png" width="16" height="16" alt="PubChase">PubChase</a></li>


                <script type="text/javascript">
                    // replace tweet with one that's pre-shortened to 140 chars
                    function truncateTweetText() {
                        var twtTitle = 'Phonological Representations Are Unconsciously Used when Processing Complex, Non-Speech Signals';
                        var twtUrl = 'http://dx.plos.org/10.1371/journal.pone.0001966';
                        // all URLs posted to twitter get auto-shortened to 20 chars.
                        var maxLength = 140 - (20 + 1);
                        // truncate the title to include space for twtTag and ellipsis (here, 10 = tag length + space + ellipsis)
                        if (twtTitle.length > maxLength) { twtTitle = twtTitle.substr(0, (maxLength - 10)) + '...'; }
                        // set the href to use the shortened tweet
                        $('#twitter-share-link').prop('href', 'http://twitter.com/intent/tweet?text=' + encodeURIComponent('#PLOSONE: ' + twtTitle + ' ' + twtUrl));
                    }
                </script>
                <li><a href="http://twitter.com/intent/tweet?text=#PLOSONE%3A%20Phonological%20Representations%20Are%20Unconsciously%20Used%20when%20Processing%20Complex%2C%20Non-Speech%20Signals http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pone.0001966" onclick="truncateTweetText();" target="_blank" title="Share on Twitter" id="twitter-share-link"><img src="/images/icon.twtr.16.png" width="16" height="16" alt="Twitter">Twitter</a></li>

                <li><a href="/article/email/info%3Adoi%2F10.1371%2Fjournal.pone.0001966" title="Email this article"><img src="/images/icon.email.16.png" width="16" height="16" alt="Email">Email</a></li>
            </ul>
        </div>
    </div><!--end btn-reveal flt-r-->
</div><!-- end article-actions-->

<!-- begin Crossmark -->

<a id="open-crossmark" href="#" style="margin-top: -28px; display:block"><img style="border: 0; display: none;
 padding: 10px 0 18px 0;"  id="crossmark-icon" src="/images/logo-crossmark-bw.png" /></a>
<div id="crossmark-dialog" style="display: none;" title="">
    <!-- the external CrossMark data is loaded inside this iframe -->
    <iframe id="crossmark-dialog-frame" frameborder="0"></iframe>
</div>

<!-- end crossmark -->


<div class="block" id="subject-area-sidebar-block">
    <div class="header">
        <h3>Subject Areas</h3><div title="More information" id="subject-area-sidebar-block-help-icon"><img align="right"
                                                                                                           alt="info" src="/images/button_info.png"/><div id="subject-area-sidebar-block-help"><img align="right"
                                                                                                                                                                                                    src="/images/button_info.png"/><p>
        <b>We want your feedback.</b> Do these subject areas make sense for this article? If not, click the flag
        next to the incorrect subject area and we will review it. Thanks for your help!
    </p></div></div>
    </div>


    <ul id="subject-area-sidebar-list">













          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Acoustics%22" title="Search for articles in the subject area:'Acoustics'"><div class="flagText">Acoustics</div></a>
              <div data-categoryid="21171" data-articleid="26756"
                   data-categoryname="Acoustics"
                   class="flagImage" title="Flag 'Acoustics' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Neuroimaging%22" title="Search for articles in the subject area:'Neuroimaging'"><div class="flagText">Neuroimaging</div></a>
              <div data-categoryid="46061" data-articleid="26756"
                   data-categoryname="Neuroimaging"
                   class="flagImage" title="Flag 'Neuroimaging' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Phonology%22" title="Search for articles in the subject area:'Phonology'"><div class="flagText">Phonology</div></a>
              <div data-categoryid="22701" data-articleid="26756"
                   data-categoryname="Phonology"
                   class="flagImage" title="Flag 'Phonology' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Sensory+perception%22" title="Search for articles in the subject area:'Sensory perception'"><div class="flagText">Sensory perception</div></a>
              <div data-categoryid="46099" data-articleid="26756"
                   data-categoryname="Sensory perception"
                   class="flagImage" title="Flag 'Sensory perception' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Speech%22" title="Search for articles in the subject area:'Speech'"><div class="flagText">Speech</div></a>
              <div data-categoryid="23971" data-articleid="26756"
                   data-categoryname="Speech"
                   class="flagImage" title="Flag 'Speech' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Speech+signal+processing%22" title="Search for articles in the subject area:'Speech signal processing'"><div class="flagText">Speech signal processing</div></a>
              <div data-categoryid="22703" data-articleid="26756"
                   data-categoryname="Speech signal processing"
                   class="flagImage" title="Flag 'Speech signal processing' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Syllables%22" title="Search for articles in the subject area:'Syllables'"><div class="flagText">Syllables</div></a>
              <div data-categoryid="21173" data-articleid="26756"
                   data-categoryname="Syllables"
                   class="flagImage" title="Flag 'Syllables' as inappropriate"></div>
          </li>
          <li>
              <a href="/search/advanced?unformattedQuery=subject%3A%22Vowels%22" title="Search for articles in the subject area:'Vowels'"><div class="flagText">Vowels</div></a>
              <div data-categoryid="27819" data-articleid="26756"
                   data-categoryname="Vowels"
                   class="flagImage" title="Flag 'Vowels' as inappropriate"></div>
          </li>
    </ul>
</div>

<div class="ad">
    <div class="title">Advertisement</div>






  <iframe id='a0852f54' name='a0852f54'
    src='http://ads.plos.org/www/delivery/afr.php?zoneid=381&amp;cb=8240'
    frameborder='0' scrolling='no' width='160' height='600'>
    <a href='http://ads.plos.org/www/delivery/ck.php?n=a0852f54&amp;cb=3093'
      target='_top'><img src='http://ads.plos.org/www/delivery/avw.php?zoneid=381&amp;cb=8616&amp;n=a0852f54'
      border='0' alt=''/>
    </a>
  </iframe>



</div>

<div id="twitter-alm-timeline" class="twitter-alm-timeline"></div>

<div class="block sidebar-comments">
    <div class="header">
        <h3>Comments</h3>
    </div>
      <p><a href="/annotation/listThread.action?root=1921">Referee Comments: Referee 4</a><br>Posted by PLoS_ONE_Group</p>
      <p><a href="/annotation/listThread.action?root=12143">Referee Comments: Referee 2</a><br>Posted by PLoS_ONE_Group</p>
      <p><a href="/annotation/listThread.action?root=20193">Referee Comments: Referee 1</a><br>Posted by PLoS_ONE_Group</p>
</div>

</div><!-- sidebar -->
    </div>
  </div>
</div>
<script src="http://wl.figshare.com/static/p_widget.js" type="text/javascript"></script><div id="pageftr">
  <div class="ftr-cols cf">
    <div class="col col-1">
      <img src="/images/logo-plos-footer.png" alt="PLOS Logo" class="logo" />
      <p><a href="/static/releaseNotes">Ambra 2.9.16</a> Managed Colocation provided <br />by <a href="http://www.isc.org/">Internet Systems Consortium</a>.<p>
      <div class="nav nav-aux">
        <a href="/static/privacy">Privacy Policy</a> |
        <a href="/static/terms">Terms of Use</a> |
        <a href="http://www.plos.org/advertise/">Advertise</a> |
        <a href="http://www.plos.org/about/media-inquiries/">Media Inquiries</a>
      </div>
    </div>
    <div class="col col-2">
      <p><a href="http://www.plos.org/publications/journals/">Publications</a></p>
      <div class="nav">
        <ul>
          <li><a href="http://www.plosbiology.org">PLOS Biology</a></li>
          <li><a href="http://www.plosmedicine.org">PLOS Medicine</a></li>
          <li><a href="http://www.ploscompbiol.org">PLOS Computational Biology</a></li>
          <li><a href="http://currents.plos.org">PLOS Currents</a></li>
          <li><a href="http://www.plosgenetics.org">PLOS Genetics</a></li>
          <li><a href="http://www.plospathogens.org">PLOS Pathogens</a></li>
          <li><a href="http://www.plosone.org">PLOS ONE</a></li>
          <li><a href="http://www.plosntds.org">PLOS Neglected Tropical Diseases</a></li>
        </ul>
      </div>
    </div>
    <div class="col col-3">
      <div class="nav">
        <p><a href="http://www.plos.org">plos.org</a></p>
        <p><a href="http://blogs.plos.org">Blogs</a></p>
        <p><a href="http://www.ploscollections.org">Collections</a></p>
        <p><a href="/feedback/new">Send us feedback</a></p>

        <p>California (US) corporation #C2354500, based in San Francisco</p>
      </div>
    </div>
  </div>
</div><!-- pageftr -->

</div><!-- end page-wrap, this div is in header.ftl -->
<script type="text/javascript" src="/javascript/jquery-1.8.1-min.js?v=Tm7VCOzZz3lE03ghpkS6SWkHbyI"></script>
<script type="text/javascript" src="/javascript/ga-min.js?v=lNQ4gt8QcPDatjsdOFl_FGpPhLY"></script>
<script type="text/javascript" src="/javascript/jquery.hoverIntent-min.js?v=mRiGNYY9cIXxVb8u0K_MdW7hHnc"></script>
<script type="text/javascript" src="/javascript/jquery.placeholder-min.js?v=21Pn56Ur9h1N4K4VZDa0nqI3Pxo"></script>
<script type="text/javascript" src="/javascript/jquery.jsonp-2.4.0-min.js?v=lqTpzoHfSq3I5Ygo01qq5WankEo"></script>
<script type="text/javascript" src="/javascript/jquery-ui-1.9.2.custom-min.js?v=raSSlfNO0YsV5uUpAKmTB9n5VTc"></script>
<script type="text/javascript" src="/javascript/jquery.tooltip-min.js?v=cw+6Smh+mdryIA25xvqIvHMrnZM"></script>
<script type="text/javascript" src="/javascript/jquery.uniform-min.js?v=kYUAnX6W2W_2fK3RIuQ2m_YFG9U"></script>
<script type="text/javascript" src="/javascript/jquery.pjax-min.js?v=939kLBjL5_YKbx71T1RHjYaD4l8"></script>
<script type="text/javascript" src="/javascript/imagesloaded-min.js?v=XeuAp8Gc3mvQUo+wZCSF8ttPwvw"></script>
<script type="text/javascript" src="/javascript/figviewer-min.js?v=yPUa0sUQ_iHkI+IRv2i9bjyZJFo"></script>
<script type="text/javascript" src="/javascript/global-min.js?v=0Q3PwjeaWtXYDnqIsQvnL_ou0qs"></script>
<script type="text/javascript" src="/javascript/jquery.touchswipe-min.js?v=huaek_e6HqTduvCNAN91dJolTyw"></script>
<script type="text/javascript" src="/javascript/jquery.base64-min.js?v=VwV1zeVqKZj5FCAdlK0q5NRxbBg"></script>
<script type="text/javascript" src="/javascript/alm-min.js?v=Y5gm6B0b4Kx2YHNObNrgEeBgXlY"></script>
<script type="text/javascript" src="/javascript/taxonomy-browser-min.js?v=vBVMuDMYkGJCXIUxLe35GoyiJNw"></script>
<script type="text/javascript" src="/javascript/jquery.filterize-min.js?v=j0ZKVnHyk2nhFy8eIuNJkp7xaM0"></script>
<script type="text/javascript" src="/javascript/plosone-min.js?v=TK4H4arL_XBSwwJq+K1N3kqYfAI"></script>
<script type="text/javascript" src="/javascript/twitter-min.js?v=xKgcxLsQFXy+at1ao1NVke8nFlM"></script>
<script type="text/javascript" src="/javascript/crossmark.1.4-min.js?v=3FO4k0SjwTaGNnKGNSqthar1080"></script>
<script type="text/javascript">
  var _sf_async_config={uid:16579,domain:"plosone.org"};
  (function(){
    function loadChartbeat() {
      window._sf_endpt=(new Date()).getTime();
      var e = document.createElement('script');
      e.setAttribute('language', 'javascript');
      e.setAttribute('type', 'text/javascript');
      e.setAttribute('src',
          (("https:" == document.location.protocol) ? "https://a248.e.akamai.net/chartbeat.download.akamai.com/102508/" : "http://static.chartbeat.com/") +
              "js/chartbeat.js");
      document.body.appendChild(e);
    }
    var oldonload = window.onload;
    window.onload = (typeof window.onload != 'function') ?
        loadChartbeat : function() { oldonload(); loadChartbeat(); };
  })();
</script>
<!-- <script type="application/javascript" src="http://crossmark.crossref.org/javascripts/v1.3/crossmark.min.js"></script> -->

</body>
</html>
